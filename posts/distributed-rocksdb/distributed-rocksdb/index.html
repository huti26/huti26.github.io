<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Making Meta's RocksDB distributed with UCX to support Infiniband Hardware | Hutan Baghery Moghaddam</title><meta name=keywords content="C++,Distributed Systems"><meta name=description content="RocksDB is a performant embedded Key-Value Store. As part of my Master's Thesis, I created a distributed version of it. This article goes over the key concepts and what I consider the most interesting parts."><meta name=author content="Hutan Baghery Moghaddam"><link rel=canonical href=https://huti26.github.io/posts/distributed-rocksdb/distributed-rocksdb/><link crossorigin=anonymous href=../../../assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=../../../assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://huti26.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://huti26.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://huti26.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://huti26.github.io/apple-touch-icon.png><link rel=mask-icon href=https://huti26.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="Making Meta's RocksDB distributed with UCX to support Infiniband Hardware"><meta property="og:description" content="RocksDB is a performant embedded Key-Value Store. As part of my Master's Thesis, I created a distributed version of it. This article goes over the key concepts and what I consider the most interesting parts."><meta property="og:type" content="article"><meta property="og:url" content="https://huti26.github.io/posts/distributed-rocksdb/distributed-rocksdb/"><meta property="og:image" content="https://huti26.github.io/distributed-rocksdb.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-07-07T00:00:00+00:00"><meta property="article:modified_time" content="2022-07-07T00:00:00+00:00"><meta property="og:site_name" content="Hutan Baghery Moghaddam"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://huti26.github.io/distributed-rocksdb.png"><meta name=twitter:title content="Making Meta's RocksDB distributed with UCX to support Infiniband Hardware"><meta name=twitter:description content="RocksDB is a performant embedded Key-Value Store. As part of my Master's Thesis, I created a distributed version of it. This article goes over the key concepts and what I consider the most interesting parts."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://huti26.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Making Meta's RocksDB distributed with UCX to support Infiniband Hardware","item":"https://huti26.github.io/posts/distributed-rocksdb/distributed-rocksdb/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Making Meta's RocksDB distributed with UCX to support Infiniband Hardware","name":"Making Meta\u0027s RocksDB distributed with UCX to support Infiniband Hardware","description":"RocksDB is a performant embedded Key-Value Store. As part of my Master's Thesis, I created a distributed version of it. This article goes over the key concepts and what I consider the most interesting parts.","keywords":["C++","Distributed Systems"],"articleBody":"Preface This project is my first C++ project and it is my Master’s Thesis. I had 6 months to create this project. However, at least 2 months of this time was spent writing the actual thesis and not coding. About 3 months were spent coding with an additional month for benchmarking.\nRocksDB RocksDB is open-source and successfully used in many projects. For example in Apache Flink, state is maintained during calculations with RocksDB. RocksDB uses Log-Structured Merge-Trees under the hood. Log-Structured Merge-Trees avoid random writes by making all data immutable. This results in only sequential writes, which are significantly more performant. Of course, there are trade-offs for that. Reads become costlier, but this is balanced out by maintaining bloom filters and index structures. Also, there is redundancy. This is limited by compaction.\nMeta uses three different distributed Key-Value Stores internally, which are all based on RocksDB. Each of these systems is fine-tuned for its use case. This adaptation to the use case is how Meta seems to extract maximum performance from their RocksDB-based systems. This is stressed by this paper, in which they describe how they developed specialized systems to track user behavior of their systems. For this to work, RocksDB has to be very adaptable. You can both adapt RocksDB in the data structures that are used internally and in the numbers. For example, you can pick from many data structures in which data is kept in-memory in the MemTable. You can also decide the maximum size of the MemTable, which is 64MB per default.\nBased on this, we are assured that a distributed version of RocksDB can be very performant.\nWe will leave it at that here, but if you are curious about more details regarding RocksDB, you should check out the RocksDB Wiki.\nUCX Unified Communication X is an open-source project, which allows users to develop for Ethernet and Infiniband hardware at the same time. Code, that runs on Infiniband hardware, can also run on Ethernet, without any changes. UCX is backed by multiple companies, one of them being NVidia. UCX has lots of functionalities, but we won’t go into detail on it here. This project only utilizes the high-level API called UCP. With UCP we can transmit data between nodes in multiple ways. We only use UCP stream sends in this project, which allow us to send arbitrary data streams.\nIf you are curious about UCX details, you can check out the source code, which is linked at the end of this article.\nCode Structure /cpp\r│ .gitignore\r│ CMakeLists.txt\r│\r├───clion-run-configs\r│ 1server.run.xml\r│ 2servers - server 1.run.xml\r│ 2servers - server 2.run.xml\r│ 4servers - server 1.run.xml\r│ 4servers - server 2.run.xml\r│ 4servers - server 3.run.xml\r│ 4servers - server 4.run.xml\r│ 8-clients.run.xml\r│ client CLI.run.xml\r│ client.run.xml\r│ Four Servers.run.xml\r│ Two Servers.run.xml\r│\r├───src\r│ │ CMakeLists.txt\r│ │\r│ └───distributed-rocksdb\r│ │ CMakeLists.txt\r│ │\r│ ├───Client\r│ │ CMakeLists.txt\r│ │ DrdbClient.cpp\r│ │ DrdbClient.h\r│ │ DrdbClientApp.cpp\r│ │ DrdbClientApp.h\r│ │ main.cpp\r│ │\r│ ├───Common\r│ │ ClientServerCommons.h\r│ │\r│ ├───JNI\r│ │ CMakeLists.txt\r│ │ JDrdbClient.cpp\r│ │ JDrdbClient.h\r│ │\r│ └───Server\r│ CMakeLists.txt\r│ DrdbConnectionAcceptor.cpp\r│ DrdbConnectionAcceptor.h\r│ DrdbConnectionDeque.cpp\r│ DrdbConnectionDeque.h\r│ DrdbConnectionHandler.cpp\r│ DrdbConnectionHandler.h\r│ DrdbEndpointState.cpp\r│ DrdbEndpointState.h\r│ DrdbServerApp.cpp\r│ DrdbServerApp.h\r│ DrdbWorker.cpp\r│ DrdbWorker.h\r│ main.cpp\r│ server_config1.txt\r│ server_config2.txt\r│ server_config4.txt\r│\r└───Valgrind\r├───leak-check-full\r│ 1000-put-client.txt\r│ 1000-put-server.txt\r│\r└───leak-check-yes\r1000-empty-reads-client.txt\r1000-empty-reads-server.txt The main content of our C++ implementation is in /cpp/src. There we have separate folders for the Client and the Server code. Additionally, we have some shared code between the Client and the Server in /cpp/src/Common. The folder /cpp/src/JNI contains the code required to access our project from Java. We will talk about that later. Apart from that, we tested our implementation with Valgrind and the results are in /cpp/Valgrind. Also, there are some helpful run configurations if you want to test the project yourself, in /cpp/clion-run-configs.\nTo make the code accessible from Java, we also have /java folder which looks as follows.\n/java\r│ .gitignore\r│ build.gradle\r│ gradlew\r│ gradlew.bat\r│ settings.gradle\r│\r├───gradle\r│ └───wrapper\r│ gradle-wrapper.jar\r│ gradle-wrapper.properties\r│\r└───src\r└───main\r├───include\r│ site_ycsb_db_JDrdbClient.h\r│\r└───java\r│ App.java\r│\r└───site\r└───ycsb\r└───db\rJDrdbClient.java\rKeyValueStore.java\rpackage-info.java We use Gradle as our build tool and implement code to make use of the Java Native Interface and the Yahoo! Cloud Serving Benchmark. Both will be discussed later.\nArchitecture The current implementation only supports static clusters. Each server needs to be given the ip-address and port of all other servers via a config file. All servers are connected with each other. By default, there is a single connection between each server, but the user can increase this amount. Each server has a static amount of Client Workers and Redirect Workers. Client Workers handle communication with clients and Redirect Workers handle incoming redirect requests.\nRedirecting occurs, when a client requests an object, that another server is responsible for. Key responsibility is decided by their hash value. The current implementation uses murmurhash2, which is the default hashing algorithm in the used version of GCC. Each server has a unique id starting from 0. The responsible server is then calculated as follows.\nresponsible_server = hash(key) % server_count So for example in a 4-node cluster, our nodes have the ids 0, 1, 2, and 3. Then for a given key, we would calculate hash(key), which is X. Then we calculate X modulo 4 and that value matches one of our server ids.\nWith all this being said, let’s look at an example to understand the current implementation.\nThis is a two-node cluster. Right from the start, we have two Client Workers and one Redirect Worker on each node. Each of these Workers is a separate thread. Not depicted in this picture, are two further threads: ClientAccepter and ClientHandler. These two threads deal with new connection requests. Due to UCX’s design, accepting and handling have to be in separate threads if you want to be able to deal with multiple connection requests at once. These two threads are omitted for clarity. We will briefly discuss connection handling later.\nNow the nodes will have to connect to each other.\nNode A creates a Redirector B. This is a class, which Node A can use for outgoing redirect requests to Node B. To handle that, Node B creates a Redirect EP A. EP stands for endpoint. Note that this is also a class. The thread count in our implementation is static, to guarantee good scaling. A thread is required to operate that Redirect EP, therefore responsibility of the Redirect EP is given to the Redirect Worker 1 of Node B.\nNow the same happens the other way around.\nSimilar to before Node B creates a Redirector and Node A creates a Redirect EP. Now the cluster is ready to handle clients.\nHere three clients connected to Node A in the following order: 33, 44, 55. Node A gives responsibility for the endpoints to the Client Workers in a round-robin fashion.\nConnection Handling For connection handling we use a std::deque with a mutex, that is shared between our ConnectionAcceptor and our ConnectionHandler. Connection requests are handled in a FIFO (First in First out) manner. To realize this, the ConnectionHandler always handles the first element of the deque, while the ConnectionAcceptor inserts new requests at the back of it. The ConnectionHandler is a fragment of earlier versions and could be refactored in an improved version. The current implementation of the ConnectionHandler only distributes the requests to yet another deque. Each of the Workers has an own deque. Whenever the Workers find a connection request in their deque, they will handle it, before continuing with further communication. To understand this better, let’s look at the infinite loop in which the Workers are running.\nint status; while (true) { // PART 1: Check for pending connection requests while (!drdbConnectionDeque-\u003econnection_contexts.empty()) { handle_connection_request(); ep = drdb_endpoints.back().get(); // Now setup the Client for a new request, this will return immediately status = receive_method(); } // PART 2: Check for new requests made by clients ... } As indicated in the code, during the first part of the loop, the Workers will check if there are any connection requests in the deque. Only once those are all handled, will the Workers move on to part 2, in which they handle communication with the clients.\nCommunication Now we will continue with the prior mentioned part 2. Let’s look at the code of it.\nint status; while (true) { // PART 1: Check for pending connection requests ... // PART 2: Check for new requests made by clients for (auto \u0026element: drdb_endpoints) { if (element-\u003ereceive_request_context.complete == 1) { ep = element.get(); // A Request has been made by the Client, handle it status = receive_method(); // When an ep disconnects, dont make the second request if (status == -1) { break; } // Now setup the Client for a new request // This will return immediately status = receive_method(); } else { ucp_worker_progress(*ucp_data_worker_p); } } } The Worker loops over all the endpoints he is responsible for and checks if there are new requests. Whenever there is such a request, the Worker will handle an entire communication cycle. Therefore, the current implementation is not asynchronous. Unfortunately, there was no time left, to implement that. The current implementation is already done with async in mind and the architecture should easily support it.\nA communication cycle consists of a message by the client and a response by the server. Currently, simple get, put, and delete commands are implemented. The communication protocol is defined as follows.\nClient To Server Communication Server To Client Communication Yahoo! Cloud Serving Benchmark To evaluate our server, we wanted to use the Yahoo! Cloud Serving Benchmark, YCSB for short. Our server is still rather minimal and misses three features:\nRange scans field lengths \u003e 1, which means, that more than a single field gets transmitted in a message. Dividing keys into multiple tables To assure, that we don’t accidentally run a benchmark that makes use of these functionalities, we define a limiting interface. The interface concatenates table name and key name to emulate table support as follows.\nprivate String generateKey(String table, String key) { return String.format(\"%s.%s\", table, key); } For our other functions, the interface will return an error.\n@Override public Status scan() { return Status.NOT_IMPLEMENTED; } @Override public Status read(Set\u003cString\u003e fields) { if(fields != null \u0026\u0026 fields.size() != 1) { System.err.println(\"Field counts other than 1 are not supported!\"); return Status.BAD_REQUEST; } ... } With this interface, we can implement YCSB as described in its documentation. The result of that can be found in the /yscb folder of this project.\nJava Native Interface Since the YCSB is written in Java, we have to make our Client C++ code available to the JVM with the Java Native Interface, JNI for short.\nTo achieve that, we define native functions in Java as follows.\npublic native long initDrdbClient(String serverAddress, int serverPort, String logDir); public native byte[] get(long nativePtr, String key, int getRequest); public native byte put(long nativePtr, String key, byte[] value, int putRequest); public native byte del(long nativePtr, String key, int delRequest); Native functions are not going to be implemented in Java. Instead, we will compile this code with the -h flag and get a header file from it. If you want to try this yourself with this project, you can use the Gradle task compileJava instead. Once we have received this header file, we have to implement it with native code.\nBefore we look at the native code, let’s talk about an encoding problem. Java Strings are UTF-16, while C++ Strings are UTF-8. So we have to convert our keys as follows.\n@Override public byte[] get(String key) { getRequest++; byte[] keyBytes = key.getBytes(StandardCharsets.UTF_8); String utf8EncodedKey = new String(keyBytes, StandardCharsets.UTF_8); byte[] result = get(nativePtr, utf8EncodedKey, getRequest); if (result.length == 0) { return null; } return result; } Now, let’s look at the native code for a get.\njbyteArray Java_site_ycsb_db_JDrdbClient_get(JNIEnv *env, jobject, jlong drdb_p, jstring jKey, jint request) { int request_number = (int) request; auto *drdbClient_p = (DrdbClient *) drdb_p; std::string key = convert_jstring(env, jKey); auto result = drdbClient_p-\u003eget(key); if (result.value.empty()) { return env-\u003eNewByteArray(0); } jbyteArray reply = env-\u003eNewByteArray(result.value.length()); env-\u003eSetByteArrayRegion(reply, 0, result.value.length(), reinterpret_cast\u003cconst jbyte *\u003e(result.value.c_str()) ); return reply; } First, we can see, that we use the address drdb_p to access our Client object. This pointer is given as an argument from the Java side. Logically, our JNI code works as follows: First, we initialize a Distributed RocksDB Client object with initDrdbClient(). This function then returns the pointer to our Client object. We then transmit this address to our get, put, or delete function every time we use it.\nNext, we have to convert our String once again, despite already changing the encoding on the Java side. This is simply due to JNI design. We can’t just use Strings or ByteArrays directly. Let’s first finish looking at this function, then we will look at the conversion functions. So after converting the key, we can simply call our get function and receive a result. We will then have to convert our result to a jbyteArray, which we then can return to the JVM. Now let’s look at the conversion functions:\nstd::string convert_jstring(JNIEnv *env, jstring jKey) { jboolean isCopy; auto converted_string = env-\u003eGetStringUTFChars(jKey, \u0026isCopy); auto key = std::string{converted_string}; env-\u003eReleaseStringUTFChars(jKey, converted_string); return key; } std::string convert_jByteArray(JNIEnv *env, jbyteArray jValue) { jsize num_bytes = env-\u003eGetArrayLength(jValue); jboolean isCopy; jbyte *elements = env-\u003eGetByteArrayElements(jValue, \u0026isCopy); std::string value{reinterpret_cast\u003cchar *\u003e(elements), static_cast\u003csize_t\u003e(num_bytes)}; env-\u003eReleaseByteArrayElements(jValue, elements, JNI_ABORT); return value; } We have to create a new object from the given bytes, we can not operate on the given object directly. Then we create a C++ std::string from that. Afterwards, we have to release the intermediate object, else wise we are going to leak memory.\nOnce the native code has been implemented, we have to compile it and create a library of it. In this project, this is done with CMake inside /cpp/src/distributed-rocksdb/JNI/CMakeLists.txt. This library then has to be loaded inside our Java program as follows.\npublic class JDrdbClient extends KeyValueStore { static { System.loadLibrary(\"jdrdb-client\"); } ... } For the scope of this article, this should be enough background knowledge on this project. If you are curious about more details, you can check out the repository.\nNext, we will talk a bit about containerization, then we will look at the benchmarks we ran.\nContainerization Docker I like Docker. The reproducibility is a big advantage for me. Therefore, I developed this project with Docker as my development environment. The newest versions of CLion allow using Docker as a dev environment and pretty much everything works flawlessly. This only applies to my M1 MacBook Air. On my Windows machine, the performance was awful. I tried plenty of solutions I found online, but none of them fixed the performance. When I make changes on my MacBook, there is close to zero latency compared to native development. On my Windows machine, there is multiple seconds latency. This occurs both “natively” and with WSL2. I have not tested it on Linux, but if I had to make a guess, I am certain the performance will be even better than on macOS.\nEnough talk about operating systems though, let’s look at the Docker config. We use /dockerfiles/debian_base.dockerfile as our dev environment. This dockerfile creates a dev environment based on Debian 11. It installs all required dependencies and sets required path variables. With that, if you want to test this project, all you have to do is clone the repository, build the docker image and select it as your dev environment in CLion. Pretty convenient as far as I am concerned. This helps if you find a bug and want someone else to help you out. I am certain VSCode also supports Docker as dev environments very well. I am not sure about other editors though.\nWith our build scripts mentioned at the beginning, we can start and test our implementation in CLion. Additionally, we also have docker-compose scripts for testing our implementation. docker-compose.4servers-4clients.yml for example starts a 4 node cluster and benchmarks it with 4 clients. The file looks as follows.\nservices: server1: build: context: . dockerfile: dockerfiles/drdb_debug.dockerfile image: \"hutii/drdb:latest\" entrypoint: \"/app/build/src/distributed-rocksdb/Server/distributed-rocksdb-server-cpp -s server1 -p 13337 -c /app/src/distributed-rocksdb/Server/server_config4.txt\" network_mode: host ... client1: build: context: . dockerfile: dockerfiles/drdb_debug.dockerfile image: \"hutii/drdb:latest\" entrypoint: \"/app/build/src/distributed-rocksdb/Client/distributed-rocksdb-client-cpp -s server1 -p 13337\" depends_on: - server1 - server2 - server3 - server4 network_mode: host ... We omit servers 2, 3, and 4, and clients 2, 3, and 4 for clarity. All services make use of drdb_debug.dockerfile. This file is rather long, so let’s look at this shortened version of it to understand the logic behind it.\n################### # BUILDER PHASE ################### FROM hutii/drdb:base AS builder # Build everything ... ################### # RELEASE IMAGE ################### FROM debian:11 # Install only required libraries for running ... # Copy binaries from builder phase ... This dockerfile makes use of multi-stage Docker builds. At the start, we have our builder phase. Here we use our previously mentioned base image, which has all kinds of build tools like GCC and CMake installed. With that, we compile our project and build everything. We can create a docker image based on this step with the following command.\ndocker build -f ./dockerfiles/drdb_debug.dockerfile --target builder -t hutii/drdb:debug . In the next phase, we start with a new base. This time we don’t use our base image, but a clean version of Debian 11 instead. We don’t install build tools like GCC and CMake this time. Instead, we only install required libraries for running. We then copy all the binaries created in the builder phase to this image. If want to build this slimmer image, we just need to omit the --target builder from our previous command.\ndocker build -f ./dockerfiles/drdb_debug.dockerfile -t hutii/drdb:latest . On my ARM-based machine, this results in the following image size difference.\nREPOSITORY TAG IMAGE ID CREATED SIZE\rhutii/drdb latest f167bfb0ddf9 About a minute ago 734MB\rhutii/drdb debug 3f84bc912b89 About a minute ago 1.96GB We save more than a GB. For deployment of a real project, this is relevant, as it saves storage on costly cloud services. For this project, it saves us some time, when uploading our image to DockerHub.\nAs a part of our containerization, we set up GitHub Actions, which automatically build and deploy our project to DockerHub, whenever we make a commit in a releases branch. The .github/workflows/docker-image.yml looks as follows.\nname: Docker Image CI on: push: branches: - \"releases/001\" jobs: build: runs-on: ubuntu-latest steps: - name: Login to DockerHub uses: docker/login-action@v1 with: username: ${{ secrets.DOCKERHUB_USERNAME }} password: ${{ secrets.DOCKERHUB_TOKEN }} - uses: actions/checkout@v2 - name: Build the Docker image run: docker build -f ./dockerfiles/debian_base.dockerfile -t hutii/drdb:base . \u0026\u0026 docker build -f ./dockerfiles/drdb_debug.dockerfile -t hutii/drdb:latest . \u0026\u0026 docker push hutii/drdb:latest We use our DockerHub-hosted image in the next step.\nSingularity We want to test our project on our University’s cluster. We use OpenPBS there and the users do not have root rights. Using Docker is therefore not possible. Podman is an option, but instead, we use Sylabs Singularity Containers, which are commonly used in High-Performance-Computing. Singularity makes it very easy to convert a Docker Image to a Singularity Image. The script looks as follows.\nBootstrap: docker\rFrom: hutii/drdb:latest\r%labels\rVersion v0.0.1\r%help\rWrapper for docker image of distributed rocksdb We just need to pull the image from DockerHub and that’s it. We can even use the Sylabs Remote Builder for this step. The Sylabs Remote Builder is a free service for building Singularity Images. With this Remote Builder and our GitHub Action, we completely removed the necessity for building anything locally. This is fairly convenient in two ways. First, I am developing on an ARM-based machine, but we need to deploy the project to x86-based machines. x86 Docker images are slow to build on my ARM-based machine and even worse, singularity does not work on ARM. The second point is, that I had to travel frequently during the time I had to write my Master’s Thesis, and internet while traveling is rather bad in Germany (strongly depending on the location).\nEvaluation Clusters We ran experiments in two different settings.\nThe first one is the Operating Systems Research Group Cluster at my university. There we have a variety of nodes with Infiniband hardware. We always used nodes with 56GBit/s Infiniband. All nodes run Intel Xeon E5-1650 CPUs. We use OpenPBS there and we requested nodes with 62GB of RAM for our servers and nodes with 15GB of RAM for our clients. We requested 8 vCPUs for both clients and servers. Both clients and servers had access to Flash Storage. All nodes run CentOS Stream 8.\nThe second setting is a cluster running on Amazon Web Services. We used r5.2xlarge instances for our servers. These have 8 vCPUs and 64GB of RAM. For our clients, we used c4.2xlarge instances, which have 15GB of RAM and also 8 vCPUs. We gave servers 200GB of gp2 Flash Storage and 30GB of gp2 Flash Storage to the clients. All nodes run Ubuntu 22.04.\nWorkloads All experiments are performed with the YCSB Workload A. Workload A consists of 50% reads and 50% updates. The distribution is Zipfian. Unless specified differently Workload A is run with a package size of 1KB. A real-world example for Workload A is a Session store recording recent actions.\nSoftware Versions The project was tested in a Singularity container running Debian 11, RocksDB 6.11.4-3, and UCX 1.12.0. For the HHU cluster singularity version 3.8.5-2.el8 was used. For the AWS cluster singularity version 3.9.9 was used.\nOperating Systems Research Group Cluster Optimal Configuration We have a static amount of Client Workers and Redirect Workers. Finding an optimal amount for both of those is interesting. Fully analyzing this was beyond the scope of my thesis, but with the following experiment, we get a first feeling for it. We compare two configurations: One with 4 Client Workers and 4 Redirect Workers. One with 6 Client Workers and 1 Redirect Worker. Both configurations create 2 connections to each other server.\nTo get a feeling for the data, violin plots are examined.\nOn the y-axis is the throughput in ops/s and on the x-axis we are increasing the number of client threads, that are sending requests to the cluster. With an increase in clients, we expect the throughput per client to reduce. We can see no anomalies in the distribution of our measurements here, which is good. Next, we can look at the summed throughput of our cluster.\nSince we have a 6-node cluster with each node having 8 vCPUs, we have 48 vCPUs in total. Therefore, we expect the total throughput to increase up to 48 clients at least. This is as expected. If we now compare the configurations, we see that the 4 Client Workers and 4 Redirect Workers configuration is slightly better on 6 and 12 clients. The 6 Client Worker and 1 Redirect Worker configuration is slightly better on 48 clients. In general, many more experiments are required to make hard statements about an optimal configuration.\nInfiniband vs Ethernet We specifically used UCX as our networking framework to support Infiniband hardware. Therefore a comparison between Ethernet and Infiniband hardware is very important. Let’s look at the achieved throughput here.\nOur Ethernet-based cluster reaches a total throughput of almost 100.000 ops/s while the Infiniband-based cluster achieves roughly 175.000 ops/s. Unfortunately only 1Gbit/s Ethernet was available. Another comparison with more powerful Ethernet connections would be nice in the future. Regardless, let us look at latency as well.\nThe Ethernet-based cluster has a median of 249 microseconds. The quantiles span from 255 to 237 microseconds. Minimum and maximum values are 227 and 262 respectively. The Infiniband-based cluster has a median of 145 microseconds. The quantiles span from 131 to 146 microseconds. Minimum and maximum values are 107 and 148 respectively.\nSo the span from minimum and maximum is 35 microseconds for the Ethernet-based cluster and 41 microseconds for the Infiniband-based cluster. The Ethernet-based cluster has a smaller total span. However, when one looks at the span of the quantiles, the Ethernet-based cluster has a span of 18 microseconds while the Infiniband-based cluster has a span of 15 microseconds. So while the Infiniband-based cluster performs better performance-wise, there is no clear winner in regards to consistency. These results are very similar for update latency.\nCluster Scalability The next experiment we will talk about in this article is regarding cluster scalability. With an increase in cluster size, we want linear scaling optimally.\nFrom the total throughput, we can already tell that it is not perfectly linear. We can analyze this in more detail by looking at the average throughput of each node in the cluster.\nHere we can see, that while an increase from 2 to 4 nodes hardly changes the performance, we see bigger dips for 6 and 8 nodes. Our average throughput in ops/s per node decreases from more than 43.000 to almost 39.000 from 2 to 8 nodes. Unfortunately, we did not have access to even bigger clusters, which would have been interesting to see.\nAmazon Web Services Cluster At this point of my thesis, we were running low on time. We still wanted to have a rough estimate about how we line up against industrial-grade alternatives. We had no time to run such alternatives on our own cluster. Instead, we used a paper from 2018 as a reference value. This paper is from Altoros. It benchmarked the following Databases as a Service: Couchbase Cloud, MongoDB Atlas, and Amazon DynamoDB. Altoros is a contributor to Couchbase, so we expect that they know their system the best and can extract maximum performance from it.\nAll the systems use AWS under the hood and the hardware is precisely described. Their benchmark with a 6 node cluster is recreated with Distributed RocksDB. An important difference is, that Altoros benchmarked with a field length of 10 and a field count of 1KB. So each transmitted package during the Altoros benchmark has a size of 10KB. Distributed RocksDB only supports field lengths of 1. So to get a close approximation, Distributed RocksDB was benchmarked with a field length of 1 and a field count of 10KB. So each transmitted package also has a size of 10KB.\nBefore seeing the results it is required to talk about bias. There is a bias that positively affects the results of Distributed RocksDB. The first one is that Distributed RocksDB has not implemented replication yet. So while the other systems use the industry standard of 3 replicas, Distributed RocksDB only uses 1 replica. The other systems are also more mature in that they implement server-failure-detection. Also, Distributed RocksDB does not support using different column families yet. How this is handled is explained in the YCSB implementation chapter. The result is that the communication protocol is simplified for Distributed RocksDB. With this bias in mind, one can look at the results.\nDatabase Throughput in ops/s Replicas Source Distributed RocksDB 13014 1 This Thesis (2022) Couchbase Cloud 33460 3 Altoros Paper (2018) MongoDB Atlas 19144 3 Altoros Paper (2018) Amazon DynamoDB 30404 3 Altoros Paper (2018) Distributed RocksDB achieves a throughput of roughly 13.000 ops/s while Couchbase Cloud, MongoDB Atlas, and Amazon DynamoDB achieve throughputs of roughly 33.000, 19.000, and 30.000 ops/s respectively. Despite all the positive biases mentioned prior, Distributed RocksDB performs worse than all compared solutions. However, the compared solutions were all very mature products in 2018 already and as mentioned, the key to optimal performance seems to be a strong optimization for the use case in Meta’s opinion. Distributed RocksDB has not been optimized at all. Also, systems often group multiple client requests and transmit them at once, to achieve better performance. Distributed RocksDB does not implement such optimizations yet. There are many aspects of RocksDB that can be fine-tuned in future work. So the results are not to be interpreted negatively.\nThis experiment was run on Ethernet hardware. It would be nice to see how Distributed RocksDB compares to other systems when it can take advantage of Infiniband hardware. Amazon does not offer access to Infiniband hardware, unfortunately. Microsoft offers Infiniband hardware in their cloud services, but using Azure in addition to AWS exceeded the time we had during my thesis.\nConclusion Building a distributed version of an already established key-Value Store was very interesting to me and I am glad I got the opportunity to have this as my Master’s Thesis topic. C++ can be tricky at times and I had some nasty bugs due to it. With some experience and code analysis features provided by modern IDEs, it is overall still fine to work with. Of course, it is not as simple as something like Python, but it should not be compared to it in the first place. Setting up your development environment can be very confusing, as there are many ways to do so and most documentations of projects expect you to already have a certain level of knowledge. I excluded those problems from this article since it is already very long as is, but I might come back to it in a separate one in the future.\nThere is a lot of potential future work to do. To improve performance the obsolete Connection Handler may be removed. Also moving to asynchronous communication should be possible without bigger architecture changes. Enhancing the communication protocol to support dealing with multiple keys in a single request is also an important aspect. Once that is done, the system should be optimized for the YCSB Workload A with RocksDB’s internal benchmarking tools. Rerunning these benchmarks afterwards would be interesting.\nYou can find the source code for the project here.\n","wordCount":"4952","inLanguage":"en","image":"https://huti26.github.io/distributed-rocksdb.png","datePublished":"2022-07-07T00:00:00Z","dateModified":"2022-07-07T00:00:00Z","author":{"@type":"Person","name":"Hutan Baghery Moghaddam"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://huti26.github.io/posts/distributed-rocksdb/distributed-rocksdb/"},"publisher":{"@type":"Organization","name":"Hutan Baghery Moghaddam","logo":{"@type":"ImageObject","url":"https://huti26.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://huti26.github.io/ accesskey=h title="Hutan BM (Alt + H)">Hutan BM</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://huti26.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://huti26.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://huti26.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://huti26.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://huti26.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Making Meta's RocksDB distributed with UCX to support Infiniband Hardware</h1><div class=post-description>RocksDB is a performant embedded Key-Value Store. As part of my Master's Thesis, I created a distributed version of it. This article goes over the key concepts and what I consider the most interesting parts.</div><div class=post-meta><span title='2022-07-07 00:00:00 +0000 UTC'>July 7, 2022</span>&nbsp;·&nbsp;24 min&nbsp;·&nbsp;4952 words&nbsp;·&nbsp;Hutan Baghery Moghaddam</div></header><figure class=entry-cover><img loading=lazy src=https://huti26.github.io/distributed-rocksdb.png alt></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#preface>Preface</a></li><li><a href=#rocksdb>RocksDB</a></li><li><a href=#ucx>UCX</a></li><li><a href=#code-structure>Code Structure</a></li><li><a href=#architecture>Architecture</a></li><li><a href=#connection-handling>Connection Handling</a></li><li><a href=#communication>Communication</a></li><li><a href=#yahoo-cloud-serving-benchmark>Yahoo! Cloud Serving Benchmark</a></li><li><a href=#java-native-interface>Java Native Interface</a></li><li><a href=#containerization>Containerization</a><ul><li><a href=#docker>Docker</a></li><li><a href=#singularity>Singularity</a></li></ul></li><li><a href=#evaluation>Evaluation</a><ul><li><a href=#clusters>Clusters</a></li><li><a href=#workloads>Workloads</a></li><li><a href=#software-versions>Software Versions</a></li><li><a href=#operating-systems-research-group-cluster>Operating Systems Research Group Cluster</a></li><li><a href=#amazon-web-services-cluster>Amazon Web Services Cluster</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></details></div><div class=post-content><h2 id=preface>Preface<a hidden class=anchor aria-hidden=true href=#preface>#</a></h2><p>This project is my first C++ project and it is my Master&rsquo;s Thesis. I had 6 months to create this project. However, at least 2 months of this time was spent writing the actual thesis and not coding. About 3 months were spent coding with an additional month for benchmarking.</p><h2 id=rocksdb>RocksDB<a hidden class=anchor aria-hidden=true href=#rocksdb>#</a></h2><p>RocksDB is open-source and successfully used in many projects. For example in Apache Flink, <a href=https://flink.apache.org/2021/01/18/rocksdb.html target=_blank rel=noopener>state is maintained during calculations with RocksDB</a>. RocksDB uses Log-Structured Merge-Trees under the hood. Log-Structured Merge-Trees avoid random writes by making all data immutable. This results in only sequential writes, which are significantly more performant. Of course, there are trade-offs for that. Reads become costlier, but this is balanced out by maintaining bloom filters and index structures. Also, there is redundancy. This is limited by compaction.</p><p>Meta uses three different distributed Key-Value Stores internally, which are all based on RocksDB. Each of these systems is fine-tuned for its use case. This adaptation to the use case is how Meta seems to extract maximum performance from their RocksDB-based systems. This is stressed by this <a href=https://www.usenix.org/conference/fast20/presentation/cao-zhichao target=_blank rel=noopener>paper</a>, in which they describe how they developed specialized systems to track user behavior of their systems. For this to work, RocksDB has to be very adaptable. You can both adapt RocksDB in the data structures that are used internally and in the numbers. For example, you can pick from many data structures in which data is kept in-memory in the <a href=https://github.com/facebook/rocksdb/wiki/MemTable target=_blank rel=noopener>MemTable</a>. You can also decide the maximum size of the MemTable, which is 64MB per default.</p><p>Based on this, we are assured that a distributed version of RocksDB can be very performant.</p><p>We will leave it at that here, but if you are curious about more details regarding RocksDB, you should check out the <a href=https://github.com/facebook/rocksdb/wiki target=_blank rel=noopener>RocksDB Wiki</a>.</p><h2 id=ucx>UCX<a hidden class=anchor aria-hidden=true href=#ucx>#</a></h2><p><a href=https://github.com/openucx/ucx target=_blank rel=noopener>Unified Communication X</a> is an open-source project, which allows users to develop for Ethernet and Infiniband hardware at the same time. Code, that runs on Infiniband hardware, can also run on Ethernet, without any changes. UCX is backed by multiple companies, one of them being NVidia. UCX has lots of functionalities, but we won&rsquo;t go into detail on it here. This project only utilizes the high-level API called UCP. With UCP we can transmit data between nodes in multiple ways. We only use UCP stream sends in this project, which allow us to send arbitrary data streams.</p><p>If you are curious about UCX details, you can check out the source code, which is linked at the end of this article.</p><h2 id=code-structure>Code Structure<a hidden class=anchor aria-hidden=true href=#code-structure>#</a></h2><pre tabindex=0><code class=language-terminal data-lang=terminal>/cpp
│   .gitignore
│   CMakeLists.txt
│
├───clion-run-configs
│       1server.run.xml
│       2servers - server 1.run.xml
│       2servers - server 2.run.xml
│       4servers - server 1.run.xml
│       4servers - server 2.run.xml
│       4servers - server 3.run.xml
│       4servers - server 4.run.xml
│       8-clients.run.xml
│       client CLI.run.xml
│       client.run.xml
│       Four Servers.run.xml
│       Two Servers.run.xml
│
├───src
│   │   CMakeLists.txt
│   │
│   └───distributed-rocksdb
│       │   CMakeLists.txt
│       │
│       ├───Client
│       │       CMakeLists.txt
│       │       DrdbClient.cpp
│       │       DrdbClient.h
│       │       DrdbClientApp.cpp
│       │       DrdbClientApp.h
│       │       main.cpp
│       │
│       ├───Common
│       │       ClientServerCommons.h
│       │
│       ├───JNI
│       │       CMakeLists.txt
│       │       JDrdbClient.cpp
│       │       JDrdbClient.h
│       │
│       └───Server
│               CMakeLists.txt
│               DrdbConnectionAcceptor.cpp
│               DrdbConnectionAcceptor.h
│               DrdbConnectionDeque.cpp
│               DrdbConnectionDeque.h
│               DrdbConnectionHandler.cpp
│               DrdbConnectionHandler.h
│               DrdbEndpointState.cpp
│               DrdbEndpointState.h
│               DrdbServerApp.cpp
│               DrdbServerApp.h
│               DrdbWorker.cpp
│               DrdbWorker.h
│               main.cpp
│               server_config1.txt
│               server_config2.txt
│               server_config4.txt
│
└───Valgrind
    ├───leak-check-full
    │       1000-put-client.txt
    │       1000-put-server.txt
    │
    └───leak-check-yes
            1000-empty-reads-client.txt
            1000-empty-reads-server.txt
</code></pre><p>The main content of our C++ implementation is in <code>/cpp/src</code>. There we have separate folders for the Client and the Server code. Additionally, we have some shared code between the Client and the Server in <code>/cpp/src/Common</code>. The folder <code>/cpp/src/JNI</code> contains the code required to access our project from Java. We will talk about that later. Apart from that, we tested our implementation with Valgrind and the results are in <code>/cpp/Valgrind</code>. Also, there are some helpful run configurations if you want to test the project yourself, in <code>/cpp/clion-run-configs</code>.</p><p>To make the code accessible from Java, we also have <code>/java</code> folder which looks as follows.</p><pre tabindex=0><code class=language-terminal data-lang=terminal>/java
│   .gitignore
│   build.gradle
│   gradlew
│   gradlew.bat
│   settings.gradle
│
├───gradle
│   └───wrapper
│           gradle-wrapper.jar
│           gradle-wrapper.properties
│
└───src
    └───main
        ├───include
        │       site_ycsb_db_JDrdbClient.h
        │
        └───java
            │   App.java
            │
            └───site
                └───ycsb
                    └───db
                            JDrdbClient.java
                            KeyValueStore.java
                            package-info.java
</code></pre><p>We use Gradle as our build tool and implement code to make use of the Java Native Interface and the Yahoo! Cloud Serving Benchmark. Both will be discussed later.</p><h2 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h2><p>The current implementation only supports static clusters. Each server needs to be given the ip-address and port of all other servers via a config file. All servers are connected with each other. By default, there is a single connection between each server, but the user can increase this amount. Each server has a static amount of Client Workers and Redirect Workers. Client Workers handle communication with clients and Redirect Workers handle incoming redirect requests.</p><p>Redirecting occurs, when a client requests an object, that another server is responsible for. Key responsibility is decided by their hash value. The current implementation uses murmurhash2, which is the default hashing algorithm in the used version of GCC. Each server has a unique id starting from 0. The responsible server is then calculated as follows.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>responsible_server</span> <span class=o>=</span> <span class=n>hash</span><span class=p>(</span><span class=n>key</span><span class=p>)</span> <span class=o>%</span> <span class=n>server_count</span>
</span></span></code></pre></div><p>So for example in a 4-node cluster, our nodes have the ids 0, 1, 2, and 3. Then for a given key, we would calculate hash(key), which is X. Then we calculate X modulo 4 and that value matches one of our server ids.</p><p>With all this being said, let&rsquo;s look at an example to understand the current implementation.</p><p><img loading=lazy src=../../../posts/distributed-rocksdb/images/v2-example-1.png alt></p><p>This is a two-node cluster. Right from the start, we have two Client Workers and one Redirect Worker on each node. Each of these Workers is a separate thread. Not depicted in this picture, are two further threads: <code>ClientAccepter</code> and <code>ClientHandler</code>. These two threads deal with new connection requests. Due to UCX&rsquo;s design, accepting and handling have to be in separate threads if you want to be able to deal with multiple connection requests at once. These two threads are omitted for clarity. We will briefly discuss connection handling later.</p><p>Now the nodes will have to connect to each other.</p><p><img loading=lazy src=../../../posts/distributed-rocksdb/images/v2-example-2.png alt></p><p>Node A creates a Redirector B. This is a class, which Node A can use for outgoing redirect requests to Node B. To handle that, Node B creates a Redirect EP A. EP stands for endpoint. Note that this is also a class. The thread count in our implementation is static, to guarantee good scaling. A thread is required to operate that Redirect EP, therefore responsibility of the Redirect EP is given to the Redirect Worker 1 of Node B.</p><p>Now the same happens the other way around.</p><p><img loading=lazy src=../../../posts/distributed-rocksdb/images/v2-example-3.png alt></p><p>Similar to before Node B creates a Redirector and Node A creates a Redirect EP. Now the cluster is ready to handle clients.</p><p><img loading=lazy src=../../../posts/distributed-rocksdb/images/v2-example-4.png alt></p><p>Here three clients connected to Node A in the following order: 33, 44, 55. Node A gives responsibility for the endpoints to the Client Workers in a round-robin fashion.</p><h2 id=connection-handling>Connection Handling<a hidden class=anchor aria-hidden=true href=#connection-handling>#</a></h2><p>For connection handling we use a <code>std::deque</code> with a mutex, that is shared between our ConnectionAcceptor and our ConnectionHandler. Connection requests are handled in a FIFO (First in First out) manner. To realize this, the ConnectionHandler always handles the first element of the deque, while the ConnectionAcceptor inserts new requests at the back of it. The ConnectionHandler is a fragment of earlier versions and could be refactored in an improved version. The current implementation of the ConnectionHandler only distributes the requests to yet another deque. Each of the Workers has an own deque. Whenever the Workers find a connection request in their deque, they will handle it, before continuing with further communication. To understand this better, let&rsquo;s look at the infinite loop in which the Workers are running.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=kt>int</span> <span class=n>status</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>while</span> <span class=p>(</span><span class=nb>true</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// PART 1: Check for pending connection requests
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>while</span> <span class=p>(</span><span class=o>!</span><span class=n>drdbConnectionDeque</span><span class=o>-&gt;</span><span class=n>connection_contexts</span><span class=p>.</span><span class=n>empty</span><span class=p>())</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>handle_connection_request</span><span class=p>();</span>
</span></span><span class=line><span class=cl>        <span class=n>ep</span> <span class=o>=</span> <span class=n>drdb_endpoints</span><span class=p>.</span><span class=n>back</span><span class=p>().</span><span class=n>get</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1>// Now setup the Client for a new request, this will return immediately
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=n>status</span> <span class=o>=</span> <span class=n>receive_method</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// PART 2: Check for new requests made by clients
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>...</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>As indicated in the code, during the first part of the loop, the Workers will check if there are any connection requests in the deque. Only once those are all handled, will the Workers move on to part 2, in which they handle communication with the clients.</p><h2 id=communication>Communication<a hidden class=anchor aria-hidden=true href=#communication>#</a></h2><p>Now we will continue with the prior mentioned part 2. Let&rsquo;s look at the code of it.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=kt>int</span> <span class=n>status</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>while</span> <span class=p>(</span><span class=nb>true</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// PART 1: Check for pending connection requests
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// PART 2: Check for new requests made by clients
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>for</span> <span class=p>(</span><span class=k>auto</span> <span class=o>&amp;</span><span class=nl>element</span><span class=p>:</span> <span class=n>drdb_endpoints</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=p>(</span><span class=n>element</span><span class=o>-&gt;</span><span class=n>receive_request_context</span><span class=p>.</span><span class=n>complete</span> <span class=o>==</span> <span class=mi>1</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>ep</span> <span class=o>=</span> <span class=n>element</span><span class=p>.</span><span class=n>get</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1>// A Request has been made by the Client, handle it
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=n>status</span> <span class=o>=</span> <span class=n>receive_method</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1>// When an ep disconnects, dont make the second request
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=k>if</span> <span class=p>(</span><span class=n>status</span> <span class=o>==</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=k>break</span><span class=p>;</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1>// Now setup the Client for a new request
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=c1>// This will return immediately
</span></span></span><span class=line><span class=cl><span class=c1></span>            <span class=n>status</span> <span class=o>=</span> <span class=n>receive_method</span><span class=p>();</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=n>ucp_worker_progress</span><span class=p>(</span><span class=o>*</span><span class=n>ucp_data_worker_p</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>The Worker loops over all the endpoints he is responsible for and checks if there are new requests. Whenever there is such a request, the Worker will handle an entire communication cycle. Therefore, the current implementation is not asynchronous. Unfortunately, there was no time left, to implement that. The current implementation is already done with async in mind and the architecture should easily support it.</p><p>A communication cycle consists of a message by the client and a response by the server. Currently, simple get, put, and delete commands are implemented. The communication protocol is defined as follows.</p><table><thead><tr><th style=text-align:center><img loading=lazy src=../../../posts/distributed-rocksdb/images/client-to-server.png alt></th></tr></thead><tbody><tr><td style=text-align:center><strong>Client To Server Communication</strong></td></tr></tbody></table><table><thead><tr><th style=text-align:center><img loading=lazy src=../../../posts/distributed-rocksdb/images/server-to-client.png alt></th></tr></thead><tbody><tr><td style=text-align:center><strong>Server To Client Communication</strong></td></tr></tbody></table><h2 id=yahoo-cloud-serving-benchmark>Yahoo! Cloud Serving Benchmark<a hidden class=anchor aria-hidden=true href=#yahoo-cloud-serving-benchmark>#</a></h2><p>To evaluate our server, we wanted to use the Yahoo! Cloud Serving Benchmark, YCSB for short. Our server is still rather minimal and misses three features:</p><ol><li>Range scans</li><li>field lengths > 1, which means, that more than a single field gets transmitted in a message.</li><li>Dividing keys into multiple tables</li></ol><p>To assure, that we don&rsquo;t accidentally run a benchmark that makes use of these functionalities, we define a limiting interface. The interface concatenates table name and key name to emulate table support as follows.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl><span class=kd>private</span> <span class=n>String</span> <span class=nf>generateKey</span><span class=o>(</span><span class=n>String</span> <span class=n>table</span><span class=o>,</span> <span class=n>String</span> <span class=n>key</span><span class=o>)</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>String</span><span class=o>.</span><span class=na>format</span><span class=o>(</span><span class=s>&#34;%s.%s&#34;</span><span class=o>,</span> <span class=n>table</span><span class=o>,</span> <span class=n>key</span><span class=o>);</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span></code></pre></div><p>For our other functions, the interface will return an error.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl><span class=nd>@Override</span>
</span></span><span class=line><span class=cl><span class=kd>public</span> <span class=n>Status</span> <span class=nf>scan</span><span class=o>()</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>Status</span><span class=o>.</span><span class=na>NOT_IMPLEMENTED</span><span class=o>;</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nd>@Override</span>
</span></span><span class=line><span class=cl><span class=kd>public</span> <span class=n>Status</span> <span class=nf>read</span><span class=o>(</span><span class=n>Set</span><span class=o>&lt;</span><span class=n>String</span><span class=o>&gt;</span> <span class=n>fields</span><span class=o>)</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span><span class=o>(</span><span class=n>fields</span> <span class=o>!=</span> <span class=kc>null</span> <span class=o>&amp;&amp;</span> <span class=n>fields</span><span class=o>.</span><span class=na>size</span><span class=o>()</span> <span class=o>!=</span> <span class=n>1</span><span class=o>)</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>        <span class=n>System</span><span class=o>.</span><span class=na>err</span><span class=o>.</span><span class=na>println</span><span class=o>(</span><span class=s>&#34;Field counts other than 1 are not supported!&#34;</span><span class=o>);</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>Status</span><span class=o>.</span><span class=na>BAD_REQUEST</span><span class=o>;</span>
</span></span><span class=line><span class=cl>    <span class=o>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span></code></pre></div><p>With this interface, we can implement YCSB as described in its documentation. The result of that can be found in the <code>/yscb</code> folder of this project.</p><h2 id=java-native-interface>Java Native Interface<a hidden class=anchor aria-hidden=true href=#java-native-interface>#</a></h2><p>Since the YCSB is written in Java, we have to make our Client C++ code available to the JVM with the Java Native Interface, JNI for short.</p><p>To achieve that, we define <code>native</code> functions in Java as follows.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl><span class=kd>public</span> <span class=kd>native</span> <span class=kt>long</span> <span class=nf>initDrdbClient</span><span class=o>(</span><span class=n>String</span> <span class=n>serverAddress</span><span class=o>,</span> <span class=kt>int</span> <span class=n>serverPort</span><span class=o>,</span> <span class=n>String</span> <span class=n>logDir</span><span class=o>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kd>public</span> <span class=kd>native</span> <span class=kt>byte</span><span class=o>[]</span> <span class=nf>get</span><span class=o>(</span><span class=kt>long</span> <span class=n>nativePtr</span><span class=o>,</span> <span class=n>String</span> <span class=n>key</span><span class=o>,</span> <span class=kt>int</span> <span class=n>getRequest</span><span class=o>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kd>public</span> <span class=kd>native</span> <span class=kt>byte</span> <span class=nf>put</span><span class=o>(</span><span class=kt>long</span> <span class=n>nativePtr</span><span class=o>,</span> <span class=n>String</span> <span class=n>key</span><span class=o>,</span> <span class=kt>byte</span><span class=o>[]</span> <span class=n>value</span><span class=o>,</span> <span class=kt>int</span> <span class=n>putRequest</span><span class=o>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kd>public</span> <span class=kd>native</span> <span class=kt>byte</span> <span class=nf>del</span><span class=o>(</span><span class=kt>long</span> <span class=n>nativePtr</span><span class=o>,</span> <span class=n>String</span> <span class=n>key</span><span class=o>,</span> <span class=kt>int</span> <span class=n>delRequest</span><span class=o>);</span>
</span></span></code></pre></div><p>Native functions are not going to be implemented in Java. Instead, we will compile this code with the <code>-h</code> flag and get a header file from it. If you want to try this yourself with this project, you can use the Gradle task <code>compileJava</code> instead. Once we have received this header file, we have to implement it with native code.</p><p>Before we look at the native code, let&rsquo;s talk about an encoding problem. Java Strings are UTF-16, while C++ Strings are UTF-8. So we have to convert our keys as follows.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl><span class=nd>@Override</span>
</span></span><span class=line><span class=cl><span class=kd>public</span> <span class=kt>byte</span><span class=o>[]</span> <span class=nf>get</span><span class=o>(</span><span class=n>String</span> <span class=n>key</span><span class=o>)</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=n>getRequest</span><span class=o>++;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=kt>byte</span><span class=o>[]</span> <span class=n>keyBytes</span> <span class=o>=</span> <span class=n>key</span><span class=o>.</span><span class=na>getBytes</span><span class=o>(</span><span class=n>StandardCharsets</span><span class=o>.</span><span class=na>UTF_8</span><span class=o>);</span>
</span></span><span class=line><span class=cl>    <span class=n>String</span> <span class=n>utf8EncodedKey</span> <span class=o>=</span> <span class=k>new</span> <span class=n>String</span><span class=o>(</span><span class=n>keyBytes</span><span class=o>,</span> <span class=n>StandardCharsets</span><span class=o>.</span><span class=na>UTF_8</span><span class=o>);</span>
</span></span><span class=line><span class=cl>    <span class=kt>byte</span><span class=o>[]</span> <span class=n>result</span> <span class=o>=</span> <span class=n>get</span><span class=o>(</span><span class=n>nativePtr</span><span class=o>,</span> <span class=n>utf8EncodedKey</span><span class=o>,</span> <span class=n>getRequest</span><span class=o>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=o>(</span><span class=n>result</span><span class=o>.</span><span class=na>length</span> <span class=o>==</span> <span class=n>0</span><span class=o>)</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=kc>null</span><span class=o>;</span>
</span></span><span class=line><span class=cl>    <span class=o>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>result</span><span class=o>;</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span></code></pre></div><p>Now, let&rsquo;s look at the native code for a <em>get</em>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>jbyteArray</span> <span class=nf>Java_site_ycsb_db_JDrdbClient_get</span><span class=p>(</span><span class=n>JNIEnv</span> <span class=o>*</span><span class=n>env</span><span class=p>,</span> <span class=n>jobject</span><span class=p>,</span> <span class=n>jlong</span> <span class=n>drdb_p</span><span class=p>,</span> <span class=n>jstring</span> <span class=n>jKey</span><span class=p>,</span> <span class=n>jint</span> <span class=n>request</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>request_number</span> <span class=o>=</span> <span class=p>(</span><span class=kt>int</span><span class=p>)</span> <span class=n>request</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>auto</span> <span class=o>*</span><span class=n>drdbClient_p</span> <span class=o>=</span> <span class=p>(</span><span class=n>DrdbClient</span> <span class=o>*</span><span class=p>)</span> <span class=n>drdb_p</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>std</span><span class=o>::</span><span class=n>string</span> <span class=n>key</span> <span class=o>=</span> <span class=n>convert_jstring</span><span class=p>(</span><span class=n>env</span><span class=p>,</span> <span class=n>jKey</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>auto</span> <span class=n>result</span> <span class=o>=</span> <span class=n>drdbClient_p</span><span class=o>-&gt;</span><span class=n>get</span><span class=p>(</span><span class=n>key</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=p>(</span><span class=n>result</span><span class=p>.</span><span class=n>value</span><span class=p>.</span><span class=n>empty</span><span class=p>())</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>env</span><span class=o>-&gt;</span><span class=n>NewByteArray</span><span class=p>(</span><span class=mi>0</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>jbyteArray</span> <span class=n>reply</span> <span class=o>=</span> <span class=n>env</span><span class=o>-&gt;</span><span class=n>NewByteArray</span><span class=p>(</span><span class=n>result</span><span class=p>.</span><span class=n>value</span><span class=p>.</span><span class=n>length</span><span class=p>());</span>
</span></span><span class=line><span class=cl>    <span class=n>env</span><span class=o>-&gt;</span><span class=n>SetByteArrayRegion</span><span class=p>(</span><span class=n>reply</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>result</span><span class=p>.</span><span class=n>value</span><span class=p>.</span><span class=n>length</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>                            <span class=k>reinterpret_cast</span><span class=o>&lt;</span><span class=k>const</span> <span class=n>jbyte</span> <span class=o>*&gt;</span><span class=p>(</span><span class=n>result</span><span class=p>.</span><span class=n>value</span><span class=p>.</span><span class=n>c_str</span><span class=p>())</span>
</span></span><span class=line><span class=cl>    <span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>reply</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>First, we can see, that we use the address <code>drdb_p</code> to access our Client object. This pointer is given as an argument from the Java side. Logically, our JNI code works as follows: First, we initialize a Distributed RocksDB Client object with <code>initDrdbClient()</code>. This function then returns the pointer to our Client object. We then transmit this address to our get, put, or delete function every time we use it.</p><p>Next, we have to convert our String once again, despite already changing the encoding on the Java side. This is simply due to JNI design. We can&rsquo;t just use Strings or ByteArrays directly. Let&rsquo;s first finish looking at this function, then we will look at the conversion functions. So after converting the key, we can simply call our get function and receive a result. We will then have to convert our result to a <code>jbyteArray</code>, which we then can return to the JVM. Now let&rsquo;s look at the conversion functions:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>std</span><span class=o>::</span><span class=n>string</span> <span class=n>convert_jstring</span><span class=p>(</span><span class=n>JNIEnv</span> <span class=o>*</span><span class=n>env</span><span class=p>,</span> <span class=n>jstring</span> <span class=n>jKey</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>jboolean</span> <span class=n>isCopy</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=k>auto</span> <span class=n>converted_string</span> <span class=o>=</span> <span class=n>env</span><span class=o>-&gt;</span><span class=n>GetStringUTFChars</span><span class=p>(</span><span class=n>jKey</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>isCopy</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>auto</span> <span class=n>key</span> <span class=o>=</span> <span class=n>std</span><span class=o>::</span><span class=n>string</span><span class=p>{</span><span class=n>converted_string</span><span class=p>};</span>
</span></span><span class=line><span class=cl>    <span class=n>env</span><span class=o>-&gt;</span><span class=n>ReleaseStringUTFChars</span><span class=p>(</span><span class=n>jKey</span><span class=p>,</span> <span class=n>converted_string</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>key</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>std</span><span class=o>::</span><span class=n>string</span> <span class=n>convert_jByteArray</span><span class=p>(</span><span class=n>JNIEnv</span> <span class=o>*</span><span class=n>env</span><span class=p>,</span> <span class=n>jbyteArray</span> <span class=n>jValue</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>jsize</span> <span class=n>num_bytes</span> <span class=o>=</span> <span class=n>env</span><span class=o>-&gt;</span><span class=n>GetArrayLength</span><span class=p>(</span><span class=n>jValue</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>jboolean</span> <span class=n>isCopy</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>jbyte</span> <span class=o>*</span><span class=n>elements</span> <span class=o>=</span> <span class=n>env</span><span class=o>-&gt;</span><span class=n>GetByteArrayElements</span><span class=p>(</span><span class=n>jValue</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>isCopy</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>std</span><span class=o>::</span><span class=n>string</span> <span class=n>value</span><span class=p>{</span><span class=k>reinterpret_cast</span><span class=o>&lt;</span><span class=kt>char</span> <span class=o>*&gt;</span><span class=p>(</span><span class=n>elements</span><span class=p>),</span> <span class=k>static_cast</span><span class=o>&lt;</span><span class=n>size_t</span><span class=o>&gt;</span><span class=p>(</span><span class=n>num_bytes</span><span class=p>)};</span>
</span></span><span class=line><span class=cl>    <span class=n>env</span><span class=o>-&gt;</span><span class=n>ReleaseByteArrayElements</span><span class=p>(</span><span class=n>jValue</span><span class=p>,</span> <span class=n>elements</span><span class=p>,</span> <span class=n>JNI_ABORT</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>value</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>We have to create a new object from the given bytes, we can not operate on the given object directly. Then we create a C++ <code>std::string</code> from that. Afterwards, we have to release the intermediate object, else wise we are going to leak memory.</p><p>Once the native code has been implemented, we have to compile it and create a library of it. In this project, this is done with CMake inside <code>/cpp/src/distributed-rocksdb/JNI/CMakeLists.txt</code>. This library then has to be loaded inside our Java program as follows.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl><span class=kd>public</span> <span class=kd>class</span> <span class=nc>JDrdbClient</span> <span class=kd>extends</span> <span class=n>KeyValueStore</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=kd>static</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>        <span class=n>System</span><span class=o>.</span><span class=na>loadLibrary</span><span class=o>(</span><span class=s>&#34;jdrdb-client&#34;</span><span class=o>);</span>
</span></span><span class=line><span class=cl>    <span class=o>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span></code></pre></div><p>For the scope of this article, this should be enough background knowledge on this project. If you are curious about more details, you can check out the repository.</p><p>Next, we will talk a bit about containerization, then we will look at the benchmarks we ran.</p><h2 id=containerization>Containerization<a hidden class=anchor aria-hidden=true href=#containerization>#</a></h2><h3 id=docker>Docker<a hidden class=anchor aria-hidden=true href=#docker>#</a></h3><p>I like Docker. The reproducibility is a big advantage for me. Therefore, I developed this project with Docker as my development environment. The newest versions of CLion allow using Docker as a dev environment and pretty much everything works flawlessly. This only applies to my M1 MacBook Air. On my Windows machine, the performance was awful. I tried plenty of solutions I found online, but none of them fixed the performance. When I make changes on my MacBook, there is close to zero latency compared to native development. On my Windows machine, there is multiple seconds latency. This occurs both &ldquo;natively&rdquo; and with WSL2. I have not tested it on Linux, but if I had to make a guess, I am certain the performance will be even better than on macOS.</p><p>Enough talk about operating systems though, let&rsquo;s look at the Docker config. We use <code>/dockerfiles/debian_base.dockerfile</code> as our dev environment. This dockerfile creates a dev environment based on Debian 11. It installs all required dependencies and sets required path variables. With that, if you want to test this project, all you have to do is clone the repository, build the docker image and select it as your dev environment in CLion. Pretty convenient as far as I am concerned. This helps if you find a bug and want someone else to help you out. I am certain VSCode also supports Docker as dev environments very well. I am not sure about other editors though.</p><p>With our build scripts mentioned at the beginning, we can start and test our implementation in CLion. Additionally, we also have docker-compose scripts for testing our implementation. <code>docker-compose.4servers-4clients.yml</code> for example starts a 4 node cluster and benchmarks it with 4 clients. The file looks as follows.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yml data-lang=yml><span class=line><span class=cl><span class=nt>services</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>server1</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>build</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>context</span><span class=p>:</span><span class=w> </span><span class=l>.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>dockerfile</span><span class=p>:</span><span class=w> </span><span class=l>dockerfiles/drdb_debug.dockerfile</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;hutii/drdb:latest&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>entrypoint</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;/app/build/src/distributed-rocksdb/Server/distributed-rocksdb-server-cpp -s server1 -p 13337 -c /app/src/distributed-rocksdb/Server/server_config4.txt&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>network_mode</span><span class=p>:</span><span class=w> </span><span class=l>host</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=l>...</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>client1</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>build</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>context</span><span class=p>:</span><span class=w> </span><span class=l>.</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span><span class=nt>dockerfile</span><span class=p>:</span><span class=w> </span><span class=l>dockerfiles/drdb_debug.dockerfile</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;hutii/drdb:latest&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>entrypoint</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;/app/build/src/distributed-rocksdb/Client/distributed-rocksdb-client-cpp -s server1 -p 13337&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>depends_on</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>server1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>server2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>server3</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=l>server4</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>network_mode</span><span class=p>:</span><span class=w> </span><span class=l>host</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=l>...</span><span class=w>
</span></span></span></code></pre></div><p>We omit servers 2, 3, and 4, and clients 2, 3, and 4 for clarity. All services make use of <code>drdb_debug.dockerfile</code>. This file is rather long, so let&rsquo;s look at this shortened version of it to understand the logic behind it.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=c>###################</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># BUILDER PHASE</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c>###################</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>FROM</span><span class=s> hutii/drdb:base AS builder</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Build everything</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span>...<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c>###################</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># RELEASE IMAGE</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c>###################</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>FROM</span><span class=s> debian:11</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Install only required libraries for running</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span>...<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Copy binaries from builder phase</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span>...<span class=err>
</span></span></span></code></pre></div><p>This dockerfile makes use of multi-stage Docker builds. At the start, we have our builder phase. Here we use our previously mentioned base image, which has all kinds of build tools like GCC and CMake installed. With that, we compile our project and build everything. We can create a docker image based on this step with the following command.</p><pre tabindex=0><code class=language-terminal data-lang=terminal>docker build -f ./dockerfiles/drdb_debug.dockerfile --target builder -t hutii/drdb:debug .
</code></pre><p>In the next phase, we start with a new base. This time we don&rsquo;t use our base image, but a clean version of Debian 11 instead. We don&rsquo;t install build tools like GCC and CMake this time. Instead, we only install required libraries for running. We then copy all the binaries created in the builder phase to this image. If want to build this slimmer image, we just need to omit the <code>--target builder</code> from our previous command.</p><pre tabindex=0><code class=language-terminal data-lang=terminal>docker build -f ./dockerfiles/drdb_debug.dockerfile -t hutii/drdb:latest .
</code></pre><p>On my ARM-based machine, this results in the following image size difference.</p><pre tabindex=0><code class=language-terminal data-lang=terminal>REPOSITORY      TAG         IMAGE ID        CREATED                 SIZE
hutii/drdb      latest      f167bfb0ddf9    About a minute ago      734MB
hutii/drdb      debug       3f84bc912b89    About a minute ago      1.96GB
</code></pre><p>We save more than a GB. For deployment of a real project, this is relevant, as it saves storage on costly cloud services. For this project, it saves us some time, when uploading our image to DockerHub.</p><p>As a part of our containerization, we set up GitHub Actions, which automatically build and deploy our project to DockerHub, whenever we make a commit in a releases branch. The <code>.github/workflows/docker-image.yml</code> looks as follows.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-yml data-lang=yml><span class=line><span class=cl><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Docker Image CI</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>on</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>push</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>branches</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=s2>&#34;releases/001&#34;</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=nt>jobs</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>  </span><span class=nt>build</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>runs-on</span><span class=p>:</span><span class=w> </span><span class=l>ubuntu-latest</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>    </span><span class=nt>steps</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Login to DockerHub</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>uses</span><span class=p>:</span><span class=w> </span><span class=l>docker/login-action@v1</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>with</span><span class=p>:</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>username</span><span class=p>:</span><span class=w> </span><span class=l>${{ secrets.DOCKERHUB_USERNAME }}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>          </span><span class=nt>password</span><span class=p>:</span><span class=w> </span><span class=l>${{ secrets.DOCKERHUB_TOKEN }}</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>uses</span><span class=p>:</span><span class=w> </span><span class=l>actions/checkout@v2</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>Build the Docker image</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w>        </span><span class=nt>run</span><span class=p>:</span><span class=w> </span><span class=l>docker build -f ./dockerfiles/debian_base.dockerfile -t hutii/drdb:base . &amp;&amp; docker build -f ./dockerfiles/drdb_debug.dockerfile -t hutii/drdb:latest . &amp;&amp; docker push hutii/drdb:latest</span><span class=w>
</span></span></span></code></pre></div><p>We use our DockerHub-hosted image in the next step.</p><h3 id=singularity>Singularity<a hidden class=anchor aria-hidden=true href=#singularity>#</a></h3><p>We want to test our project on our University&rsquo;s cluster. We use OpenPBS there and the users do not have root rights. Using Docker is therefore not possible. Podman is an option, but instead, we use <a href=https://sylabs.io/singularity/ target=_blank rel=noopener>Sylabs Singularity Containers</a>, which are commonly used in High-Performance-Computing. Singularity makes it very easy to convert a Docker Image to a Singularity Image. The script looks as follows.</p><pre tabindex=0><code class=language-terminal data-lang=terminal>Bootstrap: docker
From: hutii/drdb:latest

%labels
    Version v0.0.1

%help
    Wrapper for docker image of distributed rocksdb
</code></pre><p>We just need to pull the image from DockerHub and that&rsquo;s it. We can even use the <a href=https://cloud.sylabs.io/builder target=_blank rel=noopener>Sylabs Remote Builder</a> for this step. The Sylabs Remote Builder is a free service for building Singularity Images. With this Remote Builder and our GitHub Action, we completely removed the necessity for building anything locally. This is fairly convenient in two ways. First, I am developing on an ARM-based machine, but we need to deploy the project to x86-based machines. x86 Docker images are slow to build on my ARM-based machine and even worse, singularity does not work on ARM. The second point is, that I had to travel frequently during the time I had to write my Master&rsquo;s Thesis, and internet while traveling is rather bad in Germany (strongly depending on the location).</p><h2 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h2><h3 id=clusters>Clusters<a hidden class=anchor aria-hidden=true href=#clusters>#</a></h3><p>We ran experiments in two different settings.</p><p>The first one is the Operating Systems Research Group Cluster at my university. There we have a variety of nodes with Infiniband hardware. We always used nodes with 56GBit/s Infiniband. All nodes run Intel Xeon E5-1650 CPUs. We use OpenPBS there and we requested nodes with 62GB of RAM for our servers and nodes with 15GB of RAM for our clients. We requested 8 vCPUs for both clients and servers. Both clients and servers had access to Flash Storage. All nodes run CentOS Stream 8.</p><p>The second setting is a cluster running on Amazon Web Services. We used r5.2xlarge instances for our servers. These have 8 vCPUs and 64GB of RAM. For our clients, we used c4.2xlarge instances, which have 15GB of RAM and also 8 vCPUs. We gave servers 200GB of gp2 Flash Storage and 30GB of gp2 Flash Storage to the clients. All nodes run Ubuntu 22.04.</p><h3 id=workloads>Workloads<a hidden class=anchor aria-hidden=true href=#workloads>#</a></h3><p>All experiments are performed with the YCSB Workload A. Workload A consists of 50% reads and 50% updates. The distribution is Zipfian. Unless specified differently Workload A is run with a package size of 1KB. A real-world example for Workload A is a Session store recording recent actions.</p><h3 id=software-versions>Software Versions<a hidden class=anchor aria-hidden=true href=#software-versions>#</a></h3><p>The project was tested in a Singularity container running Debian 11, RocksDB 6.11.4-3, and UCX 1.12.0. For the HHU cluster singularity version 3.8.5-2.el8 was used. For the AWS cluster singularity version 3.9.9 was used.</p><h3 id=operating-systems-research-group-cluster>Operating Systems Research Group Cluster<a hidden class=anchor aria-hidden=true href=#operating-systems-research-group-cluster>#</a></h3><h4 id=optimal-configuration>Optimal Configuration<a hidden class=anchor aria-hidden=true href=#optimal-configuration>#</a></h4><p>We have a static amount of Client Workers and Redirect Workers. Finding an optimal amount for both of those is interesting. Fully analyzing this was beyond the scope of my thesis, but with the following experiment, we get a first feeling for it. We compare two configurations: One with 4 Client Workers and 4 Redirect Workers. One with 6 Client Workers and 1 Redirect Worker. Both configurations create 2 connections to each other server.</p><p>To get a feeling for the data, violin plots are examined.</p><p><img loading=lazy src=../../../posts/distributed-rocksdb/images/6-servers-config-comparison-throughput_boxplot.png alt></p><p>On the y-axis is the throughput in ops/s and on the x-axis we are increasing the number of client threads, that are sending requests to the cluster. With an increase in clients, we expect the throughput per client to reduce. We can see no anomalies in the distribution of our measurements here, which is good. Next, we can look at the summed throughput of our cluster.</p><p><img loading=lazy src=../../../posts/distributed-rocksdb/images/6-servers-config-comparison-throughput_summed_no_errors.png alt></p><p>Since we have a 6-node cluster with each node having 8 vCPUs, we have 48 vCPUs in total. Therefore, we expect the total throughput to increase up to 48 clients at least. This is as expected. If we now compare the configurations, we see that the 4 Client Workers and 4 Redirect Workers configuration is slightly better on 6 and 12 clients. The 6 Client Worker and 1 Redirect Worker configuration is slightly better on 48 clients. In general, many more experiments are required to make hard statements about an optimal configuration.</p><h4 id=infiniband-vs-ethernet>Infiniband vs Ethernet<a hidden class=anchor aria-hidden=true href=#infiniband-vs-ethernet>#</a></h4><p>We specifically used UCX as our networking framework to support Infiniband hardware. Therefore a comparison between Ethernet and Infiniband hardware is very important. Let&rsquo;s look at the achieved throughput here.</p><p><img loading=lazy src=../../../posts/distributed-rocksdb/images/eth-ib-throughput.png alt></p><p>Our Ethernet-based cluster reaches a total throughput of almost 100.000 ops/s while the Infiniband-based cluster achieves roughly 175.000 ops/s. Unfortunately only 1Gbit/s Ethernet was available. Another comparison with more powerful Ethernet connections would be nice in the future. Regardless, let us look at latency as well.</p><p><img loading=lazy src=../../../posts/distributed-rocksdb/images/eth-ib-read-latency.png alt></p><p>The Ethernet-based cluster has a median of 249 microseconds. The quantiles span from 255 to 237 microseconds. Minimum and maximum values are 227 and 262 respectively. The Infiniband-based cluster has a median of 145 microseconds. The quantiles span from 131 to 146 microseconds. Minimum and maximum values are 107 and 148 respectively.</p><p>So the span from minimum and maximum is 35 microseconds for the Ethernet-based cluster and 41 microseconds for the Infiniband-based cluster. The Ethernet-based cluster has a smaller total span. However, when one looks at the span of the quantiles, the Ethernet-based cluster has a span of 18 microseconds while the Infiniband-based cluster has a span of 15 microseconds. So while the Infiniband-based cluster performs better performance-wise, there is no clear winner in regards to consistency. These results are very similar for update latency.</p><h4 id=cluster-scalability>Cluster Scalability<a hidden class=anchor aria-hidden=true href=#cluster-scalability>#</a></h4><p>The next experiment we will talk about in this article is regarding cluster scalability. With an increase in cluster size, we want linear scaling optimally.</p><p><img loading=lazy src=../../../posts/distributed-rocksdb/images/cluster-total.png alt></p><p>From the total throughput, we can already tell that it is not perfectly linear. We can analyze this in more detail by looking at the average throughput of each node in the cluster.</p><p><img loading=lazy src=../../../posts/distributed-rocksdb/images/cluster-server.png alt></p><p>Here we can see, that while an increase from 2 to 4 nodes hardly changes the performance, we see bigger dips for 6 and 8 nodes. Our average throughput in ops/s per node decreases from more than 43.000 to almost 39.000 from 2 to 8 nodes. Unfortunately, we did not have access to even bigger clusters, which would have been interesting to see.</p><h3 id=amazon-web-services-cluster>Amazon Web Services Cluster<a hidden class=anchor aria-hidden=true href=#amazon-web-services-cluster>#</a></h3><p>At this point of my thesis, we were running low on time. We still wanted to have a rough estimate about how we line up against industrial-grade alternatives. We had no time to run such alternatives on our own cluster. Instead, we used a paper from 2018 as a reference value. This <a href=https://resources.enterprisetalk.com/ebook/Couchbase-Q3-EN-4.pdf target=_blank rel=noopener>paper</a> is from Altoros. It benchmarked the following Databases as a Service: Couchbase Cloud, MongoDB Atlas, and Amazon DynamoDB. Altoros is a contributor to Couchbase, so we expect that they know their system the best and can extract maximum performance from it.</p><p>All the systems use AWS under the hood and the hardware is precisely described. Their benchmark with a 6 node cluster is recreated with Distributed RocksDB. An important difference is, that Altoros benchmarked with a field length of 10 and a field count of 1KB. So each transmitted package during the Altoros benchmark has a size of 10KB. Distributed RocksDB only supports field lengths of 1. So to get a close approximation, Distributed RocksDB was benchmarked with a field length of 1 and a field count of 10KB. So each transmitted package also has a size of 10KB.</p><p>Before seeing the results it is required to talk about bias. There is a bias that positively affects the results of Distributed RocksDB. The first one is that Distributed RocksDB has not implemented replication yet. So while the other systems use the industry standard of 3 replicas, Distributed RocksDB only uses 1 replica. The other systems are also more mature in that they implement server-failure-detection. Also, Distributed RocksDB does not support using different column families yet. How this is handled is explained in the YCSB implementation chapter. The result is that the communication protocol is simplified for Distributed RocksDB. With this bias in mind, one can look at the results.</p><table><thead><tr><th>Database</th><th>Throughput in ops/s</th><th>Replicas</th><th>Source</th></tr></thead><tbody><tr><td>Distributed RocksDB</td><td>13014</td><td>1</td><td>This Thesis (2022)</td></tr><tr><td>Couchbase Cloud</td><td>33460</td><td>3</td><td>Altoros Paper (2018)</td></tr><tr><td>MongoDB Atlas</td><td>19144</td><td>3</td><td>Altoros Paper (2018)</td></tr><tr><td>Amazon DynamoDB</td><td>30404</td><td>3</td><td>Altoros Paper (2018)</td></tr></tbody></table><p>Distributed RocksDB achieves a throughput of roughly 13.000 ops/s while Couchbase Cloud, MongoDB Atlas, and Amazon DynamoDB achieve throughputs of roughly 33.000, 19.000, and 30.000 ops/s respectively. Despite all the positive biases mentioned prior, Distributed RocksDB performs worse than all compared solutions. However, the compared solutions were all very mature products in 2018 already and as mentioned, the key to optimal performance seems to be a strong optimization for the use case in Meta&rsquo;s opinion. Distributed RocksDB has not been optimized at all. Also, systems often group multiple client requests and transmit them at once, to achieve better performance. Distributed RocksDB does not implement such optimizations yet. There are many aspects of RocksDB that can be fine-tuned in future work. So the results are not to be interpreted negatively.</p><p>This experiment was run on Ethernet hardware. It would be nice to see how Distributed RocksDB compares to other systems when it can take advantage of Infiniband hardware. Amazon does not offer access to Infiniband hardware, unfortunately. Microsoft offers Infiniband hardware in their cloud services, but using Azure in addition to AWS exceeded the time we had during my thesis.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Building a distributed version of an already established key-Value Store was very interesting to me and I am glad I got the opportunity to have this as my Master&rsquo;s Thesis topic. C++ can be tricky at times and I had some nasty bugs due to it. With some experience and code analysis features provided by modern IDEs, it is overall still fine to work with. Of course, it is not as simple as something like Python, but it should not be compared to it in the first place. Setting up your development environment can be very confusing, as there are many ways to do so and most documentations of projects expect you to already have a certain level of knowledge. I excluded those problems from this article since it is already very long as is, but I might come back to it in a separate one in the future.</p><p>There is a lot of potential future work to do. To improve performance the obsolete Connection Handler may be removed. Also moving to asynchronous communication should be possible without bigger architecture changes. Enhancing the communication protocol to support dealing with multiple keys in a single request is also an important aspect. Once that is done, the system should be optimized for the YCSB Workload A with RocksDB&rsquo;s internal benchmarking tools. Rerunning these benchmarks afterwards would be interesting.</p><p>You can find the source code for the project <a href=https://github.com/huti26/distributed-rocksdb target=_blank rel=noopener>here</a>.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://huti26.github.io/tags/c++/>C++</a></li><li><a href=https://huti26.github.io/tags/distributed-systems/>Distributed Systems</a></li></ul><nav class=paginav><a class=next href=https://huti26.github.io/posts/formula-1/hamilton-vs-bottas/><span class=title>Next »</span><br><span>Lewis Hamilton 2017-2021</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://huti26.github.io/>Hutan Baghery Moghaddam</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>