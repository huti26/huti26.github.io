<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>OpenJDK's Project Loom: User-Level Threads in Java | Hutan Baghery Moghaddam</title><meta name=keywords content="Java,OpenJDK"><meta name=description content="Preface Nowadays it is universally known that a Java thread corresponds to a Kernel-Level Thread when looking at it in a very simplified matter. Project Loom is trying to introduce User-Level Threads to the Java ecosystem. They call these User-Level Threads Virtual Threads. Virtual Threads are supossed to increase performance and be more ressource efficient than just using Kernel-Level Threads. It is planned, that Virtual Threads become a drop-in replacement, which would be a free performance boost for any multithreaded Java application."><meta name=author content="Hutan Baghery Moghaddam"><link rel=canonical href=https://huti26.github.io/posts/project-loom/project-loom/><link crossorigin=anonymous href=../../../assets/css/stylesheet.css rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=../../../assets/js/highlight.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://huti26.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://huti26.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://huti26.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://huti26.github.io/apple-touch-icon.png><link rel=mask-icon href=https://huti26.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:title" content="OpenJDK's Project Loom: User-Level Threads in Java"><meta property="og:description" content="Preface Nowadays it is universally known that a Java thread corresponds to a Kernel-Level Thread when looking at it in a very simplified matter. Project Loom is trying to introduce User-Level Threads to the Java ecosystem. They call these User-Level Threads Virtual Threads. Virtual Threads are supossed to increase performance and be more ressource efficient than just using Kernel-Level Threads. It is planned, that Virtual Threads become a drop-in replacement, which would be a free performance boost for any multithreaded Java application."><meta property="og:type" content="article"><meta property="og:url" content="https://huti26.github.io/posts/project-loom/project-loom/"><meta property="og:image" content="https://huti26.github.io/loom.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-05-29T00:00:00+00:00"><meta property="article:modified_time" content="2022-05-29T00:00:00+00:00"><meta property="og:site_name" content="Hutan Baghery Moghaddam"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://huti26.github.io/loom.png"><meta name=twitter:title content="OpenJDK's Project Loom: User-Level Threads in Java"><meta name=twitter:description content="Preface Nowadays it is universally known that a Java thread corresponds to a Kernel-Level Thread when looking at it in a very simplified matter. Project Loom is trying to introduce User-Level Threads to the Java ecosystem. They call these User-Level Threads Virtual Threads. Virtual Threads are supossed to increase performance and be more ressource efficient than just using Kernel-Level Threads. It is planned, that Virtual Threads become a drop-in replacement, which would be a free performance boost for any multithreaded Java application."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://huti26.github.io/posts/"},{"@type":"ListItem","position":2,"name":"OpenJDK's Project Loom: User-Level Threads in Java","item":"https://huti26.github.io/posts/project-loom/project-loom/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"OpenJDK's Project Loom: User-Level Threads in Java","name":"OpenJDK\u0027s Project Loom: User-Level Threads in Java","description":"Preface Nowadays it is universally known that a Java thread corresponds to a Kernel-Level Thread when looking at it in a very simplified matter. Project Loom is trying to introduce User-Level Threads to the Java ecosystem. They call these User-Level Threads Virtual Threads. Virtual Threads are supossed to increase performance and be more ressource efficient than just using Kernel-Level Threads. It is planned, that Virtual Threads become a drop-in replacement, which would be a free performance boost for any multithreaded Java application.","keywords":["Java","OpenJDK"],"articleBody":"Preface Nowadays it is universally known that a Java thread corresponds to a Kernel-Level Thread when looking at it in a very simplified matter. Project Loom is trying to introduce User-Level Threads to the Java ecosystem. They call these User-Level Threads Virtual Threads. Virtual Threads are supossed to increase performance and be more ressource efficient than just using Kernel-Level Threads. It is planned, that Virtual Threads become a drop-in replacement, which would be a free performance boost for any multithreaded Java application. This is particularly interesting, because many Big-Data-Frameworks such as Hadoop or Spark are written in Java.\nAll my findings are from my Bachelor’s Thesis, which I wrote during the first three months of 2020. I now recapitulate my findings, so beware, that some findings might already be outdated, since two years have passed. The build used to benchmark the code is: Build 15-loom+4-55, released on 22.02.2020.\nIf you are ealready familiar with the general concept of virtual threads, feel free to skip to the evaluation chapter as the chapters prior can be a bit detailed.\nBackground We will start with processes and programms and work ourselves towards Virtual Threads from there.\nProcess A process is a program that is being executed. In the following figure, the relation between a Process and a program is presented.\nThe segment text contains the programcode. The segment initialized datacontains initialized global and static variables. The segment bss contains not yet initialized global variables and static variables. The heap is the extension of the bss. The stack is used for saving local variables, function parameters and memory areas of register contents. Both the stack and the heap will be dynamically extended if needed. Processes allow a system to process several programs simultaneously. An active process can have three different statuses.\nSource: Andrew S. Tannenbaum, Modern Operating Systems The scheduler plays an important role here. A scheduler is responsible for when a process is processed by the CPU. In UNIX systems, for example, a round-robin scheduler is used. This changes the active process in a predetermined cycle, such as 10-100ms. An active process is a process with the status Running. The CPU is then assigned to the process and executes the program of the process. If the CPU is removed from the process by the scheduler, the process is in Ready status. The process is then ready to be reassigned to the CPU and is waiting for it. A process can also block. This happens if the process has to wait for a certain input. The process is then in Blocked status and will not change to Ready status until the input becomes available.\nThread A process is heavy. Each process has its own address space and switching between processes takes a lot of time. Therefore threads were created as lightweight processes. Threads are part of a process and therefore they share the same address space. Switching between threads is a lot faster. The following diagram lists the unique items for threads and processes.\nEach process can have many threads. The following figure shows the hierarchy of processes and threads.\nIn modern programming, the line between processes and threads gets somewhat blurred, because processes often start with a single thread. Standalone processes without any threads are a thing of the past.\nThreads increase CPU efficiency. Whenever a thread has to block, for example, because it has to wait for I/O input, another thread of the same process can quickly continue and make use of the CPU. Constantly switching between processes would be very inefficient.\nA distinction is made between two types of threads. Kernel-level threads, which are managed directly by the operating system’s scheduler and User-level threads, where the programmer has to do the scheduling himself. Here is where Virtual Threads come into play.\nVirtual Threads Similar to how threads were created as lightweight processes, virtual threads were created as lightweight threads. Therefore they line up in the hierarchy right behind the kernel threads.\nSimilar to how the scheduler of an operating system allocates CPU time to each kernel thread, the JVM allocates CPU time to each virtual thread.\nSuperficially, this is all there is to virtual threads. A lightweight thread, which is supossed to make switching even more efficient than switching between threads.\nUnderneath the hood, there is a concept, which the developers of Project Loom call Continuation. We will look at that now. This chapter will be rather low level and just about coding details, which we found, when analyzing the source code. If that’s too detailed for you, skip ahead to the evaulation chapter, in which we run various benchmarks with virtual threads.\nContinuation In this chapter we will analyze concintuations conceptiually and especially how they are implemented by Project Loom on a code level.\nA continuation is a sequence of instructions, that can be yielded and continued. The following example shows how a continuation can calculate the sum of the first five natural numbers while yielding after each increment.\nvar scope = new ContinuationScope(\"ContinuationScope\"); var continuation = new Continuation(scope, () -\u003e { int n = 0; for(int i = 0; i \u003c 6; i++) { n = n + i; System.out.println(\"i: \" + i + \"\\t\" + \"n: \" + n); Continuation.yield(scope); } }); while(!continuation.isDone()) { System.out.println(); continuation.run(); } A continuation requires two arguments: A scope and a runnable. First, a scope is created. Then the continuation is created. Afterwards, the continuation is repeatedly run in a loop until it is done.\nThe scope is used to allow continuations to be nested. Each continuation has exactly one parent continuation and exactly one child continuation. Continuations run on top of kernel-level Java threads, which are referred to as carrier-threads. Every Java carrier-thread has exactly one continuation as one of its attributes. This continuation is supposed to be the innermost one.\nStack Each continuation has two stacks: One for objects and one for non-objects. Project Loom calls the continuation stack horizontal stack and the thread stack vertical stack. The abbreviated versions are called h-stack and v-stack. When a continuation yields, it will be unmounted. Unmounting copies the continuation frames from the thread stack to the continuation stack. Afterwards, the continuation frames are removed from the thread stack. Project Loom calls this process freezing. When a continuation starts or continues, it will be mounted. Mounting is the reverse process of unmounting. Therefore project Loom calls this process thawing.\nSource: https://www.youtube.com/watch?v=NV46KFV1m-4 Looking at the previously mentioned two stacks on code level: One stack is an integer array for primitive values and metadata. The other one is an object array for references. All stack related methods exist twice: Once for the integer array and once for the object array. The methods are very similar. Therefore this thesis will only explain one set of them, which is the integer set.\nProject Loom uses an integer sp as a stack pointer. Every time the stack is changed, the stack pointer will be updated. This is done by using the fixDecreasingIndexAfterResize method. As an example observe fixDecreasingIndexAfterResize using the arguments: index = 5, oldLength = 10, newLength = 20. It returns 15. The new stack pointer is therefore 15. That means that if a stack size gets changed, the remaining values are inserted starting at the end.\nprivate int sp = -1; // index into the h-stack private int fixDecreasingIndexAfterResize(int index, int oldLength, int newLength) { return newLength - (oldLength - index); } There are 3 functions related to stack management:\ngetStack: The getStack method is used to expand the stack. First, the program will test, whether the stack is null. If that is the case, the stack has not been created yet. Then the program will create the stack and adjust the stack pointer. If the stack is not empty, the program will create a new stack, that is big enough to fit the old and the new frames. Then it will copy the old frames into the new stack. Afterwards, the continuation’s stack is set to the new stack. The stack pointer will be updated. The return value is boolean. It is True when getStack succeded. The method fails when the old stack length is larger than the new one.\nresizeStack: Similar to how the getStack method expands the stack, the resizeStack method shrinks it. In contrast to the getStack method, the resizeStack method is a void and does not require to return a boolean on whether it succeeded or not.\nmaybeShrink: The maybeShrink method is called after yielding. It checks whether the stack size is bigger than a watermark it keeps track of. If it is, the watermark will be set to the stack size. Therefore if maybeShrink is called without adjusting the watermark, that means, that the stack size has not increased since the last time. If maybeShrink is called ten times without adjusting the watermark, the resizeStack method will be called with the watermark as it’s argument.\nAnother interesting detail is how Project Loom solves critical sections. This is done with a classic semaphore design. The method pin increments the semaphore cs and the method unpin decrements it.\nprivate short cs; // critical section semaphore public static void pin() { Continuation cont = currentCarrierThread().getContinuation(); if (cont != null) { if (cont.cs == Short.MAX_VALUE) throw new IllegalStateException(\"Too many pins\"); cont.cs++; } } public static void unpin() { Continuation cont = currentCarrierThread().getContinuation(); if (cont != null) { if (cont.cs == 0) throw new IllegalStateException(\"Not pinned\"); cont.cs--; } } Intrinsic Functions Intrinsic Functions are not implemented in Java, but in native code instead. Project Loom uses intrinsics to increase performance. Usually, if there is an intrinsic version of a function, there will still be a non-intrinsic one. In this case, there are no non-intrinsic versions of the functions. The following functions are exclusively intrinsic and relevant for this thesis:\n@HotSpotIntrinsicCandidate private static long getSP() @HotSpotIntrinsicCandidate private void doContinue() @HotSpotIntrinsicCandidate private static int doYield(int scopes) For the x86 platform the following functions in stubGenerator_x86_64.cpp create the bytecode for the intrinsic versions of the functions named above:\naddress generate_cont_getSP() address generate_cont_thaw(bool return_barrier, bool exception) RuntimeStub *generate_cont_doYield() In these functions, the maintainers of project Loom use their macroassembler. The macroassembler class can be found in src/hotspot/cpu/x86/macroAssembler_x86.cpp. The yield method also utilizes two other big classes. One is a codebuffer located in src/hotspot/cpu/x86/asm/codeBuffer.cpp and the other is a oopmap, used for garbage collection, located in src/hotspot/share/compiler/oopMap.cpp.\nAnalyzing these functions was not possible within the timeframe of my Bachelor’s Thesis, but if you are curious about even more details, these are the places to look for.\nRun When a continuation gets started or continued the run method is called. First the run method mounts the continuation. Mounting is realized using a VarHandle, which atomically sets a boolean to true.\nAfterwards the current carrier-thread’s continuation has to be updated. As an example the current carrier-thread’s continuation before the update will be called bar. The new continuation will be called foo.\nThread t = currentCarrierThread(); if (parent != null) { if (parent != t.getContinuation()) throw new IllegalStateException(); } else this.parent = t.getContinuation(); t.setContinuation(this); foo is not supossed to have a parent at this point of time. If foo has a parent, that parent has to be bar. If it is not, something seriously went wrong and an error is thrown. The expected outcome is, that foo has no parent. In that case foo will become the child bar. Afterwards foo will become the current carrier-threads’s continuation.\nBefore Carrier-Thread’s Continuation gets updated After Carrier-Thread’s Continuation gets updated After that, the run method will call either the enter method or the continue method depending on whether foo is being started for the first time.\nOnce foo is done, it returns to the run method. The current carrier-thread’s continuation will be set to bar. foo will be removed as a child of bar. Finally, foo will be unmounted, similar to how it was mounted at the beginning.\nYield Project Loom uses three different yield functions, which are always called in the same order.\nFirst public static boolean yield(ContinuationScope scope) is called. This method checks, if the current carrier-thread’s continuation is part of the given ContinuationScope scope. This is done by looping a variable c through the parents of the current carrier-thread’s continuation. The loop stops once c is either null or the scope of c is the same as the given scope. If c ends up being null, that means, that the program is being told to yield a ContinuationScope, while it is running a continuation, that is not part of that scope. This is undesired behavior and throws an exception. If c is not null, the program can continue and the second function will be called.\nThe other two functions were beyond the scope of my thesis, because they either are completely intrinsic or because they utilize variables that are returned from intrinsics. This is once again a nice spot to continue research, if you are curious about Project Loom.\nContinue The continue method is entirely intrinsic. Therefore it was beyond the scope of my thesis.\nEvaluation We will run two different sets of experiments. The first one is provided by Project Loom themselves. Those experiments focus on analyzing performance of the Continuation class and its components. The second one is a simple experiment made by myself, which will benchmark virtual threads instead.\nHardware Setup All experiments were run on the following environment:\nCPU: Intel i7 8700k RAM: 16GB OS: Ubuntu Desktop 18.04 LTS The JVM is always invoked without any additional flags.\nJava Microbenchmark Harness (JMH) The Java Microbenchmark Harness is often abbreviated as JMH . It is a project of OpenJDK. They created it to benchmark JVMs. JVMs automatically make many optimizations to code. In general, this is very helpful, since it increases the performance. Unfortunately, it is not possible to turn off such optimizations. Therefore benchmarking Java programs is not an easy task.\nBenchmarks can be configured by using annotations. The most important annotations for this thesis are:\n@BenchmarkMode @OutputTimeUnit @State @Warmup @Measurement @Fork @BenchmarkMode defines what is measured. An example of that would be average time or throughput. @OutputTimeUnit defines how the measured results are returned. It can be any time unit, such as nanoseconds, seconds or minutes. Classes marked with the @State annotation are “instantiated on demand and will be reused during the entire benchmark trial” according to OpenJDK. This annotation is a bit more complex, but most of the time it is used in a very simple manner: Just the benchmark class itself will be annotated with it. Then the JMH can reference its own fields just like any other Java program. This is called the default state. Typically the JVM is running a couple of Warmup runs before starting the actual benchmark. This is to be configured using the @Warmup annotation. Very similar to that the @Measurement annotation is used to configure the actual benchmark runs. Both support arguments like iterations. Which will tell the JMH how often to warmup or measure. There are two Fork options:\n@Fork(0) = forking disabled @Fork(1) = forking enabled Per default, JMH forking is enabled. Forking means that the tests will run in separate processes. This is done to avoid JVM optimizations. Often @Fork(1) is still annotated regardless, just for the sake of clarity.\nOpenJDK’s Benchmarks The following benchmarks are made by Project Loom themselves using the JMH.\nThe benchmarks contains five different classes:\nFreeze Thaw FreezeAndThaw OneShot Oscillation All classes use the same settings:\n@BenchmarkMode(Mode.AverageTime) @OutputTimeUnit(TimeUnit.NANOSECONDS) @State(Scope.Thread) @Warmup(iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS) @Measurement(iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS) @Fork(1) They measure the average time and return the results in nanoseconds. They run five warmup iterations before measuring. Each measurement is ran five times. The state is the default state. Forking is enabled.\nEach class contains at least one function which is benchmarked multiple times using different parameters. All classes except for the Oscillation class use the same two parameters. Those parameters are called paramCount and stackDepth:\n@Param({\"1\", \"2\", \"3\"}) public int paramCount; @Param({\"5\", \"10\", \"20\", \"100\"}) public int stackDepth; Measurement series are run for every combination of those two parameters. Resulting in 12 different measurements. The parameters of the Oscillation class class are sligthly different:\n@Param({\"2\", \"3\", \"4\"}) public int minDepth; @Param({\"5\", \"6\", \"7\", \"8\"}) public int maxDepth; @Param({\"10\", \"100\", \"1000\"}) public int repeat; This results in 36 runs for the Oscillation class.\nAll classes use the same core task to benchmark. It is a simple recursive function. For example, if the stackDepth is 10, the function will call itself 10 times. Depending on the paramCount and which class is being benchmarked, the recursive function will then perform certain actions at a certain stackDepth.\nFreeze The Freeze benchmark optionally yields at a certain depth. It only measures yielding. An increase in the parameter count was expected to increase the time it takes to freeze since there are more frames to copy. This behavior can only be observed at a stack depth of 100. Measurement inaccuracies might be to blame for that. An increase in stack depth consistently increases the time it takes to finish the task, as expected.\nThaw Similar to how the Freeze benchmark only measures yielding, the Thaw benchmark only measures the reverse, which is continuing. The expectations here were the same as before: Both an increase in parameter count and an increase in stack depth should increase the time. This time around only at a stack depth of 5 the results were different from the expectations.\nFreezeAndThaw As the name implies FreezeAndThaw measures both freezing and thawing. It runs two different methods: baseline and yieldAndContinue. The difference between those two is, that the continuation of the baseline method doesn’t yield at the limit. Which means it doesn’t yield at all. It just completes the task. So one can compare the baseline and the yieldAndContinue run to see how strongly freezing and thawing affected the time to finish the task. As one can see in the figure below, the impact of yielding and continuing is big. Without it, the task is completed up to 10 times faster.\nOneShot The OneShot class has many functions. The names of those functions very clearly describe, what they do. The functions become increasingly more demanding. The first function just runs the method without yielding. The last one yields before and after each call.\nOne can also look at the individual functions in an isolated matter. One can immediately see that an increase in stack depth results in an increase in time. An increase in parameter count also seems to impact the time it takes to finish the task. One can observe it at a stack depth of 100.\nOscillation Here the continuation oscillates between a minimum and a maximum stack depth. It freezes at the maximum and continues afterwards. One expectation here was that an increase in repetitions increases the time it takes to complete the task. This expectation has proven to be true. Another expectation was, that the bigger the difference from the minimal depth to the maximum depth is, the more time it would take to finish the task. This has also proven to be true.\nEcho Server Three Java classes are used in this experiment: EchoServerThread, EchoServerVThread, and Responder. If you are curious about the coding details, you can check them out in the GitHub Repo.\nEchoServerThread and EchoServerVThread are almost identical. They both start a server that listens for requests on port 5566. Once a request is made, they start a separate thread, which runs the Responder class. Then they will continue to listen for further requests. The difference between the two classes is, that the EchoServerThread class utilizes kernel threads, while the EchoServerVThread class uses virtual threads. The Responder class sends a valid HTTP 1.1 header. Then it will echo the request it received.\nRequests will be made using ApacheBench. All requests will be made with a concurrency of 100. Instead of running ApacheBench once with for example 1.000.000 requests, a shell script will be used. The shell script will then instead call ApacheBench 100 times in a row, with each run making 10.000 requests. This is done to remove ApacheBench as a potential bottleneck since it might not be fit to run 1.000.000 requests in a single process.\nThere will be two different kinds of measurement series:\nThe first one is being recorded with VisualVM. The second one is recorded with JProfiler. There are multiple reasons for that. First off all a profiler might influence the measurements. Running similar tests with different profilers should improve the accuracy of the results. Also exporting the data is not possible using VisualVM. JProfiler allows one to export all measurements and model them as one wants to.\nResults - Profiler: VisualVM Virtual threads in orange and kernel threads in blue. We meassure Time to complete 1.000, 10.000, and 100.000. requests.\nLooking at the scatter plots, one can immediately see that the virtual threads seem to not only perform better but also much more consistent than the kernel threads. When trying to model the results with linear regression, the first impression is further verified: The graph trying to model the virtual thread results is very precise. The margin for error is very small. On the other hand, the graph trying to model the kernel threads has a much bigger margin for error. This proves to be true in all 3 measurement series.\nModeling the same results using boxplots allows one to take a closer look at the details. The 1000 requests series using virtual threads has a median of 30ms. The first and the third quartile are each 1ms above and below the median. Therefore 50% of all the runs were completed between 29ms and 31ms. The 1000 requests series using kernel threads has a median around 100ms. The first quartile is slightly below 50ms and the third quartile is slightly above 200ms. That results in a spread of more than 150ms in the 50% box.\nThe 10.000 requests series is more balanced: The 50% box of the virtual threads series spans from 279ms to 289ms, resulting in a span of 10ms. The median is at 284ms. The 50% box of the kernel threads series spans from 395ms to 430ms, resulting in a span of 35ms. The median is at 415ms. Any run that took longer than 480ms was considered an outlier for the kernel thread series and was not plotted. There are 10 such outliers.\nIn the 100.000 requests series, the 50% box of the virtual threads run is slightly bigger than 25ms. The 50% box of the kernel threads is slightly smaller than 20ms. Therefore this is the first series, in which the 50% box spans a smaller range for the kernel threads. When one looks at the corresponding scatter plot, one can see, that there are more than 20 out of 100 runs, in which kernel threads needed more than 5 seconds to finish a run. Also, there are around 10 more runs, which were finished in less than 4 seconds. These 30 runs were considered outliers. No such outliers can be observed in the virtual thread series. Therefore, virtual threads perform more consistently once again.\nResults - Profiler: JProfiler In the previous chapter the series with 1000 requests per run stuck out. Kernel threads performed especially more inconsistent in that series. Therefore the same experiment was run again here. Additionally, the number of runs was increased from 100 to 1000.\nLooking at the boxplots one can see, that the virtual threads performed similarly to before. The median is at 29ms, the 50% box spans from 28ms to 30ms. The median of the kernel thread series is slightly higher than before. The 50% box spans a range that is bigger than 100ms.\nAdditionally, the heap usage is monitored this time around. Virtual threads use less heap space. They also are more consistent in their heap usage.\nThe following series measures 10.000 requests, ran 500 times. Originally this series was intended to be ran 1000 times. That was not possible, because the Linux kernel consistently killed the EchoServerThread process at around 600 runs.\nVirtual threads perform better and more consistent once again. Most runs were finished faster than 300ms. Looking at the kernel threads, the majority of runs were finished around 500ms. There also is a significant minority of runs finishing around 800-900ms.\nThe median of the virtual thread series is 278ms. The median of the kernel thread series is 513ms. That is a decrease of more than 40%. Also, the virtual thread 50% box spans an area of 5ms here, while the kernel threads span an area of around 50ms.\nOnce again, the heap usage is significantly higher using kernel threads. The median of the kernel thread series is 200MB, while the virtual thread series has a median of 100MB.\nConclusion Our experiment was rather simple and tested on a single computer. This does not reflect Big-Data workloads well. Still, it gives us a first hint at the potential of virtual threads. Virtual threads beat the old implementation constantly and recently Oracle announced that virtual threads will come to Java 19. It will be very interesting to see, how quickly the popular Big-Data-Frameworks can update to Java 19 and how good the performance gains will be for real workloads. We will be able to get very precise results by comparing old versions of a Big-Data-Framework with new ones.\n","wordCount":"4236","inLanguage":"en","image":"https://huti26.github.io/loom.png","datePublished":"2022-05-29T00:00:00Z","dateModified":"2022-05-29T00:00:00Z","author":{"@type":"Person","name":"Hutan Baghery Moghaddam"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://huti26.github.io/posts/project-loom/project-loom/"},"publisher":{"@type":"Organization","name":"Hutan Baghery Moghaddam","logo":{"@type":"ImageObject","url":"https://huti26.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://huti26.github.io/ accesskey=h title="Hutan BM (Alt + H)">Hutan BM</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://huti26.github.io/links/ title=Links><span>Links</span></a></li><li><a href=https://huti26.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://huti26.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://huti26.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://huti26.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>OpenJDK's Project Loom: User-Level Threads in Java</h1><div class=post-meta><span title='2022-05-29 00:00:00 +0000 UTC'>May 29, 2022</span>&nbsp;·&nbsp;20 min&nbsp;·&nbsp;4236 words&nbsp;·&nbsp;Hutan Baghery Moghaddam</div></header><figure class=entry-cover><img loading=lazy src=https://huti26.github.io/loom.png alt></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#preface>Preface</a></li><li><a href=#background>Background</a><ul><li><a href=#process>Process</a></li><li><a href=#thread>Thread</a></li><li><a href=#virtual-threads>Virtual Threads</a></li></ul></li><li><a href=#continuation>Continuation</a><ul><li><a href=#stack>Stack</a></li><li><a href=#intrinsic-functions>Intrinsic Functions</a></li><li><a href=#run>Run</a></li><li><a href=#yield>Yield</a></li><li><a href=#continue>Continue</a></li></ul></li><li><a href=#evaluation>Evaluation</a><ul><li><a href=#hardware-setup>Hardware Setup</a></li><li><a href=#java-microbenchmark-harness-jmh>Java Microbenchmark Harness (JMH)</a></li><li><a href=#openjdks-benchmarks>OpenJDK&rsquo;s Benchmarks</a></li><li><a href=#oscillation>Oscillation</a></li><li><a href=#echo-server>Echo Server</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li></ul></nav></div></details></div><div class=post-content><h2 id=preface>Preface<a hidden class=anchor aria-hidden=true href=#preface>#</a></h2><p>Nowadays it is universally known that a Java thread corresponds to a Kernel-Level Thread when looking at it in a very simplified matter. Project Loom is trying to introduce User-Level Threads to the Java ecosystem. They call these User-Level Threads Virtual Threads. Virtual Threads are supossed to increase performance and be more ressource efficient than just using Kernel-Level Threads. It is planned, that Virtual Threads become a drop-in replacement, which would be a free performance boost for any multithreaded Java application. This is particularly interesting, because many Big-Data-Frameworks such as Hadoop or Spark are written in Java.</p><p>All my findings are from my Bachelor&rsquo;s Thesis, which I wrote during the first three months of 2020. I now recapitulate my findings, so beware, that some findings might already be outdated, since two years have passed. The build used to benchmark the code is: Build 15-loom+4-55, released on 22.02.2020.</p><p>If you are ealready familiar with the general concept of virtual threads, feel free to skip to the evaluation chapter as the chapters prior can be a bit detailed.</p><h2 id=background>Background<a hidden class=anchor aria-hidden=true href=#background>#</a></h2><p>We will start with processes and programms and work ourselves towards Virtual Threads from there.</p><h3 id=process>Process<a hidden class=anchor aria-hidden=true href=#process>#</a></h3><p>A process is a program that is being executed. In the following figure, the relation between a Process and a program is presented.</p><p><img loading=lazy src=../../../posts/project-loom/images/process-program.png alt></p><ul><li>The segment <em>text</em> contains the programcode.</li><li>The segment <em>initialized</em> datacontains initialized global and static variables.</li><li>The segment <em>bss</em> contains not yet initialized global variables and static variables.</li><li>The <em>heap</em> is the extension of the <em>bss</em>.</li><li>The <em>stack</em> is used for saving local variables, function parameters and memory areas of register contents.</li><li>Both the <em>stack</em> and the <em>heap</em> will be dynamically extended if needed.</li></ul><p>Processes allow a system to process several programs simultaneously. An active process can have three different statuses.</p><table><thead><tr><th style=text-align:center><img loading=lazy src=../../../posts/project-loom/images/prozess-status.png alt></th></tr></thead><tbody><tr><td style=text-align:center>Source: Andrew S. Tannenbaum, Modern Operating Systems</td></tr></tbody></table><p>The scheduler plays an important role here. A scheduler is responsible for when a process is processed by the CPU. In UNIX systems, for example, a round-robin scheduler is used. This changes the active process in a predetermined cycle, such as 10-100ms. An active process is a process with the status <em>Running</em>. The CPU is then assigned to the process and executes the program of the process. If the CPU is removed from the process by the scheduler, the process is in <em>Ready</em> status. The process is then ready to be reassigned to the CPU and is waiting for it. A process can also block. This happens if the process has to wait for a certain input. The process is then in <em>Blocked</em> status and will not change to <em>Ready</em> status until the input becomes available.</p><h3 id=thread>Thread<a hidden class=anchor aria-hidden=true href=#thread>#</a></h3><p>A process is heavy. Each process has its own address space and switching between processes takes a lot of time. Therefore threads were created as lightweight processes. Threads are part of a process and therefore they share the same address space. Switching between threads is a lot faster. The following diagram lists the unique items for threads and processes.</p><p><img loading=lazy src=../../../posts/project-loom/images/process-thread-items.png alt></p><p>Each process can have many threads. The following figure shows the hierarchy of processes and threads.</p><p><img loading=lazy src=../../../posts/project-loom/images/process-thread.png alt></p><p>In modern programming, the line between processes and threads gets somewhat blurred, because processes often start with a single thread. Standalone processes without any threads are a thing of the past.</p><p>Threads increase CPU efficiency. Whenever a thread has to block, for example, because it has to wait for I/O input, another thread of the same process can quickly continue and make use of the CPU. Constantly switching between processes would be very inefficient.</p><p>A distinction is made between two types of threads. Kernel-level threads, which are managed directly by the operating system&rsquo;s scheduler and User-level threads, where the programmer has to do the scheduling himself. Here is where Virtual Threads come into play.</p><h3 id=virtual-threads>Virtual Threads<a hidden class=anchor aria-hidden=true href=#virtual-threads>#</a></h3><p>Similar to how threads were created as lightweight processes, virtual threads were created as lightweight threads. Therefore they line up in the hierarchy right behind the kernel threads.</p><p><img loading=lazy src=../../../posts/project-loom/images/process-thread-vthread.png alt></p><p>Similar to how the scheduler of an operating system allocates CPU time to each kernel thread, the JVM allocates CPU time to each virtual thread.</p><p><img loading=lazy src=../../../posts/project-loom/images/thread-vthread-scheduler.png alt></p><p>Superficially, this is all there is to virtual threads. A lightweight thread, which is supossed to make switching even more efficient than switching between threads.</p><p>Underneath the hood, there is a concept, which the developers of Project Loom call <em>Continuation</em>. We will look at that now. This chapter will be rather low level and just about coding details, which we found, when analyzing the source code. If that&rsquo;s too detailed for you, skip ahead to the evaulation chapter, in which we run various benchmarks with virtual threads.</p><h2 id=continuation>Continuation<a hidden class=anchor aria-hidden=true href=#continuation>#</a></h2><p>In this chapter we will analyze concintuations conceptiually and especially how they are implemented by Project Loom on a code level.</p><p>A continuation is a sequence of instructions, that can be yielded and continued. The following example shows how a continuation can calculate the sum of the first five natural numbers while yielding after each increment.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl><span class=n>var</span> <span class=n>scope</span> <span class=o>=</span> <span class=k>new</span> <span class=n>ContinuationScope</span><span class=o>(</span><span class=s>&#34;ContinuationScope&#34;</span><span class=o>);</span>
</span></span><span class=line><span class=cl><span class=n>var</span> <span class=n>continuation</span> <span class=o>=</span> <span class=k>new</span> <span class=n>Continuation</span><span class=o>(</span><span class=n>scope</span><span class=o>,</span> <span class=o>()</span> <span class=o>-&gt;</span>  <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>int</span> <span class=n>n</span> <span class=o>=</span> <span class=n>0</span><span class=o>;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span><span class=o>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=n>0</span><span class=o>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>6</span><span class=o>;</span> <span class=n>i</span><span class=o>++)</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>        <span class=n>n</span> <span class=o>=</span> <span class=n>n</span> <span class=o>+</span> <span class=n>i</span><span class=o>;</span>
</span></span><span class=line><span class=cl>        <span class=n>System</span><span class=o>.</span><span class=na>out</span><span class=o>.</span><span class=na>println</span><span class=o>(</span><span class=s>&#34;i: &#34;</span> <span class=o>+</span> <span class=n>i</span> <span class=o>+</span> <span class=s>&#34;\t&#34;</span> <span class=o>+</span> <span class=s>&#34;n: &#34;</span> <span class=o>+</span> <span class=n>n</span><span class=o>);</span>
</span></span><span class=line><span class=cl>        <span class=n>Continuation</span><span class=o>.</span><span class=na>yield</span><span class=o>(</span><span class=n>scope</span><span class=o>);</span>
</span></span><span class=line><span class=cl>    <span class=o>}</span>
</span></span><span class=line><span class=cl><span class=o>});</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>while</span><span class=o>(!</span><span class=n>continuation</span><span class=o>.</span><span class=na>isDone</span><span class=o>())</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=n>System</span><span class=o>.</span><span class=na>out</span><span class=o>.</span><span class=na>println</span><span class=o>();</span>
</span></span><span class=line><span class=cl>    <span class=n>continuation</span><span class=o>.</span><span class=na>run</span><span class=o>();</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span></code></pre></div><p>A continuation requires two arguments: A scope and a runnable. First, a scope is created. Then the continuation is created. Afterwards, the continuation is repeatedly run in a loop until it is done.</p><p>The scope is used to allow continuations to be nested. Each continuation has exactly one parent continuation and exactly one child continuation. Continuations run on top of kernel-level Java threads, which are referred to as carrier-threads. Every Java carrier-thread has exactly one continuation as one of its attributes. This continuation is supposed to be the innermost one.</p><h3 id=stack>Stack<a hidden class=anchor aria-hidden=true href=#stack>#</a></h3><p>Each continuation has two stacks: One for objects and one for non-objects. Project Loom calls the continuation stack horizontal stack and the thread stack vertical stack. The abbreviated versions are called h-stack and v-stack.
When a continuation yields, it will be unmounted. Unmounting copies the continuation frames from the thread stack to the continuation stack. Afterwards, the continuation frames are removed from the thread stack. Project Loom calls this process freezing.
When a continuation starts or continues, it will be mounted. Mounting is the reverse process of unmounting. Therefore project Loom calls this process thawing.</p><table><thead><tr><th style=text-align:center><img loading=lazy src=../../../posts/project-loom/images/freeze-jvmls-2019.png alt></th></tr></thead><tbody><tr><td style=text-align:center>Source: <a href="https://www.youtube.com/watch?v=NV46KFV1m-4" target=_blank rel=noopener>https://www.youtube.com/watch?v=NV46KFV1m-4</a></td></tr></tbody></table><p>Looking at the previously mentioned two stacks on code level: One stack is an integer array for primitive values and metadata. The other one is an object array for references. All stack related methods exist twice: Once for the integer array and once for the object array. The methods are very similar. Therefore this thesis will only explain one set of them, which is the integer set.</p><p>Project Loom uses an integer sp as a stack pointer. Every time the stack is changed, the stack pointer will be updated. This is done by using the fixDecreasingIndexAfterResize method. As an example observe fixDecreasingIndexAfterResize using the arguments: index = 5, oldLength = 10, newLength = 20. It returns 15. The new stack pointer is therefore 15. That means that if a stack size gets changed, the remaining values are inserted starting at the end.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl><span class=kd>private</span> <span class=kt>int</span> <span class=n>sp</span> <span class=o>=</span> <span class=o>-</span><span class=n>1</span><span class=o>;</span> <span class=c1>// index into the h-stack
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=kd>private</span> <span class=kt>int</span> <span class=nf>fixDecreasingIndexAfterResize</span><span class=o>(</span><span class=kt>int</span> <span class=n>index</span><span class=o>,</span> <span class=kt>int</span> <span class=n>oldLength</span><span class=o>,</span> <span class=kt>int</span> <span class=n>newLength</span><span class=o>)</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>newLength</span> <span class=o>-</span> <span class=o>(</span><span class=n>oldLength</span> <span class=o>-</span> <span class=n>index</span><span class=o>);</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span></code></pre></div><p>There are 3 functions related to stack management:</p><ul><li><p><strong>getStack</strong>: The getStack method is used to expand the stack. First, the program will test, whether the stack is null. If that is the case, the stack has not been created yet. Then the program will create the stack and adjust the stack pointer. If the stack is not empty, the program will create a new stack, that is big enough to fit the old and the new frames. Then it will copy the old frames into the new stack. Afterwards, the continuation&rsquo;s stack is set to the new stack. The stack pointer will be updated. The return value is boolean. It is True when getStack succeded. The method fails when the old stack length is larger than the new one.</p></li><li><p><strong>resizeStack</strong>: Similar to how the getStack method expands the stack, the resizeStack method shrinks it. In contrast to the getStack method, the resizeStack method is a void and does not require to return a boolean on whether it succeeded or not.</p></li><li><p><strong>maybeShrink</strong>: The maybeShrink method is called after yielding. It checks whether the stack size is bigger than a watermark it keeps track of. If it is, the watermark will be set to the stack size. Therefore if maybeShrink is called without adjusting the watermark, that means, that the stack size has not increased since the last time. If maybeShrink is called ten times without adjusting the watermark, the resizeStack method will be called with the watermark as it&rsquo;s argument.</p></li></ul><p>Another interesting detail is how Project Loom solves critical sections. This is done with a classic semaphore design. The method pin increments the semaphore cs and the method unpin decrements it.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl><span class=kd>private</span> <span class=kt>short</span> <span class=n>cs</span><span class=o>;</span> <span class=c1>// critical section semaphore
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=kd>public</span> <span class=kd>static</span> <span class=kt>void</span> <span class=nf>pin</span><span class=o>()</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=n>Continuation</span> <span class=n>cont</span> <span class=o>=</span> <span class=n>currentCarrierThread</span><span class=o>().</span><span class=na>getContinuation</span><span class=o>();</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=o>(</span><span class=n>cont</span> <span class=o>!=</span> <span class=kc>null</span><span class=o>)</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=o>(</span><span class=n>cont</span><span class=o>.</span><span class=na>cs</span> <span class=o>==</span> <span class=n>Short</span><span class=o>.</span><span class=na>MAX_VALUE</span><span class=o>)</span>
</span></span><span class=line><span class=cl>            <span class=k>throw</span> <span class=k>new</span> <span class=n>IllegalStateException</span><span class=o>(</span><span class=s>&#34;Too many pins&#34;</span><span class=o>);</span>
</span></span><span class=line><span class=cl>        <span class=n>cont</span><span class=o>.</span><span class=na>cs</span><span class=o>++;</span>
</span></span><span class=line><span class=cl>    <span class=o>}</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kd>public</span> <span class=kd>static</span> <span class=kt>void</span> <span class=nf>unpin</span><span class=o>()</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=n>Continuation</span> <span class=n>cont</span> <span class=o>=</span> <span class=n>currentCarrierThread</span><span class=o>().</span><span class=na>getContinuation</span><span class=o>();</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=o>(</span><span class=n>cont</span> <span class=o>!=</span> <span class=kc>null</span><span class=o>)</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=o>(</span><span class=n>cont</span><span class=o>.</span><span class=na>cs</span> <span class=o>==</span> <span class=n>0</span><span class=o>)</span>
</span></span><span class=line><span class=cl>            <span class=k>throw</span> <span class=k>new</span> <span class=n>IllegalStateException</span><span class=o>(</span><span class=s>&#34;Not pinned&#34;</span><span class=o>);</span>
</span></span><span class=line><span class=cl>        <span class=n>cont</span><span class=o>.</span><span class=na>cs</span><span class=o>--;</span>
</span></span><span class=line><span class=cl>    <span class=o>}</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span></code></pre></div><h3 id=intrinsic-functions>Intrinsic Functions<a hidden class=anchor aria-hidden=true href=#intrinsic-functions>#</a></h3><p>Intrinsic Functions are not implemented in Java, but in native code instead. Project Loom uses intrinsics to increase performance. Usually, if there is an intrinsic version of a function, there will still be a non-intrinsic one. In this case, there are no non-intrinsic versions of the functions. The following functions are exclusively intrinsic and relevant for this thesis:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl><span class=nd>@HotSpotIntrinsicCandidate</span>
</span></span><span class=line><span class=cl><span class=kd>private</span> <span class=kd>static</span> <span class=kt>long</span> <span class=nf>getSP</span><span class=o>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@HotSpotIntrinsicCandidate</span>
</span></span><span class=line><span class=cl><span class=kd>private</span> <span class=kt>void</span> <span class=nf>doContinue</span><span class=o>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@HotSpotIntrinsicCandidate</span>
</span></span><span class=line><span class=cl><span class=kd>private</span> <span class=kd>static</span> <span class=kt>int</span> <span class=nf>doYield</span><span class=o>(</span><span class=kt>int</span> <span class=n>scopes</span><span class=o>)</span>
</span></span></code></pre></div><p>For the x86 platform the following functions in <code>stubGenerator_x86_64.cpp</code> create the bytecode for the intrinsic versions of the functions named above:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>address</span> <span class=n>generate_cont_getSP</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>address</span> <span class=n>generate_cont_thaw</span><span class=p>(</span><span class=kt>bool</span> <span class=n>return_barrier</span><span class=p>,</span> <span class=kt>bool</span> <span class=n>exception</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>RuntimeStub</span> <span class=o>*</span><span class=n>generate_cont_doYield</span><span class=p>()</span>
</span></span></code></pre></div><p>In these functions, the maintainers of project Loom use their macroassembler. The macroassembler class can be found in <code>src/hotspot/cpu/x86/macroAssembler_x86.cpp</code>. The yield method also utilizes two other big classes. One is a codebuffer located in <code>src/hotspot/cpu/x86/asm/codeBuffer.cpp</code> and the other is a oopmap, used for garbage collection, located in <code>src/hotspot/share/compiler/oopMap.cpp</code>.</p><p>Analyzing these functions was not possible within the timeframe of my Bachelor&rsquo;s Thesis, but if you are curious about even more details, these are the places to look for.</p><h3 id=run>Run<a hidden class=anchor aria-hidden=true href=#run>#</a></h3><p>When a continuation gets started or continued the run method is called.
First the run method mounts the continuation. Mounting is realized using a VarHandle, which atomically sets a boolean to true.</p><p>Afterwards the current carrier-thread&rsquo;s continuation has to be updated. As an example the current carrier-thread&rsquo;s continuation before the update will be called bar. The new continuation will be called foo.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl><span class=n>Thread</span> <span class=n>t</span> <span class=o>=</span> <span class=n>currentCarrierThread</span><span class=o>();</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=o>(</span><span class=n>parent</span> <span class=o>!=</span> <span class=kc>null</span><span class=o>)</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=o>(</span><span class=n>parent</span> <span class=o>!=</span> <span class=n>t</span><span class=o>.</span><span class=na>getContinuation</span><span class=o>())</span>
</span></span><span class=line><span class=cl>        <span class=k>throw</span> <span class=k>new</span> <span class=n>IllegalStateException</span><span class=o>();</span>
</span></span><span class=line><span class=cl><span class=o>}</span> <span class=k>else</span>
</span></span><span class=line><span class=cl>    <span class=k>this</span><span class=o>.</span><span class=na>parent</span> <span class=o>=</span> <span class=n>t</span><span class=o>.</span><span class=na>getContinuation</span><span class=o>();</span>
</span></span><span class=line><span class=cl><span class=n>t</span><span class=o>.</span><span class=na>setContinuation</span><span class=o>(</span><span class=k>this</span><span class=o>);</span>
</span></span></code></pre></div><p>foo is not supossed to have a parent at this point of time. If foo has a parent, that parent has to be bar. If it is not, something seriously went wrong and an error is thrown. The expected outcome is, that foo has no parent. In that case foo will become the child bar. Afterwards foo will become the current carrier-threads&rsquo;s continuation.</p><table><thead><tr><th style=text-align:center><img loading=lazy src=../../../posts/project-loom/images/before-run-changes-current-continuation.png alt></th><th style=text-align:center><img loading=lazy src=../../../posts/project-loom/images/after-run-changes-current-continuation.png alt></th></tr></thead><tbody><tr><td style=text-align:center>Before Carrier-Thread&rsquo;s Continuation gets updated</td><td style=text-align:center>After Carrier-Thread&rsquo;s Continuation gets updated</td></tr></tbody></table><p>After that, the run method will call either the enter method or the continue method depending on whether foo is being started for the first time.</p><p>Once foo is done, it returns to the run method. The current carrier-thread&rsquo;s continuation will be set to bar. foo will be removed as a child of bar. Finally, foo will be unmounted, similar to how it was mounted at the beginning.</p><h3 id=yield>Yield<a hidden class=anchor aria-hidden=true href=#yield>#</a></h3><p>Project Loom uses three different yield functions, which are always called in the same order.</p><p>First <code>public static boolean yield(ContinuationScope scope)</code> is called. This method checks, if the current carrier-thread&rsquo;s continuation is part of the given ContinuationScope scope. This is done by looping a variable c through the parents of the current carrier-thread&rsquo;s continuation. The loop stops once c is either null or the scope of c is the same as the given scope. If c ends up being null, that means, that the program is being told to yield a ContinuationScope, while it is running a continuation, that is not part of that scope. This is undesired behavior and throws an exception. If c is not null, the program can continue and the second function will be called.</p><p>The other two functions were beyond the scope of my thesis, because they either are completely intrinsic or because they utilize variables that are returned from intrinsics. This is once again a nice spot to continue research, if you are curious about Project Loom.</p><h3 id=continue>Continue<a hidden class=anchor aria-hidden=true href=#continue>#</a></h3><p>The continue method is entirely intrinsic. Therefore it was beyond the scope of my thesis.</p><h2 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h2><p>We will run two different sets of experiments. The first one is provided by Project Loom themselves. Those experiments focus on analyzing performance of the Continuation class and its components. The second one is a simple experiment made by myself, which will benchmark virtual threads instead.</p><h3 id=hardware-setup>Hardware Setup<a hidden class=anchor aria-hidden=true href=#hardware-setup>#</a></h3><p>All experiments were run on the following environment:</p><ul><li>CPU: Intel i7 8700k</li><li>RAM: 16GB</li><li>OS: Ubuntu Desktop 18.04 LTS</li></ul><p>The JVM is always invoked without any additional flags.</p><h3 id=java-microbenchmark-harness-jmh>Java Microbenchmark Harness (JMH)<a hidden class=anchor aria-hidden=true href=#java-microbenchmark-harness-jmh>#</a></h3><p>The Java Microbenchmark Harness is often abbreviated as <a href=https://github.com/openjdk/jmh target=_blank rel=noopener>JMH</a> . It is a project of OpenJDK. They created it to benchmark JVMs. JVMs automatically make many optimizations to code. In general, this is very helpful, since it increases the performance. Unfortunately, it is not possible to turn off such optimizations. Therefore benchmarking Java programs is not an easy task.</p><p>Benchmarks can be configured by using annotations. The most important annotations for this thesis are:</p><ul><li><code>@BenchmarkMode</code></li><li><code>@OutputTimeUnit</code></li><li><code>@State</code></li><li><code>@Warmup</code></li><li><code>@Measurement</code></li><li><code>@Fork</code></li></ul><p>@BenchmarkMode defines what is measured. An example of that would be average time or throughput. @OutputTimeUnit defines how the measured results are returned. It can be any time unit, such as nanoseconds, seconds or minutes. Classes marked with the @State annotation are &ldquo;instantiated on demand and will be reused during the entire benchmark trial&rdquo; according to OpenJDK. This annotation is a bit more complex, but most of the time it is used in a very simple manner: Just the benchmark class itself will be annotated with it. Then the JMH can reference its own fields just like any other Java program. This is called the default state.
Typically the JVM is running a couple of Warmup runs before starting the actual benchmark. This is to be configured using the @Warmup annotation. Very similar to that the @Measurement annotation is used to configure the actual benchmark runs. Both support arguments like iterations. Which will tell the JMH how often to warmup or measure. There are two Fork options:</p><ul><li><code>@Fork(0) = forking disabled</code></li><li><code>@Fork(1) = forking enabled</code></li></ul><p>Per default, JMH forking is enabled. Forking means that the tests will run in separate processes. This is done to avoid JVM optimizations. Often @Fork(1) is still annotated regardless, just for the sake of clarity.</p><h3 id=openjdks-benchmarks>OpenJDK&rsquo;s Benchmarks<a hidden class=anchor aria-hidden=true href=#openjdks-benchmarks>#</a></h3><p>The following benchmarks are made by Project Loom themselves using the JMH.</p><p>The benchmarks contains five different classes:</p><ul><li>Freeze</li><li>Thaw</li><li>FreezeAndThaw</li><li>OneShot</li><li>Oscillation</li></ul><p>All classes use the same settings:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl><span class=nd>@BenchmarkMode</span><span class=o>(</span><span class=n>Mode</span><span class=o>.</span><span class=na>AverageTime</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=nd>@OutputTimeUnit</span><span class=o>(</span><span class=n>TimeUnit</span><span class=o>.</span><span class=na>NANOSECONDS</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=nd>@State</span><span class=o>(</span><span class=n>Scope</span><span class=o>.</span><span class=na>Thread</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=nd>@Warmup</span><span class=o>(</span><span class=n>iterations</span> <span class=o>=</span> <span class=n>5</span><span class=o>,</span> <span class=n>time</span> <span class=o>=</span> <span class=n>1</span><span class=o>,</span> <span class=n>timeUnit</span> <span class=o>=</span> <span class=n>TimeUnit</span><span class=o>.</span><span class=na>SECONDS</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=nd>@Measurement</span><span class=o>(</span><span class=n>iterations</span> <span class=o>=</span> <span class=n>5</span><span class=o>,</span> <span class=n>time</span> <span class=o>=</span> <span class=n>1</span><span class=o>,</span> <span class=n>timeUnit</span> <span class=o>=</span> <span class=n>TimeUnit</span><span class=o>.</span><span class=na>SECONDS</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=nd>@Fork</span><span class=o>(</span><span class=n>1</span><span class=o>)</span>
</span></span></code></pre></div><p>They measure the average time and return the results in nanoseconds. They run five warmup iterations before measuring. Each measurement is ran five times. The state is the default state. Forking is enabled.</p><p>Each class contains at least one function which is benchmarked multiple times using different parameters.
All classes except for the Oscillation class use the same two parameters. Those parameters are called paramCount and stackDepth:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl><span class=nd>@Param</span><span class=o>({</span><span class=s>&#34;1&#34;</span><span class=o>,</span> <span class=s>&#34;2&#34;</span><span class=o>,</span> <span class=s>&#34;3&#34;</span><span class=o>})</span>
</span></span><span class=line><span class=cl><span class=kd>public</span> <span class=kt>int</span> <span class=n>paramCount</span><span class=o>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@Param</span><span class=o>({</span><span class=s>&#34;5&#34;</span><span class=o>,</span> <span class=s>&#34;10&#34;</span><span class=o>,</span> <span class=s>&#34;20&#34;</span><span class=o>,</span> <span class=s>&#34;100&#34;</span><span class=o>})</span>
</span></span><span class=line><span class=cl><span class=kd>public</span> <span class=kt>int</span> <span class=n>stackDepth</span><span class=o>;</span>
</span></span></code></pre></div><p>Measurement series are run for every combination of those two parameters. Resulting in 12 different measurements.
The parameters of the Oscillation class class are sligthly different:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-java data-lang=java><span class=line><span class=cl><span class=nd>@Param</span><span class=o>({</span><span class=s>&#34;2&#34;</span><span class=o>,</span> <span class=s>&#34;3&#34;</span><span class=o>,</span> <span class=s>&#34;4&#34;</span><span class=o>})</span>
</span></span><span class=line><span class=cl><span class=kd>public</span> <span class=kt>int</span> <span class=n>minDepth</span><span class=o>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@Param</span><span class=o>({</span><span class=s>&#34;5&#34;</span><span class=o>,</span> <span class=s>&#34;6&#34;</span><span class=o>,</span> <span class=s>&#34;7&#34;</span><span class=o>,</span> <span class=s>&#34;8&#34;</span><span class=o>})</span>
</span></span><span class=line><span class=cl><span class=kd>public</span> <span class=kt>int</span> <span class=n>maxDepth</span><span class=o>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@Param</span><span class=o>({</span><span class=s>&#34;10&#34;</span><span class=o>,</span> <span class=s>&#34;100&#34;</span><span class=o>,</span> <span class=s>&#34;1000&#34;</span><span class=o>})</span>
</span></span><span class=line><span class=cl><span class=kd>public</span> <span class=kt>int</span> <span class=n>repeat</span><span class=o>;</span>
</span></span></code></pre></div><p>This results in 36 runs for the Oscillation class.</p><p>All classes use the same core task to benchmark. It is a simple recursive function. For example, if the stackDepth is 10, the function will call itself 10 times. Depending on the paramCount and which class is being benchmarked, the recursive function will then perform certain actions at a certain stackDepth.</p><h4 id=freeze>Freeze<a hidden class=anchor aria-hidden=true href=#freeze>#</a></h4><p>The Freeze benchmark optionally yields at a certain depth. It only measures yielding. An increase in the parameter count was expected to increase the time it takes to freeze since there are more frames to copy. This behavior can only be observed at a stack depth of 100. Measurement inaccuracies might be to blame for that. An increase in stack depth consistently increases the time it takes to finish the task, as expected.</p><p><img loading=lazy src=../../../posts/project-loom/images/Freeze.png alt></p><h4 id=thaw>Thaw<a hidden class=anchor aria-hidden=true href=#thaw>#</a></h4><p>Similar to how the Freeze benchmark only measures yielding, the Thaw benchmark only measures the reverse, which is continuing. The expectations here were the same as before: Both an increase in parameter count and an increase in stack depth should increase the time. This time around only at a stack depth of 5 the results were different from the expectations.</p><p><img loading=lazy src=../../../posts/project-loom/images/Thaw.png alt></p><h4 id=freezeandthaw>FreezeAndThaw<a hidden class=anchor aria-hidden=true href=#freezeandthaw>#</a></h4><p>As the name implies FreezeAndThaw measures both freezing and thawing. It runs two different methods: baseline and yieldAndContinue. The difference between those two is, that the continuation of the baseline method doesn&rsquo;t yield at the limit. Which means it doesn&rsquo;t yield at all. It just completes the task. So one can compare the baseline and the yieldAndContinue run to see how strongly freezing and thawing affected the time to finish the task. As one can see in the figure below, the impact of yielding and continuing is big. Without it, the task is completed up to 10 times faster.</p><p><img loading=lazy src=../../../posts/project-loom/images/FreezeAndThaw.png alt></p><h4 id=oneshot>OneShot<a hidden class=anchor aria-hidden=true href=#oneshot>#</a></h4><p>The OneShot class has many functions. The names of those functions very clearly describe, what they do. The functions become increasingly more demanding. The first function just runs the method without yielding. The last one yields before and after each call.</p><p><img loading=lazy src=../../../posts/project-loom/images/OneShot.png alt></p><p>One can also look at the individual functions in an isolated matter. One can immediately see that an increase in stack depth results in an increase in time. An increase in parameter count also seems to impact the time it takes to finish the task. One can observe it at a stack depth of 100.</p><p><img loading=lazy src=../../../posts/project-loom/images/OneShot2.png alt></p><h3 id=oscillation>Oscillation<a hidden class=anchor aria-hidden=true href=#oscillation>#</a></h3><p>Here the continuation oscillates between a minimum and a maximum stack depth. It freezes at the maximum and continues afterwards.
One expectation here was that an increase in repetitions increases the time it takes to complete the task. This expectation has proven to be true. Another expectation was, that the bigger the difference from the minimal depth to the maximum depth is, the more time it would take to finish the task. This has also proven to be true.</p><p><img loading=lazy src=../../../posts/project-loom/images/Oscillation.png alt></p><h3 id=echo-server>Echo Server<a hidden class=anchor aria-hidden=true href=#echo-server>#</a></h3><p>Three Java classes are used in this experiment: EchoServerThread, EchoServerVThread, and Responder. If you are curious about the coding details, you can check them out in the <a href=https://github.com/huti26/project-loom-analysation/tree/master/Benchmarks/Footprint target=_blank rel=noopener>GitHub Repo</a>.</p><p>EchoServerThread and EchoServerVThread are almost identical. They both start a server that listens for requests on port 5566. Once a request is made, they start a separate thread, which runs the Responder class. Then they will continue to listen for further requests. The difference between the two classes is, that the EchoServerThread class utilizes kernel threads, while the EchoServerVThread class uses virtual threads. The Responder class sends a valid HTTP 1.1 header. Then it will echo the request it received.</p><p>Requests will be made using <a href=https://httpd.apache.org/docs/2.4/programs/ab.html target=_blank rel=noopener>ApacheBench</a>. All requests will be made with a concurrency of 100. Instead of running ApacheBench once with for example 1.000.000 requests, a shell script will be used. The shell script will then instead call ApacheBench 100 times in a row, with each run making 10.000 requests. This is done to remove ApacheBench as a potential bottleneck since it might not be fit to run 1.000.000 requests in a single process.</p><p>There will be two different kinds of measurement series:</p><ul><li>The first one is being recorded with VisualVM.</li><li>The second one is recorded with JProfiler.</li></ul><p>There are multiple reasons for that. First off all a profiler might influence the measurements. Running similar tests with different profilers should improve the accuracy of the results. Also exporting the data is not possible using VisualVM. JProfiler allows one to export all measurements and model them as one wants to.</p><h4 id=results---profiler-visualvm>Results - Profiler: VisualVM<a hidden class=anchor aria-hidden=true href=#results---profiler-visualvm>#</a></h4><p>Virtual threads in orange and kernel threads in blue. We meassure Time to complete 1.000, 10.000, and 100.000. requests.</p><p>Looking at the scatter plots, one can immediately see that the virtual threads seem to not only perform better but also much more consistent than the kernel threads. When trying to model the results with linear regression, the first impression is further verified: The graph trying to model the virtual thread results is very precise. The margin for error is very small. On the other hand, the graph trying to model the kernel threads has a much bigger margin for error. This proves to be true in all 3 measurement series.</p><p><img loading=lazy src=../../../posts/project-loom/images/scatter-100.png alt></p><p><img loading=lazy src=../../../posts/project-loom/images/linres-100.png alt></p><p>Modeling the same results using boxplots allows one to take a closer look at the details. The 1000 requests series using virtual threads has a median of 30ms. The first and the third quartile are each 1ms above and below the median. Therefore 50% of all the runs were completed between 29ms and 31ms. The 1000 requests series using kernel threads has a median around 100ms. The first quartile is slightly below 50ms and the third quartile is slightly above 200ms. That results in a spread of more than 150ms in the 50% box.</p><p>The 10.000 requests series is more balanced: The 50% box of the virtual threads series spans from 279ms to 289ms, resulting in a span of 10ms. The median is at 284ms. The 50% box of the kernel threads series spans from 395ms to 430ms, resulting in a span of 35ms. The median is at 415ms. Any run that took longer than 480ms was considered an outlier for the kernel thread series and was not plotted. There are 10 such outliers.</p><p>In the 100.000 requests series, the 50% box of the virtual threads run is slightly bigger than 25ms. The 50% box of the kernel threads is slightly smaller than 20ms. Therefore this is the first series, in which the 50% box spans a smaller range for the kernel threads. When one looks at the corresponding scatter plot, one can see, that there are more than 20 out of 100 runs, in which kernel threads needed more than 5 seconds to finish a run. Also, there are around 10 more runs, which were finished in less than 4 seconds. These 30 runs were considered outliers. No such outliers can be observed in the virtual thread series. Therefore, virtual threads perform more consistently once again.</p><p><img loading=lazy src=../../../posts/project-loom/images/boxplots-t-100.png alt></p><p><img loading=lazy src=../../../posts/project-loom/images/boxplots-vt-100.png alt></p><h4 id=results---profiler-jprofiler>Results - Profiler: JProfiler<a hidden class=anchor aria-hidden=true href=#results---profiler-jprofiler>#</a></h4><p>In the previous chapter the series with 1000 requests per run stuck out. Kernel threads performed especially more inconsistent in that series. Therefore the same experiment was run again here. Additionally, the number of runs was increased from 100 to 1000.</p><table><thead><tr><th style=text-align:center><img loading=lazy src=../../../posts/project-loom/images/scatter-1000.png alt></th><th style=text-align:center><img loading=lazy src=../../../posts/project-loom/images/linres-1000.png alt></th></tr></thead></table><p>Looking at the boxplots one can see, that the virtual threads performed similarly to before. The median is at 29ms, the 50% box spans from 28ms to 30ms. The median of the kernel thread series is slightly higher than before. The 50% box spans a range that is bigger than 100ms.</p><table><thead><tr><th style=text-align:center><img loading=lazy src=../../../posts/project-loom/images/boxplots-t-1000.png alt></th><th style=text-align:center><img loading=lazy src=../../../posts/project-loom/images/boxplots-vt-1000.png alt></th></tr></thead></table><p>Additionally, the heap usage is monitored this time around. Virtual threads use less heap space. They also are more consistent in their heap usage.</p><table><thead><tr><th style=text-align:center><img loading=lazy src=../../../posts/project-loom/images/heap_1000_line.png alt></th><th style=text-align:center><img loading=lazy src=../../../posts/project-loom/images/heap_1000_box.png alt></th></tr></thead></table><p>The following series measures 10.000 requests, ran 500 times. Originally this series was intended to be ran 1000 times. That was not possible, because the Linux kernel consistently killed the EchoServerThread process at around 600 runs.</p><p>Virtual threads perform better and more consistent once again. Most runs were finished faster than 300ms. Looking at the kernel threads, the majority of runs were finished around 500ms. There also is a significant minority of runs finishing around 800-900ms.</p><table><thead><tr><th style=text-align:center><img loading=lazy src=../../../posts/project-loom/images/scatter-500.png alt></th><th style=text-align:center><img loading=lazy src=../../../posts/project-loom/images/linres-500.png alt></th></tr></thead></table><p>The median of the virtual thread series is 278ms. The median of the kernel thread series is 513ms. That is a decrease of more than 40%. Also, the virtual thread 50% box spans an area of 5ms here, while the kernel threads span an area of around 50ms.</p><table><thead><tr><th style=text-align:center><img loading=lazy src=../../../posts/project-loom/images/boxplots-t-500.png alt></th><th style=text-align:center><img loading=lazy src=../../../posts/project-loom/images/boxplots-vt-500.png alt></th></tr></thead></table><p>Once again, the heap usage is significantly higher using kernel threads. The median of the kernel thread series is 200MB, while the virtual thread series has a median of 100MB.</p><table><thead><tr><th style=text-align:center><img loading=lazy src=../../../posts/project-loom/images/heap_500_line.png alt></th><th style=text-align:center><img loading=lazy src=../../../posts/project-loom/images/heap_500_box.png alt></th></tr></thead></table><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Our experiment was rather simple and tested on a single computer. This does not reflect Big-Data workloads well. Still, it gives us a first hint at the potential of virtual threads. Virtual threads beat the old implementation constantly and recently Oracle announced that <a href="https://blogs.oracle.com/javamagazine/post/java-loom-virtual-threads-platform-threads#:~:text=Virtual%20threads%20are%20fully%20integrated,or%20blocks%20while%20being%20pinned." target=_blank rel=noopener>virtual threads will come to Java 19</a>. It will be very interesting to see, how quickly the popular Big-Data-Frameworks can update to Java 19 and how good the performance gains will be for real workloads. We will be able to get very precise results by comparing old versions of a Big-Data-Framework with new ones.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://huti26.github.io/tags/java/>Java</a></li><li><a href=https://huti26.github.io/tags/openjdk/>OpenJDK</a></li></ul><nav class=paginav><a class=prev href=https://huti26.github.io/posts/discordbot/discordbot/><span class=title>« Prev</span><br><span>Creating a Multi-Process Python Bot to serve Pixiv Images with discord.py and SQLite</span></a>
<a class=next href=https://huti26.github.io/posts/flutter-calorietracker/flutter-calorietracker/><span class=title>Next »</span><br><span>Building a Calorietracker WebApp with Flutter and Firebase</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://huti26.github.io/>Hutan Baghery Moghaddam</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>