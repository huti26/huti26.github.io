[{"content":"Preface This preface is for people who don\u0026rsquo;t know Teamfight Tactics.\nTeamfight Tactics is a board game, in which you buy champions from a shop and place them on the board. There are 5 different tiers of champions: common, uncommon, rare, epic, and legendary. The champions of each tier cost 1, 2, 3, 4, and 5 gold respectively. With your board, you fight other people and the loser of the fight loses Health Points. Everyone starts with 100 Health Points and the last player to live wins.\nEconomy is a key aspect of TFT. You start with 0 gold, but you gain gold for a variety of reasons. To upgrade your board you have to invest that gold. You can spend gold to level up, which does two things:\nIt increases the number of champions you can place on your board. It improves your chances to hit champions of higher tiers. Additionally, you can reroll the champions offered in the shop. This costs you 2 gold and gives you 5 new champions to buy. Often, players will save up a bunch of gold and then invest it all at once, with the intention to roll specific champions. We are trying to simulate this kind of scenario with our code.\nIt is to be noted, that there is a limited amount of champions available. For example, all legendary units only exist 10 times. So if other players already bought your desired legendary 5 times, your chance of finding it is halved.\nLet\u0026rsquo;s conclude this introduction with a screenshot of what Teamfight Tactics looks like during a game.\nIn the bottom middle, you can see the shop, where currently 4 champions are offered, as the fifth one has been bought already. On top of the shop, you can see, that the player currently has 10 gold. On the very right side, you can see the Health Points remaining for each player. There are many more aspects to Teamfight Tactics, but they are not required for an understanding of this article. Regardless, I highly suggest you try playing the game!\nWhat are we trying to create Often in Teamfight Tactics players will do something called a roll down. The players will try improve their board state by investing a certain amount of gold. We are creating two different solutions here to help players make decisions.\nOne is called Calculator. The name stems from the fact, that we use precise Math to calculate the probability for a specific scenario. In this scenario the player is looking for at least one copy of a single champion while rolling down and nothing else. This actually occurs very frequently and is useful. The Calculator looks as follows.\nThe player inserts his level, gold and the tier of the champion he is trying to hit. Addtionally, he can add how many times his desired champion has already been bought by others and how many champions of the same tier have already been picked already. The player can test with the last two values, wheter he should let himself get influenced by other players potentially buying the champion before he does it or not. Also, there is a dragon toggle. Dragons are part of the new Patch and twice as costy as other units, not an important detail here, but it sligthly changes the results.\nThe other one is called Simulator. The simulator approximates probabilites for more complex scenarios by using Monte-Carlo-Simulation. The Simulator looks as follows.\nJust like before the player inserts his level and gold. Now he can search for multple champions, not just a single one as before. In the green fields he can increase the amount of times he wants to get a certain champion. With the red field he can insert how many times the champion has already been bought by himself or others. With this, the player can analyse much more complex scenarios than with the Calculator.\nCode Structure .\r├── __init__.py\r├── app.py\r├── autochess\r│ ├── __init__.py\r│ ├── ac_calculator.py\r│ ├── ac_data.py\r│ ├── ac_json.py\r│ ├── ac_simulation.py\r│ ├── ac_simulation_analyzer.py\r│ └── data.ini\r├── Procfile\r├── README.md\r├── requirements.txt\r├── static\r│ ├── custom.css\r│ ├── main.js\r│ ├── plot4.png\r│ └── tft\r│ ├── TFT7_Aatrox.png\r│ └── ...\r│ └── TFT7_Zoe.png\r└── templates\r├── ac_calculator.html\r├── ac_calculator_explanation.html\r├── ac_home.html\r├── ac_simulator.html\r├── ac_simulator_explanation.html\r└── layout\r├── base.html\r├── footer.html\r└── navbar.html Out backend logic is inside /autochess. Autochess is the general term for games like Teamfight Tactics. Our Frontend is made with Jinja2 and everything related to that is in the /templates folder. Additional web-design components are inside /static. We also have a Procfile, which we need for deployment with Heroku, but we will see that later on.\nFrontend I am not a Webdesigner and the frontend is more of a means to an end than anything else. Of course, it should still be decently looking and functional. We achieve this by using Jinja2 templates and Bootstrap. The most important pages are the Calculator and Simulator, which we have already seen. Both pages contain form data, which the user needs to input. Additionally, we have 3 sites that contain further information on the project. These sites only contain text. If you are curious about details on the frontend, you can check out the GitHub repository linked at the end. We will focus on the backend here.\nBackend For our backend we use Flask. We define our site as follows.\n@app.route(\u0026#39;/calculator\u0026#39;) def calculator(): return render_template( \u0026#39;ac_calculator.html\u0026#39; ) @app.route(\u0026#39;/simulator\u0026#39;) def simulator(): return render_template( \u0026#39;ac_simulator.html\u0026#39;, game_data=tft_game_data ) @app.route(\u0026#39;/api/tft/simulation\u0026#39;, methods=[\u0026#39;POST\u0026#39;]) def api_tft_simulation(): data = request.json ... return {\u0026#39;chance\u0026#39;: result} @app.route(\u0026#39;/api/tft/calculation\u0026#39;, methods=[\u0026#39;POST\u0026#39;]) def api_tft_calculation(): data = request.json ... return {\u0026#39;chance\u0026#39;: result} Our user operates on the sites /calculator and /simulator. These sites then collect the data inside the forms and transmit it to the respective /api endpoints. These endpoints then process the user input and eventually return a calculated value called result.\nThis transmitting step is realized with Javascript. I rarely use Javascript, so don\u0026rsquo;t blindly copy this piece of code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 async function submitForSimulation() { let data = {}; $(\u0026#34;#ac-calculate-form\u0026#34;) .serializeArray() .map(function (x) { data[x.name] = x.value; }); data = JSON.stringify(data); document.getElementById(\u0026#34;ac-calculated-chance\u0026#34;).innerHTML = \u0026#39;\u0026lt;div class=\u0026#34;spinner-border\u0026#34; role=\u0026#34;status\u0026#34; id=\u0026#34;ac-calculated-chance\u0026#34;\u0026gt;\u0026lt;/div\u0026gt;\\n\u0026#39;; let api_address = \u0026#34;https://hutan-tft-calculator.herokuapp.com/api/tft/simulation\u0026#34;; if (location.hostname === \u0026#34;localhost\u0026#34; || location.hostname === \u0026#34;127.0.0.1\u0026#34;) { api_address = \u0026#34;http://127.0.0.1:5000/api/tft/simulation\u0026#34;; } const response = await fetch(api_address, { method: \u0026#34;post\u0026#34;, headers: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; }, body: data, }); if (!response.ok) { const message = `An error has occured: ${response.status}`; throw new Error(message); } const calculatedResult = await response.json(); document.getElementById(\u0026#34;ac-calculated-chance\u0026#34;).innerHTML = calculatedResult[\u0026#34;chance\u0026#34;]; return false; } At the start, we collect all fields inside our form. This is done with jquery. All our fields need to have a unique name for this to work. In rows 11-12 we insert a loading indicator, so that our users get some feedback, that a calculation is ongoing. Depending on the number of simulations, the task can take 1 or 2 seconds. In row 21 we post our collected data to the API. To guarantee, that we can use this script both during deployment and local development, we dynamically adjust the api_address before that. In row 27 we check if an error occurred. If so, we give the user feedback. If everything works as intended, we remove the loading indicator and insert the result value into the page.\nNow, we can see what happens once the data has been posted to our API endpoint. First, let\u0026rsquo;s look at the simulator.\nSimulator For our simulations, we need to maintain a board state. The initial board state is created based on the user input. This is done with ac_json.py. Our user input will look as follows.\ndata = { \u0026#39;Yasuo\u0026#39;: 1, \u0026#39;Yasuo-taken\u0026#39;: 3 } For each champion, there will be a value of how many copies the user wants and how many are already taken. We split this into two dictionaries.\nIn one, we maintain how many copies of each champion the user still wants. Whenever we buy one copy, we decrement that value from this dictionary.\nIn the second dictionary, we maintain how many copies of each champion are bought out. This is because in Teamfight Tactics there is a limited amount of copies of each champion, so buying them changes the probability of hitting another copy. So basically instead of maintaining the board state, we do the inverse and maintain the champion pool.\nOnce we have prepared our initial data, we pass it to the ACSimulationAnalyzer.\nresult = ACSimulationAnalyzer( player_level=player_level, gold=gold, desired_champ_pool=desired_champ_pool, starting_champ_pool=starting_champ_pool, n=1000, game_data=tft_game_data ).analyze() player_level, gold, desired_champ_pool, starting_champ_pool are all dependent on user input. n is the number of simulations to run and we use 1000 here. How many iterations are to be run during Monte-Carlo-Simulation is a big topic and there are entire papers on it. 1000 offers us a good balance between precision and calculation time here based on simply trying it out. The last variable is game_data. Teamfight Tactics is an ever-changing game. We have a modular design, that can deal with any kind of situation. We can reuse the same code for different sets (Yearly big overhaul of Teamfight Tactics) and different patches (Small frequent number changes). For that, we store our data inside /autochess/data.ini. Inside it, we can have the data for multiple sets and patches and dynamically change between them, if we want to. Next, let\u0026rsquo;s look at the analyze() function in ACSimulationAnalyzer.\ndef analyze(self):\rfor _ in range(self.n):\rhit_everything = ACSimulation(player_level=self.player_level,\rgold=self.gold,\rstarting_champ_pool=deepcopy(self.starting_champ_pool),\rdesired_champ_pool=deepcopy(self.desired_champ_pool),\rgame_data=self.game_data\r).start()\rif hit_everything:\rself.hits += 1\rreturn self.hits / self.n This function simply runs n simulations and checks how many were successful. Since our simulations are all independent of each other, we can easily multithread this code. We don\u0026rsquo;t do that here, as we intend to run this on a base-tier Heroku machine, which has only 1 CPU anyways.\nNow we can look at the code that simulates a roll down in Teamfight Tactics.\ndef simulate_champs_rolled(self): while self.gold \u0026gt;= 3 and self.champs_to_hit_count \u0026gt; 0: self.gold -= 2 for _ in range(5): # roll a tier tier = choice( options=[\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;3\u0026#34;, \u0026#34;4\u0026#34;, \u0026#34;5\u0026#34;], probs=self.game_data.get_probabilities_of_level(self.player_level), n=1 )[0] # roll a champ in rolled tier champ_names = list(self.champ_pool[tier].keys()) poolsizes = list(self.champ_pool[tier].values()) probabilites = [poolsize / self.champ_pool[\u0026#34;sum\u0026#34;][tier] for poolsize in poolsizes] rolled_champ = choice(options=champ_names, probs=probabilites, n=1)[0] rolled_champ_cost = self.game_data.champion_costs[rolled_champ] # buy it? if self.desired_champ_pool[tier][rolled_champ] \u0026gt; 0 and self.gold \u0026gt;= rolled_champ_cost: self.gold -= rolled_champ_cost self.desired_champ_pool[tier][rolled_champ] -= 1 self.desired_champ_pool[\u0026#34;sum\u0026#34;][tier] -= 1 self.champ_pool[tier][rolled_champ] -= 1 self.champ_pool[\u0026#34;sum\u0026#34;][tier] -= 1 self.champs_to_hit_count -= 1 We start with a while loop, that keeps going, as long as our gold is larger than 3. With 2 gold, we can start a new roll and we need at least 1 gold to buy a new unit. So if we have less than 3 gold, our simulation is finished. The second condition on which our simulation is finished is that we hit all the champions we desired. For that, we keep track of the total count of units to hit.\nInside the loop, we subtract the 2 gold required for rolling from our gold. Then we simulate refreshing the shop. We do this by repeating five times, what we defined as a throw in the beginning. A throw starts with tier selection. We randomly select one of the tiers with the choice() function. The chance of hitting each tier is dependent on the player level, which the user input provided. Then, once we have picked a tier by random, we pick one of the champions inside that tier\u0026rsquo;s champion pool. As a simple example, if there are 2 champions in a tier and there are 3 copies of champion A and 7 copies of champion B left, then we have a 30% and 70% chance to roll the champion respectively. Finally, we will check if the rolled champion is one of the champions, that our user wants. If we have enough gold to buy the unit, we do so. We then subtract the buying costs from our gold, adjust our two dictionaries and decrement the champs_to_hit_count.\nOnce our simulation is finished, we check with create_output(), if we hit all desired champions. Based on that, we return true or false.\nAnd this already concludes the important parts of our simulation.\nCalculator As mentioned, the calculator can handle one specific scenario: The player is going to do a roll down and wants at least 1 single copy of a champion. We won\u0026rsquo;t look at code here, instead, we will look at the maths, which our code reflects.\nWe will understand the general idea with an example. Imagine you are rolling for one specific Legendary on Level 7. Let\u0026rsquo;s pick Amumu here as our desired Legendary. The chance to roll a random Legendary is 2% per throw:\nP(Roll a random Legendary) = 2% The chance to roll Amumu is:\nP(Roll Amumu) = P(Roll a random Legendary) * (Amount Amumus in the pool / Amount of all Legendaries in the pool) There are 7 Legendary units as of right now. Each Legendary unit is 10 times in the Pool. So the Chance per throw to roll Amumu, assuming 0 Legendaries have been picked so far is:\nP(Roll Amumu) = 2% * (10 / 7 * 10) = 0,28% Therefore the Chance per throw to not roll Amumu is:\nP(Dont roll Amumu) = 1 - P(Roll Amumu) = 100% - 0,28% = 99,72% Assume you have 50 gold. 50 gold gives you 25 rerolls. Each reroll gives you 5 throws. That\u0026rsquo;s 125 throws in total. So the chance to not roll Amumu even once in 125 throws is:\nP(Dont roll Amumu in 125 throws) = 99,72% to the power of 125 = 70% In every other scenario, you will get at least one Amumu:\nP(Roll Amumu at least once in 125 throws) = 1 - P(Dont roll Amumu in 125 throws) = 100% - 70% = 30% So you would get at least one Amumu 30% of the time.\nNot mentioned here is that at the start we have to consider the buying cost of the desired champion too, so with 50 gold for a 5-star unit, we only get 44 gold worth of rolls.\nBut this sums up how our calculator operates. Next, let\u0026rsquo;s look at how this project was deployed with Heroku.\nHeroku Deployment To deploy our app on Heroku, you will have to register there. Then you should be able to see your dashboard. There you have to create a new app. Give it a name and click create. Now we want to turn this into a Python app, so go to Settings -\u0026gt; Add buildpack and select Python. Now all that\u0026rsquo;s left to do is creating two files:\nrequirements.txt Procfile With requirements.txt we determine which packages are required to build our project. Heroku needs to know this. There are many ways to create requirements.txt, if you use an IDE like PyCharm, there is probably a function built-in already. If you like cli, you can use the following command\npip3 freeze \u0026gt; requirements.txt Now the Procfile. Here we insert information that is specific to Heroku. In this case, it is very simple, we just insert the command to run our project. This looks as follows.\nweb: gunicorn app:app We deploy a web-type application and our deployment server is gunicorn. Many people make the mistake of using the built-in server of Flask for deployment, which the developers strictly tell us not to do, whenever we use it locally for testing.\nFinally, to automatically deploy our application, whenever we make any changes to it go to Deploy and connect your app to the GitHub repository you are developing in. Then you can pick the branch which you want to deploy from, as you maybe want to develop in certain branches and only push changes to deployment from time to time.\nThis brings us to the end of the article. I enjoyed building this app since I really like playing TFT. Working with Flask and Jinja2 was pretty intuitive. You can check out the codebase on GitHub.\n","permalink":"https://huti26.github.io/posts/teamfight-tactics/teamfight-tactics/","summary":"Preface This preface is for people who don\u0026rsquo;t know Teamfight Tactics.\nTeamfight Tactics is a board game, in which you buy champions from a shop and place them on the board. There are 5 different tiers of champions: common, uncommon, rare, epic, and legendary. The champions of each tier cost 1, 2, 3, 4, and 5 gold respectively. With your board, you fight other people and the loser of the fight loses Health Points.","title":"Teamfight Tactics Monte-Carlo-Simulation with Flask and Heroku"},{"content":"Preface This project is my first C++ project and it is my Master\u0026rsquo;s Thesis. I had 6 months to create this project. However, at least 2 months of this time was spent writing the actual thesis and not coding. About 3 months were spent coding with an additional month for benchmarking.\nRocksDB RocksDB is open-source and successfully used in many projects. For example in Apache Flink, state is maintained during calculations with RocksDB. RocksDB uses Log-Structured Merge-Trees under the hood. Log-Structured Merge-Trees avoid random writes by making all data immutable. This results in only sequential writes, which are significantly more performant. Of course, there are trade-offs for that. Reads become costlier, but this is balanced out by maintaining bloom filters and index structures. Also, there is redundancy. This is limited by compaction.\nMeta uses three different distributed Key-Value Stores internally, which are all based on RocksDB. Each of these systems is fine-tuned for its use case. This adaptation to the use case is how Meta seems to extract maximum performance from their RocksDB-based systems. This is stressed by this paper, in which they describe how they developed specialized systems to track user behavior of their systems. For this to work, RocksDB has to be very adaptable. You can both adapt RocksDB in the data structures that are used internally and in the numbers. For example, you can pick from many data structures in which data is kept in-memory in the MemTable. You can also decide the maximum size of the MemTable, which is 64MB per default.\nBased on this, we are assured that a distributed version of RocksDB can be very performant.\nWe will leave it at that here, but if you are curious about more details regarding RocksDB, you should check out the RocksDB Wiki.\nUCX Unified Communication X is an open-source project, which allows users to develop for Ethernet and Infiniband hardware at the same time. Code, that runs on Infiniband hardware, can also run on Ethernet, without any changes. UCX is backed by multiple companies, one of them being NVidia. UCX has lots of functionalities, but we won\u0026rsquo;t go into detail on it here. This project only utilizes the high-level API called UCP. With UCP we can transmit data between nodes in multiple ways. We only use UCP stream sends in this project, which allow us to send arbitrary data streams.\nIf you are curious about UCX details, you can check out the source code, which is linked at the end of this article.\nCode Structure /cpp\r│ .gitignore\r│ CMakeLists.txt\r│\r├───clion-run-configs\r│ 1server.run.xml\r│ 2servers - server 1.run.xml\r│ 2servers - server 2.run.xml\r│ 4servers - server 1.run.xml\r│ 4servers - server 2.run.xml\r│ 4servers - server 3.run.xml\r│ 4servers - server 4.run.xml\r│ 8-clients.run.xml\r│ client CLI.run.xml\r│ client.run.xml\r│ Four Servers.run.xml\r│ Two Servers.run.xml\r│\r├───src\r│ │ CMakeLists.txt\r│ │\r│ └───distributed-rocksdb\r│ │ CMakeLists.txt\r│ │\r│ ├───Client\r│ │ CMakeLists.txt\r│ │ DrdbClient.cpp\r│ │ DrdbClient.h\r│ │ DrdbClientApp.cpp\r│ │ DrdbClientApp.h\r│ │ main.cpp\r│ │\r│ ├───Common\r│ │ ClientServerCommons.h\r│ │\r│ ├───JNI\r│ │ CMakeLists.txt\r│ │ JDrdbClient.cpp\r│ │ JDrdbClient.h\r│ │\r│ └───Server\r│ CMakeLists.txt\r│ DrdbConnectionAcceptor.cpp\r│ DrdbConnectionAcceptor.h\r│ DrdbConnectionDeque.cpp\r│ DrdbConnectionDeque.h\r│ DrdbConnectionHandler.cpp\r│ DrdbConnectionHandler.h\r│ DrdbEndpointState.cpp\r│ DrdbEndpointState.h\r│ DrdbServerApp.cpp\r│ DrdbServerApp.h\r│ DrdbWorker.cpp\r│ DrdbWorker.h\r│ main.cpp\r│ server_config1.txt\r│ server_config2.txt\r│ server_config4.txt\r│\r└───Valgrind\r├───leak-check-full\r│ 1000-put-client.txt\r│ 1000-put-server.txt\r│\r└───leak-check-yes\r1000-empty-reads-client.txt\r1000-empty-reads-server.txt The main content of our C++ implementation is in /cpp/src. There we have separate folders for the Client and the Server code. Additionally, we have some shared code between the Client and the Server in /cpp/src/Common. The folder /cpp/src/JNI contains the code required to access our project from Java. We will talk about that later. Apart from that, we tested our implementation with Valgrind and the results are in /cpp/Valgrind. Also, there are some helpful run configurations if you want to test the project yourself, in /cpp/clion-run-configs.\nTo make the code accessible from Java, we also have /java folder which looks as follows.\n/java\r│ .gitignore\r│ build.gradle\r│ gradlew\r│ gradlew.bat\r│ settings.gradle\r│\r├───gradle\r│ └───wrapper\r│ gradle-wrapper.jar\r│ gradle-wrapper.properties\r│\r└───src\r└───main\r├───include\r│ site_ycsb_db_JDrdbClient.h\r│\r└───java\r│ App.java\r│\r└───site\r└───ycsb\r└───db\rJDrdbClient.java\rKeyValueStore.java\rpackage-info.java We use Gradle as our build tool and implement code to make use of the Java Native Interface and the Yahoo! Cloud Serving Benchmark. Both will be discussed later.\nArchitecture The current implementation only supports static clusters. Each server needs to be given the ip-address and port of all other servers via a config file. All servers are connected with each other. By default, there is a single connection between each server, but the user can increase this amount. Each server has a static amount of Client Workers and Redirect Workers. Client Workers handle communication with clients and Redirect Workers handle incoming redirect requests.\nRedirecting occurs, when a client requests an object, that another server is responsible for. Key responsibility is decided by their hash value. The current implementation uses murmurhash2, which is the default hashing algorithm in the used version of GCC. Each server has a unique id starting from 0. The responsible server is then calculated as follows.\nresponsible_server = hash(key) % server_count So for example in a 4-node cluster, our nodes have the ids 0, 1, 2, and 3. Then for a given key, we would calculate hash(key), which is X. Then we calculate X modulo 4 and that value matches one of our server ids.\nWith all this being said, let\u0026rsquo;s look at an example to understand the current implementation.\nThis is a two-node cluster. Right from the start, we have two Client Workers and one Redirect Worker on each node. Each of these Workers is a separate thread. Not depicted in this picture, are two further threads: ClientAccepter and ClientHandler. These two threads deal with new connection requests. Due to UCX\u0026rsquo;s design, accepting and handling have to be in separate threads if you want to be able to deal with multiple connection requests at once. These two threads are omitted for clarity. We will briefly discuss connection handling later.\nNow the nodes will have to connect to each other.\nNode A creates a Redirector B. This is a class, which Node A can use for outgoing redirect requests to Node B. To handle that, Node B creates a Redirect EP A. EP stands for endpoint. Note that this is also a class. The thread count in our implementation is static, to guarantee good scaling. A thread is required to operate that Redirect EP, therefore responsibility of the Redirect EP is given to the Redirect Worker 1 of Node B.\nNow the same happens the other way around.\nSimilar to before Node B creates a Redirector and Node A creates a Redirect EP. Now the cluster is ready to handle clients.\nHere three clients connected to Node A in the following order: 33, 44, 55. Node A gives responsibility for the endpoints to the Client Workers in a round-robin fashion.\nConnection Handling For connection handling we use a std::deque with a mutex, that is shared between our ConnectionAcceptor and our ConnectionHandler. Connection requests are handled in a FIFO (First in First out) manner. To realize this, the ConnectionHandler always handles the first element of the deque, while the ConnectionAcceptor inserts new requests at the back of it. The ConnectionHandler is a fragment of earlier versions and could be refactored in an improved version. The current implementation of the ConnectionHandler only distributes the requests to yet another deque. Each of the Workers has an own deque. Whenever the Workers find a connection request in their deque, they will handle it, before continuing with further communication. To understand this better, let\u0026rsquo;s look at the infinite loop in which the Workers are running.\nint status; while (true) { // PART 1: Check for pending connection requests while (!drdbConnectionDeque-\u0026gt;connection_contexts.empty()) { handle_connection_request(); ep = drdb_endpoints.back().get(); // Now setup the Client for a new request, this will return immediately status = receive_method(); } // PART 2: Check for new requests made by clients ... } As indicated in the code, during the first part of the loop, the Workers will check if there are any connection requests in the deque. Only once those are all handled, will the Workers move on to part 2, in which they handle communication with the clients.\nCommunication Now we will continue with the prior mentioned part 2. Let\u0026rsquo;s look at the code of it.\nint status; while (true) { // PART 1: Check for pending connection requests ... // PART 2: Check for new requests made by clients for (auto \u0026amp;element: drdb_endpoints) { if (element-\u0026gt;receive_request_context.complete == 1) { ep = element.get(); // A Request has been made by the Client, handle it status = receive_method(); // When an ep disconnects, dont make the second request if (status == -1) { break; } // Now setup the Client for a new request // This will return immediately status = receive_method(); } else { ucp_worker_progress(*ucp_data_worker_p); } } } The Worker loops over all the endpoints he is responsible for and checks if there are new requests. Whenever there is such a request, the Worker will handle an entire communication cycle. Therefore, the current implementation is not asynchronous. Unfortunately, there was no time left, to implement that. The current implementation is already done with async in mind and the architecture should easily support it.\nA communication cycle consists of a message by the client and a response by the server. Currently, simple get, put, and delete commands are implemented. The communication protocol is defined as follows.\nClient To Server Communication Server To Client Communication Yahoo! Cloud Serving Benchmark To evaluate our server, we wanted to use the Yahoo! Cloud Serving Benchmark, YCSB for short. Our server is still rather minimal and misses three features:\nRange scans field lengths \u0026gt; 1, which means, that more than a single field gets transmitted in a message. Dividing keys into multiple tables To assure, that we don\u0026rsquo;t accidentally run a benchmark that makes use of these functionalities, we define a limiting interface. The interface concatenates table name and key name to emulate table support as follows.\nprivate String generateKey(String table, String key) { return String.format(\u0026#34;%s.%s\u0026#34;, table, key); } For our other functions, the interface will return an error.\n@Override public Status scan() { return Status.NOT_IMPLEMENTED; } @Override public Status read(Set\u0026lt;String\u0026gt; fields) { if(fields != null \u0026amp;\u0026amp; fields.size() != 1) { System.err.println(\u0026#34;Field counts other than 1 are not supported!\u0026#34;); return Status.BAD_REQUEST; } ... } With this interface, we can implement YCSB as described in its documentation. The result of that can be found in the /yscb folder of this project.\nJava Native Interface Since the YCSB is written in Java, we have to make our Client C++ code available to the JVM with the Java Native Interface, JNI for short.\nTo achieve that, we define native functions in Java as follows.\npublic native long initDrdbClient(String serverAddress, int serverPort, String logDir); public native byte[] get(long nativePtr, String key, int getRequest); public native byte put(long nativePtr, String key, byte[] value, int putRequest); public native byte del(long nativePtr, String key, int delRequest); Native functions are not going to be implemented in Java. Instead, we will compile this code with the -h flag and get a header file from it. If you want to try this yourself with this project, you can use the Gradle task compileJava instead. Once we have received this header file, we have to implement it with native code.\nBefore we look at the native code, let\u0026rsquo;s talk about an encoding problem. Java Strings are UTF-16, while C++ Strings are UTF-8. So we have to convert our keys as follows.\n@Override public byte[] get(String key) { getRequest++; byte[] keyBytes = key.getBytes(StandardCharsets.UTF_8); String utf8EncodedKey = new String(keyBytes, StandardCharsets.UTF_8); byte[] result = get(nativePtr, utf8EncodedKey, getRequest); if (result.length == 0) { return null; } return result; } Now, let\u0026rsquo;s look at the native code for a get.\njbyteArray Java_site_ycsb_db_JDrdbClient_get(JNIEnv *env, jobject, jlong drdb_p, jstring jKey, jint request) { int request_number = (int) request; auto *drdbClient_p = (DrdbClient *) drdb_p; std::string key = convert_jstring(env, jKey); auto result = drdbClient_p-\u0026gt;get(key); if (result.value.empty()) { return env-\u0026gt;NewByteArray(0); } jbyteArray reply = env-\u0026gt;NewByteArray(result.value.length()); env-\u0026gt;SetByteArrayRegion(reply, 0, result.value.length(), reinterpret_cast\u0026lt;const jbyte *\u0026gt;(result.value.c_str()) ); return reply; } First, we can see, that we use the address drdb_p to access our Client object. This pointer is given as an argument from the Java side. Logically, our JNI code works as follows: First, we initialize a Distributed RocksDB Client object with initDrdbClient(). This function then returns the pointer to our Client object. We then transmit this address to our get, put, or delete function every time we use it.\nNext, we have to convert our String once again, despite already changing the encoding on the Java side. This is simply due to JNI design. We can\u0026rsquo;t just use Strings or ByteArrays directly. Let\u0026rsquo;s first finish looking at this function, then we will look at the conversion functions. So after converting the key, we can simply call our get function and receive a result. We will then have to convert our result to a jbyteArray, which we then can return to the JVM. Now let\u0026rsquo;s look at the conversion functions:\nstd::string convert_jstring(JNIEnv *env, jstring jKey) { jboolean isCopy; auto converted_string = env-\u0026gt;GetStringUTFChars(jKey, \u0026amp;isCopy); auto key = std::string{converted_string}; env-\u0026gt;ReleaseStringUTFChars(jKey, converted_string); return key; } std::string convert_jByteArray(JNIEnv *env, jbyteArray jValue) { jsize num_bytes = env-\u0026gt;GetArrayLength(jValue); jboolean isCopy; jbyte *elements = env-\u0026gt;GetByteArrayElements(jValue, \u0026amp;isCopy); std::string value{reinterpret_cast\u0026lt;char *\u0026gt;(elements), static_cast\u0026lt;size_t\u0026gt;(num_bytes)}; env-\u0026gt;ReleaseByteArrayElements(jValue, elements, JNI_ABORT); return value; } We have to create a new object from the given bytes, we can not operate on the given object directly. Then we create a C++ std::string from that. Afterwards, we have to release the intermediate object, else wise we are going to leak memory.\nOnce the native code has been implemented, we have to compile it and create a library of it. In this project, this is done with CMake inside /cpp/src/distributed-rocksdb/JNI/CMakeLists.txt. This library then has to be loaded inside our Java program as follows.\npublic class JDrdbClient extends KeyValueStore { static { System.loadLibrary(\u0026#34;jdrdb-client\u0026#34;); } ... } For the scope of this article, this should be enough background knowledge on this project. If you are curious about more details, you can check out the repository.\nNext, we will talk a bit about containerization, then we will look at the benchmarks we ran.\nContainerization Docker I like Docker. The reproducibility is a big advantage for me. Therefore, I developed this project with Docker as my development environment. The newest versions of CLion allow using Docker as a dev environment and pretty much everything works flawlessly. This only applies to my M1 MacBook Air. On my Windows machine, the performance was awful. I tried plenty of solutions I found online, but none of them fixed the performance. When I make changes on my MacBook, there is close to zero latency compared to native development. On my Windows machine, there is multiple seconds latency. This occurs both \u0026ldquo;natively\u0026rdquo; and with WSL2. I have not tested it on Linux, but if I had to make a guess, I am certain the performance will be even better than on macOS.\nEnough talk about operating systems though, let\u0026rsquo;s look at the Docker config. We use /dockerfiles/debian_base.dockerfile as our dev environment. This dockerfile creates a dev environment based on Debian 11. It installs all required dependencies and sets required path variables. With that, if you want to test this project, all you have to do is clone the repository, build the docker image and select it as your dev environment in CLion. Pretty convenient as far as I am concerned. This helps if you find a bug and want someone else to help you out. I am certain VSCode also supports Docker as dev environments very well. I am not sure about other editors though.\nWith our build scripts mentioned at the beginning, we can start and test our implementation in CLion. Additionally, we also have docker-compose scripts for testing our implementation. docker-compose.4servers-4clients.yml for example starts a 4 node cluster and benchmarks it with 4 clients. The file looks as follows.\nservices: server1: build: context: . dockerfile: dockerfiles/drdb_debug.dockerfile image: \u0026#34;hutii/drdb:latest\u0026#34; entrypoint: \u0026#34;/app/build/src/distributed-rocksdb/Server/distributed-rocksdb-server-cpp -s server1 -p 13337 -c /app/src/distributed-rocksdb/Server/server_config4.txt\u0026#34; network_mode: host ... client1: build: context: . dockerfile: dockerfiles/drdb_debug.dockerfile image: \u0026#34;hutii/drdb:latest\u0026#34; entrypoint: \u0026#34;/app/build/src/distributed-rocksdb/Client/distributed-rocksdb-client-cpp -s server1 -p 13337\u0026#34; depends_on: - server1 - server2 - server3 - server4 network_mode: host ... We omit servers 2, 3, and 4, and clients 2, 3, and 4 for clarity. All services make use of drdb_debug.dockerfile. This file is rather long, so let\u0026rsquo;s look at this shortened version of it to understand the logic behind it.\n################### # BUILDER PHASE ################### FROM hutii/drdb:base AS builder # Build everything ... ################### # RELEASE IMAGE ################### FROM debian:11 # Install only required libraries for running ... # Copy binaries from builder phase ... This dockerfile makes use of multi-stage Docker builds. At the start, we have our builder phase. Here we use our previously mentioned base image, which has all kinds of build tools like GCC and CMake installed. With that, we compile our project and build everything. We can create a docker image based on this step with the following command.\ndocker build -f ./dockerfiles/drdb_debug.dockerfile --target builder -t hutii/drdb:debug . In the next phase, we start with a new base. This time we don\u0026rsquo;t use our base image, but a clean version of Debian 11 instead. We don\u0026rsquo;t install build tools like GCC and CMake this time. Instead, we only install required libraries for running. We then copy all the binaries created in the builder phase to this image. If want to build this slimmer image, we just need to omit the --target builder from our previous command.\ndocker build -f ./dockerfiles/drdb_debug.dockerfile -t hutii/drdb:latest . On my ARM-based machine, this results in the following image size difference.\nREPOSITORY TAG IMAGE ID CREATED SIZE\rhutii/drdb latest f167bfb0ddf9 About a minute ago 734MB\rhutii/drdb debug 3f84bc912b89 About a minute ago 1.96GB We save more than a GB. For deployment of a real project, this is relevant, as it saves storage on costly cloud services. For this project, it saves us some time, when uploading our image to DockerHub.\nAs a part of our containerization, we set up GitHub Actions, which automatically build and deploy our project to DockerHub, whenever we make a commit in a releases branch. The .github/workflows/docker-image.yml looks as follows.\nname: Docker Image CI on: push: branches: - \u0026#34;releases/001\u0026#34; jobs: build: runs-on: ubuntu-latest steps: - name: Login to DockerHub uses: docker/login-action@v1 with: username: ${{ secrets.DOCKERHUB_USERNAME }} password: ${{ secrets.DOCKERHUB_TOKEN }} - uses: actions/checkout@v2 - name: Build the Docker image run: docker build -f ./dockerfiles/debian_base.dockerfile -t hutii/drdb:base . \u0026amp;\u0026amp; docker build -f ./dockerfiles/drdb_debug.dockerfile -t hutii/drdb:latest . \u0026amp;\u0026amp; docker push hutii/drdb:latest We use our DockerHub-hosted image in the next step.\nSingularity We want to test our project on our University\u0026rsquo;s cluster. We use OpenPBS there and the users do not have root rights. Using Docker is therefore not possible. Podman is an option, but instead, we use Sylabs Singularity Containers, which are commonly used in High-Performance-Computing. Singularity makes it very easy to convert a Docker Image to a Singularity Image. The script looks as follows.\nBootstrap: docker\rFrom: hutii/drdb:latest\r%labels\rVersion v0.0.1\r%help\rWrapper for docker image of distributed rocksdb We just need to pull the image from DockerHub and that\u0026rsquo;s it. We can even use the Sylabs Remote Builder for this step. The Sylabs Remote Builder is a free service for building Singularity Images. With this Remote Builder and our GitHub Action, we completely removed the necessity for building anything locally. This is fairly convenient in two ways. First, I am developing on an ARM-based machine, but we need to deploy the project to x86-based machines. x86 Docker images are slow to build on my ARM-based machine and even worse, singularity does not work on ARM. The second point is, that I had to travel frequently during the time I had to write my Master\u0026rsquo;s Thesis, and internet while traveling is rather bad in Germany (strongly depending on the location).\nEvaluation Clusters We ran experiments in two different settings.\nThe first one is the Operating Systems Research Group Cluster at my university. There we have a variety of nodes with Infiniband hardware. We always used nodes with 56GBit/s Infiniband. All nodes run Intel Xeon E5-1650 CPUs. We use OpenPBS there and we requested nodes with 62GB of RAM for our servers and nodes with 15GB of RAM for our clients. We requested 8 vCPUs for both clients and servers. Both clients and servers had access to Flash Storage. All nodes run CentOS Stream 8.\nThe second setting is a cluster running on Amazon Web Services. We used r5.2xlarge instances for our servers. These have 8 vCPUs and 64GB of RAM. For our clients, we used c4.2xlarge instances, which have 15GB of RAM and also 8 vCPUs. We gave servers 200GB of gp2 Flash Storage and 30GB of gp2 Flash Storage to the clients. All nodes run Ubuntu 22.04.\nWorkloads All experiments are performed with the YCSB Workload A. Workload A consists of 50% reads and 50% updates. The distribution is Zipfian. Unless specified differently Workload A is run with a package size of 1KB. A real-world example for Workload A is a Session store recording recent actions.\nSoftware Versions The project was tested in a Singularity container running Debian 11, RocksDB 6.11.4-3, and UCX 1.12.0. For the HHU cluster singularity version 3.8.5-2.el8 was used. For the AWS cluster singularity version 3.9.9 was used.\nOperating Systems Research Group Cluster Optimal Configuration We have a static amount of Client Workers and Redirect Workers. Finding an optimal amount for both of those is interesting. Fully analyzing this was beyond the scope of my thesis, but with the following experiment, we get a first feeling for it. We compare two configurations: One with 4 Client Workers and 4 Redirect Workers. One with 6 Client Workers and 1 Redirect Worker. Both configurations create 2 connections to each other server.\nTo get a feeling for the data, violin plots are examined.\nOn the y-axis is the throughput in ops/s and on the x-axis we are increasing the number of client threads, that are sending requests to the cluster. With an increase in clients, we expect the throughput per client to reduce. We can see no anomalies in the distribution of our measurements here, which is good. Next, we can look at the summed throughput of our cluster.\nSince we have a 6-node cluster with each node having 8 vCPUs, we have 48 vCPUs in total. Therefore, we expect the total throughput to increase up to 48 clients at least. This is as expected. If we now compare the configurations, we see that the 4 Client Workers and 4 Redirect Workers configuration is slightly better on 6 and 12 clients. The 6 Client Worker and 1 Redirect Worker configuration is slightly better on 48 clients. In general, many more experiments are required to make hard statements about an optimal configuration.\nInfiniband vs Ethernet We specifically used UCX as our networking framework to support Infiniband hardware. Therefore a comparison between Ethernet and Infiniband hardware is very important. Let\u0026rsquo;s look at the achieved throughput here.\nOur Ethernet-based cluster reaches a total throughput of almost 100.000 ops/s while the Infiniband-based cluster achieves roughly 175.000 ops/s. Unfortunately only 1Gbit/s Ethernet was available. Another comparison with more powerful Ethernet connections would be nice in the future. Regardless, let us look at latency as well.\nThe Ethernet-based cluster has a median of 249 microseconds. The quantiles span from 255 to 237 microseconds. Minimum and maximum values are 227 and 262 respectively. The Infiniband-based cluster has a median of 145 microseconds. The quantiles span from 131 to 146 microseconds. Minimum and maximum values are 107 and 148 respectively.\nSo the span from minimum and maximum is 35 microseconds for the Ethernet-based cluster and 41 microseconds for the Infiniband-based cluster. The Ethernet-based cluster has a smaller total span. However, when one looks at the span of the quantiles, the Ethernet-based cluster has a span of 18 microseconds while the Infiniband-based cluster has a span of 15 microseconds. So while the Infiniband-based cluster performs better performance-wise, there is no clear winner in regards to consistency. These results are very similar for update latency.\nCluster Scalability The next experiment we will talk about in this article is regarding cluster scalability. With an increase in cluster size, we want linear scaling optimally.\nFrom the total throughput, we can already tell that it is not perfectly linear. We can analyze this in more detail by looking at the average throughput of each node in the cluster.\nHere we can see, that while an increase from 2 to 4 nodes hardly changes the performance, we see bigger dips for 6 and 8 nodes. Our average throughput in ops/s per node decreases from more than 43.000 to almost 39.000 from 2 to 8 nodes. Unfortunately, we did not have access to even bigger clusters, which would have been interesting to see.\nAmazon Web Services Cluster At this point of my thesis, we were running low on time. We still wanted to have a rough estimate about how we line up against industrial-grade alternatives. We had no time to run such alternatives on our own cluster. Instead, we used a paper from 2018 as a reference value. This paper is from Altoros. It benchmarked the following Databases as a Service: Couchbase Cloud, MongoDB Atlas, and Amazon DynamoDB. Altoros is a contributor to Couchbase, so we expect that they know their system the best and can extract maximum performance from it.\nAll the systems use AWS under the hood and the hardware is precisely described. Their benchmark with a 6 node cluster is recreated with Distributed RocksDB. An important difference is, that Altoros benchmarked with a field length of 10 and a field count of 1KB. So each transmitted package during the Altoros benchmark has a size of 10KB. Distributed RocksDB only supports field lengths of 1. So to get a close approximation, Distributed RocksDB was benchmarked with a field length of 1 and a field count of 10KB. So each transmitted package also has a size of 10KB.\nBefore seeing the results it is required to talk about bias. There is a bias that positively affects the results of Distributed RocksDB. The first one is that Distributed RocksDB has not implemented replication yet. So while the other systems use the industry standard of 3 replicas, Distributed RocksDB only uses 1 replica. The other systems are also more mature in that they implement server-failure-detection. Also, Distributed RocksDB does not support using different column families yet. How this is handled is explained in the YCSB implementation chapter. The result is that the communication protocol is simplified for Distributed RocksDB. With this bias in mind, one can look at the results.\nDatabase Throughput in ops/s Replicas Source Distributed RocksDB 13014 1 This Thesis (2022) Couchbase Cloud 33460 3 Altoros Paper (2018) MongoDB Atlas 19144 3 Altoros Paper (2018) Amazon DynamoDB 30404 3 Altoros Paper (2018) Distributed RocksDB achieves a throughput of roughly 13.000 ops/s while Couchbase Cloud, MongoDB Atlas, and Amazon DynamoDB achieve throughputs of roughly 33.000, 19.000, and 30.000 ops/s respectively. Despite all the positive biases mentioned prior, Distributed RocksDB performs worse than all compared solutions. However, the compared solutions were all very mature products in 2018 already and as mentioned, the key to optimal performance seems to be a strong optimization for the use case in Meta\u0026rsquo;s opinion. Distributed RocksDB has not been optimized at all. Also, systems often group multiple client requests and transmit them at once, to achieve better performance. Distributed RocksDB does not implement such optimizations yet. There are many aspects of RocksDB that can be fine-tuned in future work. So the results are not to be interpreted negatively.\nThis experiment was run on Ethernet hardware. It would be nice to see how Distributed RocksDB compares to other systems when it can take advantage of Infiniband hardware. Amazon does not offer access to Infiniband hardware, unfortunately. Microsoft offers Infiniband hardware in their cloud services, but using Azure in addition to AWS exceeded the time we had during my thesis.\nConclusion Building a distributed version of an already established key-Value Store was very interesting to me and I am glad I got the opportunity to have this as my Master\u0026rsquo;s Thesis topic. C++ can be tricky at times and I had some nasty bugs due to it. With some experience and code analysis features provided by modern IDEs, it is overall still fine to work with. Of course, it is not as simple as something like Python, but it should not be compared to it in the first place. Setting up your development environment can be very confusing, as there are many ways to do so and most documentations of projects expect you to already have a certain level of knowledge. I excluded those problems from this article since it is already very long as is, but I might come back to it in a separate one in the future.\nThere is a lot of potential future work to do. To improve performance the obsolete Connection Handler may be removed. Also moving to asynchronous communication should be possible without bigger architecture changes. Enhancing the communication protocol to support dealing with multiple keys in a single request is also an important aspect. Once that is done, the system should be optimized for the YCSB Workload A with RocksDB\u0026rsquo;s internal benchmarking tools. Rerunning these benchmarks afterwards would be interesting.\nYou can find the source code for the project here.\n","permalink":"https://huti26.github.io/posts/distributed-rocksdb/distributed-rocksdb/","summary":"Preface This project is my first C++ project and it is my Master\u0026rsquo;s Thesis. I had 6 months to create this project. However, at least 2 months of this time was spent writing the actual thesis and not coding. About 3 months were spent coding with an additional month for benchmarking.\nRocksDB RocksDB is open-source and successfully used in many projects. For example in Apache Flink, state is maintained during calculations with RocksDB.","title":"Making Meta's RocksDB distributed with UCX to support Infiniband Hardware"},{"content":"Preface The 2021 season of Formula 1 is the first one I watched and I have been a fan of both Hamilton and Bottas ever since. During this season Bottas has been criticized a lot. That\u0026rsquo;s why I wanted to do a bit of data analysis on how Hamilton and Bottas compare in raw numbers. Raw numbers are not a particularly good way to make hard statements in Formula 1, since they lack a lot of context. However, the raw numbers can be used as an indicator to find outliers or general trends, which then should be further analyzed with context. Additionally, there will be some comparisons between Lewis Hamilton and Sebastian Vettel during their Championship Races in 2017 and 2018. The same comparisons will be done for the 2021 season in which Lewis Hamilton and Max Verstappen competed for the Championship.\nData Source The used data can be found on Kaggle: Formula 1 World Championship (1950 - 2022). The original data source is the Ergast Developer API. The maintainers on Kaggle provide 14 CSV files containing information about drivers, constructors, and all kinds of results. The data is built in a relational format, so it can easily be imported into a SQL database of choice. In this case, SQLite is entirely sufficient. Additionally, some transformations will be made with the Python library Pandas.\nData Engineering The source code can be found on GitHub\nInitial data transformations are done in SQL and the results are in the /data folder. Any further transformations done with Pandas are stored in the /hamilton-vs-bottas folder.\nThe first analyzed data is about the race pace. For that, the lap times of Hamilton and Bottas from 2017 to 2021 were extracted. Transformations are explained in detail for this first analysis and omitted for the others. All transformations can be found in the mentioned repository and if these steps are not of your interest, just skip ahead to the analysis.\nSELECT lap_times.raceId, lap_times.driverId, lap_times.lap, lap_times.position, lap_times.milliseconds, races.year, races.round, races.name, races.date, drivers.code, drivers.forename, drivers.surname, drivers.dob, drivers.nationality, circuits.name, circuits.country, results.statusId FROM lap_times, races, drivers, circuits, results WHERE (lap_times.driverId = 1 OR lap_times.driverId = 822) AND results.statusId = 1 AND (year \u0026gt;= 2017 AND year \u0026lt;= 2021) AND lap_times.driverId = drivers.driverId AND lap_times.raceId = races.raceId AND races.circuitId = circuits.circuitId AND results.raceId = lap_times.raceId AND results.driverId = lap_times.driverId This data contains the lap time of each lap that Hamilton and Bottas drove. To further analyze this data, it was transformed with Pandas.\nFirst, the mean race pace was created.\ndata = pd.read_csv(\u0026#34;data/lap_times_ham_vs_bot_without_dnfs.csv\u0026#34;) means = data.groupby([\u0026#34;race_name\u0026#34;, \u0026#34;circuit_name\u0026#34;, \u0026#34;year\u0026#34;, \u0026#34;code\u0026#34;])[\u0026#34;milliseconds\u0026#34;].mean().reset_index() means.to_csv(\u0026#34;hamilton-vs-bottas/racepace-means-alphabetical.csv\u0026#34;, index=False) This data is now in the long format, but we require wide format data for the next step, in which we calculate the Deltas. Delta here simply means the difference in mean race pace between the drivers.\ndata = pd.read_csv(\u0026#34;hamilton-vs-bottas/racepace-means-alphabetical.csv\u0026#34;) data = data.pivot(index=[\u0026#34;race_name\u0026#34;, \u0026#34;circuit_name\u0026#34;, \u0026#34;year\u0026#34;], columns=\u0026#34;code\u0026#34;, values=\u0026#34;milliseconds\u0026#34;).reset_index() data.to_csv(\u0026#34;hamilton-vs-bottas/racepace-means-alphabetical-wide.csv\u0026#34;, index=False) Before we calculate the Deltas, we remove the 2021 race in Belgium. This is an outlier race, that did not start due to rain.\ndata = pd.read_csv(\u0026#34;hamilton-vs-bottas/racepace-means-alphabetical-wide.csv\u0026#34;) data = data.drop(data[(data[\u0026#34;circuit_name\u0026#34;] == \u0026#34;Circuit de Spa-Francorchamps\u0026#34;) \u0026amp; (data[\u0026#34;year\u0026#34;] == 2021)].index) data.to_csv(\u0026#34;hamilton-vs-bottas/racepace-means-alphabetical-wide-without-spa-2021.csv\u0026#34;, index=False) Finally, we can calculate the Deltas.\ndata = pd.read_csv(\u0026#34;hamilton-vs-bottas/racepace-means-alphabetical-wide-without-spa-2021.csv\u0026#34;) data[\u0026#34;delta\u0026#34;] = data[\u0026#34;BOT\u0026#34;] - data[\u0026#34;HAM\u0026#34;] data.to_csv(\u0026#34;hamilton-vs-bottas/racepace-means-alphabetical-wide-without-spa-with-deltas.csv\u0026#34;, index=False) Data Analysis All visualizations are created with R and ggplot.\nRacepace Here one can see two strong outliers in 2021. Race 2 and 12 favor Bottas and Hamilton respectively. In race 2 there was a DNF, which means that one of the drivers did not finish the race in case you are not familiar with Formula 1. So next, all races containing DNFs were removed.\nOne can see, that not only did the second race of 2021 get removed but also a couple of others too. This is good because the races with DNFs skew the data. However, the outlier race 12 in 2021 was not removed. This is because race 12 of 2021 is the Belgium Grand Prix, which only ran for two laps due to rain. We remove that race too.\nFinally, we can analyze the filtered data. The years in which Bottas contested Hamilton for the Championship were 2019 and 2020. Race pace-wise Bottas was faster 10 times, while Hamilton was faster 24 times. Also, when Hamilton was faster than Bottas, he was faster by a bigger margin. Similar trends can be found in the other seasons too, although it is noteworthy, that especially in the later races of the seasons Mercedes has favored Hamilton since he was the one competing for the Championship.\nThe plots above show the performance chronologically, the races are sorted from first race to last race. In the following plot, the performance sorted by racetrack is analyzed instead.\nA particularly strong track for Hamilton is his home race, the British Grand Prix. Hamilton is unbeaten by Bottas in front of his home crowd. Spa (The Belgian Grand Prix) and Barcelona (The Spanish Grand Prix) are also unbeaten tracks for Hamilton. On the other hand, Bottas has outperformed Hamilton both on the Austrian Grand Prix and in Baku (The Azerbaijan Grand Prix). Exceptionally strong races by Bottas can be found in the 2017 Russian Grand Prix, the 2019 Italian Grand Prix, and the 2021 Turkish Grand Prix.\nDNFs DNFs are another interesting aspect to analyze. Hamilton, who is self-proclaimed blessed, is known to have very DNFs, while Bottas is known for some unlucky DNFs such as his Pitstop in Monaco in 2021.\nIn the plot, we can see, that Hamilton has DNFed twice from 2017 to 2021, while Bottas has DNFed 10 times. Noteworthy is that Bottas has DNFed every single time in the 11th race since 2019. At the time I am writing this, the 10th race of the 2022 season, the British Grand Prix, has just finished and the 11th race in Austria is around the corner. Hopefully the next time we analyze this type of data, we can say, that Bottas broke this curse.\nStandings Next, we will look at how close Bottas and Hamilton were points-wise. In grey, there will be a dotted line to indicate at which race the Championship was decided. It is important to note, that this refers to the point at which Hamilton won the Championship, Bottas has been out of the Championship race earlier than that in 2017, 2018, and 2021.\nThroughout all seasons Bottas was never ahead in points after the 5th race. Hamilton quickly built a fair margin in points and kept that throughout all the seasons, when compared with Bottas.\nPositions Next, we will move away from comparing only Hamilton and Bottas. Instead, we will look at the Championship Contenders of each year.\nIt is known, that Hamilton is particularly strong in the final phase of the Championship. First, let\u0026rsquo;s look at his title fight with Vettel in 2017 and 2018. In 2017 starting with race 12, he finished 1, 1, 1, 2, 1, 1. Then a 9th place finish was enough for him to win the Championship with two more races left. 2018 was very similar, starting with race 12 he finished 1, 2, 1, 1, 1, 1, which then allowed him to win the Championship with a 3rd and 4th place, with two more races left. During both of these Hamilton streaks, Vettel has struggled a lot. Vettel finished 2, 3, 18, 4, 19, 2 in 2017 and 2, 1, 4, 3, 3, 6 in 2018.\nThe finish of 2019 against Bottas was not as smooth 2017 and 2018. Hamilton and Bottas had a strong exchange of blows starting with race 13. Overall, Bottas still lost too many points prior to that and Hamilton won the Championship with two races left. 2020 was a very dominant season for Hamilton. The season had a couple fewer races than the others due to covid and Hamilton still concluded the Championship with three races left. This is also underlined by the mean race pace per year. In 2019 Bottas was 24 milliseconds faster on average, while Hamilton was 374 milliseconds faster on average in 2020.\nFinally, we can look at the 2021 season, which is the first season Hamilton lost since 2016. The Championship was decided in the very last race, with a really small margin. The 2021 season is the closest of all analyzed seasons. Hamilton had a very strong final phase here too. Starting with race 17, he finished 2nd twice behind Verstappen. At this point, the pressure was extremely high and Hamilton had to win the next three races, which he did.\nPitstops Formula 1 is of course a Constructors Sport. No matter how good a Driver is, if the car is not good enough, a Championship is simply not possible. Building a good car is only half the battle. During the races, the Team has to make the right calls regarding strategy and perform well during Pitstops. In the following, we will compare how much time the Championship Contenders lost or gained during their Pitstops. Each bar in the following plot represents the Delta between two Pitstops. If for example, Hamilton has a two Pitstop race and Verstappen has a three Pitstop race, there will only be two bars for that race: One for comparing Hamilton\u0026rsquo;s first Pitstop to Verstappen\u0026rsquo;s first Pitstop and another one for comparing Hamilton\u0026rsquo;s second Pitstop to Verstappens second Pitstop. Additionally, we exclude any Pitstops with Deltas larger than 4 seconds. This is done to avoid comparing a normal Pitstop in which only the tyres are changed to a Pitstop in which a damaged front wing had to be replaced.\nIn 2017 and 2018 the results are fairly close in terms of how often each driver gained an advantage. Vettel gained time 21 times, while Hamilton gained time 28 times. However, the overall mean time gain per Pitstop for 2017 and 2018 respectively was 381 and 271 milliseconds in favor of Hamilton.\nIn 2019 and 2020 the Pitstops are a lot closer. Overall Bottas gained 49 milliseconds in 2019 and 79 milliseconds in 2020 on average.\nRed Bull is known for their fast Pitstops. This is represented by the data as well. In the 30 compared Pitstops Mercedes only beat Red Bull 9 times in 2021. On average Verstappen gained 268 milliseconds per Pitstop during that season.\nConclusion As mentioned at the start, the data should not be overinterpreted. Instead of drawing conclusions from this data, we looked at commonly known things in Formula 1, such as Red Bull\u0026rsquo;s strong Pitstops and Hamilton\u0026rsquo;s strong final phase, and looked if we could find these impressions in the data. Overall this was a fun way for me to refresh my SQL, Pandas, and ggplot knowledge.\n","permalink":"https://huti26.github.io/posts/formula-1/lewis-hamilton-2017-2021/","summary":"Preface The 2021 season of Formula 1 is the first one I watched and I have been a fan of both Hamilton and Bottas ever since. During this season Bottas has been criticized a lot. That\u0026rsquo;s why I wanted to do a bit of data analysis on how Hamilton and Bottas compare in raw numbers. Raw numbers are not a particularly good way to make hard statements in Formula 1, since they lack a lot of context.","title":"Lewis Hamilton 2017-2021"},{"content":"Preface Discord is a popular chatting platform and there are plenty of bots available, which can entertain you and your friends. There are bots out there, that post images from for example Reddit. The bots do not post the image itself, instead, they post the link to the image, and Discord itself will embed the image into the chat. Pixiv is a Japanese platform on which artists share their creations. At the time I created this bot, embedding images from Pixiv was blocked. One could only embed posts, which does not look as good as only embedding the actual image. In the following image, you can see the proper way on top, and the not wanted way on the bottom.\nThe last time I tested, embedding images from Pixiv now works properly, so this bot is not needed anymore. Regardless, I thought I would write down this article to recapitulate my findings. The general learned experience from this project might come in handy in the future.\nCode Structure The code is available on GitHub.\n/Bot\r│ config-template.ini\r│ config.ini\r│ discord_bot.py\r│ image_fetcher.py\r│ __init__.py\r│\r├───Cogs\r│ │ README.md\r│ │\r│ ├───Resources\r│ │ admins.txt\r│ │ artist-blacklist.txt\r│ │ blacklist-nsfw.txt\r│ │ blacklist.txt\r│ │ reports.txt\r│ │\r│ └───Source\r│ general_cog.py\r│ pixiv_cog.py\r│ __init__.py\r│\r└───Pixiv\r│ README.md\r│\r├───Docs\r│ └───Images\r│ illustrations.png\r│ multiimages.png\r│\r├───Resources\r│ offsets.json\r│\r├───Source\r│ pixiv_db.py\r│ pixiv_db_initializer.py\r│ pixiv_downloader.py\r│ __init__.py\r│\r└───Tests\rtest_pixiv_db.py Our code is separated into two processes, which communicate with each other. /Bot/discord_bot.py handles our discord communication and /Bot/image_fetcher.py starts our Pixiv image downloader.\nInside /Bot/Cogs/Source is code that uses the module discord.py. This code handles the communication with the Discord API. With discord.py you can separate your Discord bot\u0026rsquo;s logic into segments, called cogs. We have a general cog, which handles typical bot commands, such as responding to a ping. Anything Pixiv related is coded inside pixiv_cog.py. Additionally, we have a /Bot/Resources folder, in which you can find various configuration files.\nCode handling communication with Pixiv is inside /Bot/Pixiv. This part of the code downloads images from Pixiv and saves them inside our database.\nDatabase Before the overall architecture, we want to look at the database. This will help with the understanding of the architecture. The database consists of 3 tables: Images, Tags, and ImageTags. Pictures on Pixiv are tagged, just like for example on Instagram. A picture of a cat might have the tags \u0026ldquo;cat\u0026rdquo; and \u0026ldquo;cute\u0026rdquo;. Our tables are created as follows.\nCREATE TABLE IF NOT EXISTS \u0026#34;Images\u0026#34; ( \u0026#34;pixiv_image_id\u0026#34; integer, \u0026#34;file_path\u0026#34; TEXT NOT NULL, \u0026#34;safety_level\u0026#34; integer NOT NULL, \u0026#34;artist\u0026#34; integer NOT NULL, \u0026#34;total_bookmarks\u0026#34; integer NOT NULL, PRIMARY KEY(\u0026#34;pixiv_image_id\u0026#34;) ); Our Images table does not store the actual image, instead, it stores the file path. Additionally, it stores some metadata. The primary key is the pixiv_image_id.\nCREATE TABLE IF NOT EXISTS \u0026#34;Tags\u0026#34; ( \u0026#34;tag_id\u0026#34; INTEGER, \u0026#34;tag\u0026#34; TEXT NOT NULL UNIQUE, PRIMARY KEY(\u0026#34;tag_id\u0026#34;) ); Separately we store our tags. Each tag is a string and has an associated primary key tag_id.\nCREATE TABLE IF NOT EXISTS \u0026#34;ImageTags\u0026#34; ( \u0026#34;pixiv_image_id\u0026#34; INTEGER, \u0026#34;tag_id\u0026#34; INTEGER, PRIMARY KEY(\u0026#34;pixiv_image_id\u0026#34;,\u0026#34;tag_id\u0026#34;) ); Finally, we have the ImageTags table which connects our Images and Tags. If for example, an image has two tags, there will also be two rows inside this table.\nRegarding performance we want our bot to have fast reads. We want to respond to our users quickly. We care a lot less about writing speed. The most common way to use this bot is to give it a hashtag and expect a random picture with that hashtag in return. To optimize for this use case, we create indexes.\nCREATE INDEX IF NOT EXISTS \u0026#34;imagetags_index\u0026#34; ON \u0026#34;ImageTags\u0026#34; ( \u0026#34;tag_id\u0026#34;, \u0026#34;pixiv_image_id\u0026#34; ); CREATE INDEX IF NOT EXISTS \u0026#34;tags_index\u0026#34; ON \u0026#34;Tags\u0026#34; ( \u0026#34;tag\u0026#34;, \u0026#34;tag_id\u0026#34; ); We index both our Tags and our ImageTags. To understand why let\u0026rsquo;s look at what happens when a user requests a random image with a certain hashtag. For that we look inside /Bot/Pixiv/Source/pixiv_db.py and the return_image(self, tags: list, safety_level: int) function inside it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def return_image(self, tags: list, safety_level: int): ... for _ in tags: where_condition += \u0026#34;tag = ? or \u0026#34; # remove last \u0026#34;or \u0026#34; where_condition = where_condition[:-3] query = f\u0026#34;\u0026#34;\u0026#34; SELECT * FROM Images WHERE pixiv_image_id IN ( SELECT pixiv_image_id FROM ImageTags WHERE ImageTags.tag_id IN ( SELECT tag_id FROM Tags WHERE {where_condition} ) GROUP BY pixiv_image_id HAVING COUNT(*) \u0026gt;= ? ) AND {safety_condition} ORDER BY RANDOM() LIMIT 1 \u0026#34;\u0026#34;\u0026#34; tag_count = len(tags) tags.append(tag_count) tags.append(safety_level) values = tags self.cursor.execute(query, values) data = self.cursor.fetchone() if data is not None: return dict(data) else: return {} On Pixiv images are separated into SFW (Safe For Work) and NSFW (Not Safe For Work). This is indicated with a safety level of 0 or 1 respectively. Our user may insert multiple tags. To handle that we dynamically adjust our query. This is done in rows 4 to 8. It is important to note, that we do not insert our tags directly into a string in that step. That would be a security issue. Instead, we insert question marks, which then will be replaced later on by our SQL Framework. In that step, these values will be sanitized.\nNow regarding our indexes, the actual query is of interest. We have a double nested query. Let\u0026rsquo;s start with the innermost query.\nSELECT tag_id FROM Tags WHERE {where_condition} First, we have to select all tag_ids matching our string hashtags. Here our tags_index will improve the performance. Next, we use these selected tag_ids as follows.\nSELECT pixiv_image_id FROM ImageTags WHERE ImageTags.tag_id IN ( -- tag_ids we selected before, that match our hashtags ) GROUP BY pixiv_image_id HAVING COUNT(*) \u0026gt;= ? So now we select pixiv_image_ids. These pixiv_image_ids have to be tagged with the prior filtered tag_ids. Our imagetags_index will improve performance here. Finally, our pixiv_image_ids are grouped and their count is taken into consideration. This is because the user might request images that match with multiple tags.\nSELECT * FROM Images WHERE pixiv_image_id IN ( -- pixiv_image_ids we selected before ) AND {safety_condition} ORDER BY RANDOM() LIMIT 1 In the next step, we will select these pixiv_image_ids from our Images table and return one at random. The safety condition will also be applied. Here we don\u0026rsquo;t have any indexes at play. Why? Having our data sorted by the safety condition which is either 0 or 1 seems useful here, right? Creating a normal B+ Tree-based index is bad on columns with such few values. Instead, a bitmap index would be useful here. Unfortunately, SQLite does not support bitmap indexes.\nTwo final aspects to consider here regarding indexes are the following:\nThe order of the columns in our indexes is important in SQLite. We pick tag_id, then pixiv_image_id for our imagetags_index, because we will always query for tag_id.This should apply to any SQL database in theory, but if you use a different database, make sure to read the documentation. In case you are curious about details on SQLite, here is the link. If we create our imagetags_index with only the column tag_id, it would return us the table row ids for the matching tag_ids quickly. But then we would have to find the pixiv_image_ids in another step. That\u0026rsquo;s why it is important to have both fields as part of the index. Architecture Our overall architecture looks as follows.\nIn grey, our two processes are indicated. On the left, the Image Fetcher is constantly downloading images and writing them into the database. This occurs constantly, disregarding user activity. On the right, we have user activity. Whenever a user is requesting an image with a certain hashtag, the Discordbot will read such an image from the database and respond. Additionally, the Discordbot transmits the Hashtag to the Image Fetcher. The Image Fetcher keeps track of which hashtags are popular and will favor those when requesting images.\nDownloading images is realised with pixivpy. If you are curious about the details you can check this file. As mentioned, Discord communication is realized with discord.py.\nLogging, Scheduling, and Inter-Process Communication are realized with Python\u0026rsquo;s internal libraries.\nFunctionality The bot can fetch SFW or NSFW images, with an arbitrarily amount of tags. Additionally, the users can report certain images. The admins can then either blacklist an image or the entire artist. This can come in handy if you for example an artist is uploading inappropriate art and not tagging them as NSFW. Also, you might not want to see any images related to Finland. In this case, you can completely blacklist the hashtag \u0026ldquo;Finland\u0026rdquo;. Blacklisting also deletes all images fitting that criteria from the Database. No new images fitting the blacklist will be downloaded.\nDeployment This project is deployed on my Raspberry Pi 4 with Docker and Docker-Compose. I like not having to maintain any dependencies natively on my Raspberry Pi and Docker-Compose automatically restarts the Bot, anytime I might need to restart the Raspberry Pi. Here is the Dockerfile:\nFROM python:3.8-slim WORKDIR /discordapp # Need build-essential for building wheels RUN apt-get update RUN apt-get install build-essential -y --no-install-recommends # First copy just the requirements and install them COPY requirements.txt requirements.txt RUN pip install -r requirements.txt # COPY entrypoint.sh entrypoint.sh RUN [\u0026#34;chmod\u0026#34;, \u0026#34;+x\u0026#34;, \u0026#34;entrypoint.sh\u0026#34;] # Only part that changes, do this last for maximum caching COPY ./Bot/ ./Bot/ CMD [\u0026#34;./entrypoint.sh\u0026#34;] It uses entrypoint.sh to start our processes.\n#!/bin/bash python3 -u ./Bot/Pixiv/Source/pixiv_db_initializer.py python3 -u ./Bot/image_fetcher.py \u0026amp; python3 -u ./Bot/discord_bot.py And this is then utilized by our docker-compose file.\n--- version: \u0026#34;2.1\u0026#34; services: discordbot: build: . image: discordbot-huti container_name: discordbot volumes: # Database - pixiv_db:/discordapp/DB/ # Offsets - pixiv_resources:/discordapp/Bot/Pixiv/Resources/ # admins, blacklisted_tags - cog_resources:/discordapp/Bot/Cogs/Resources/ restart: unless-stopped volumes: pixiv_db: pixiv_resources: cog_resources: We have separate volumes for the actual database, the resources for our Pixiv Image Fetcher, and the resources for our Cogs.\nThe dockerfile is very simple and does not make any optimizations such as removing not required build files. If you are curious about how that would work, you can check out my blog entry about my Master\u0026rsquo;s Thesis, in which I made RocksDB distributed. There I have a more complex building system.\n","permalink":"https://huti26.github.io/posts/discordbot/discordbot/","summary":"Preface Discord is a popular chatting platform and there are plenty of bots available, which can entertain you and your friends. There are bots out there, that post images from for example Reddit. The bots do not post the image itself, instead, they post the link to the image, and Discord itself will embed the image into the chat. Pixiv is a Japanese platform on which artists share their creations. At the time I created this bot, embedding images from Pixiv was blocked.","title":"Creating a Multi-Process Python Bot to serve Pixiv Images with discord.py and SQLite"},{"content":"Preface Nowadays it is universally known that a Java thread corresponds to a Kernel-Level Thread when looking at it in a very simplified matter. Project Loom is trying to introduce User-Level Threads to the Java ecosystem. They call these User-Level Threads Virtual Threads. Virtual Threads are supposed to increase performance and be more resource efficient than just using Kernel-Level Threads. It is planned, that Virtual Threads become a drop-in replacement, which would be a free performance boost for any multithreaded Java application. This is particularly interesting because many Big-Data-Frameworks such as Hadoop or Spark are written in Java.\nAll my findings are from my Bachelor\u0026rsquo;s Thesis, which I wrote during the first three months of 2020. I now recapitulate my findings, so beware, that some findings might already be outdated since two years have passed. The build used to benchmark the code is: Build 15-loom+4-55, released on 22.02.2020.\nIf you are already familiar with the general concept of virtual threads, feel free to skip to the evaluation chapter as the chapters prior can be a bit detailed.\nBackground We will start with processes and programs and work ourselves towards Virtual Threads from there.\nProcess A process is a program that is being executed. In the following figure, the relation between a Process and a program is presented.\nThe segment text contains the programcode. The segment initialized data contains initialized global and static variables. The segment bss contains not yet initialized global variables and static variables. The heap is the extension of the bss. The stack is used for saving local variables, function parameters, and memory areas of register contents. Both the stack and the heap will be dynamically extended if needed. Processes allow a system to process several programs simultaneously. An active process can have three different statuses.\nSource: Andrew S. Tannenbaum, Modern Operating Systems The scheduler plays an important role here. A scheduler is responsible for when a process is processed by the CPU. In UNIX systems, for example, a round-robin scheduler is used. This changes the active process in a predetermined cycle, such as 10-100ms. An active process is a process with the status Running. The CPU is then assigned to the process and executes the program of the process. If the CPU is removed from the process by the scheduler, the process is in Ready status. The process is then ready to be reassigned to the CPU and is waiting for it. A process can also block. This happens if the process has to wait for a certain input. The process is then in Blocked status and will not change to Ready status until the input becomes available.\nThread A process is heavy. Each process has its own address space and switching between processes takes a lot of time. Therefore threads were created as lightweight processes. Threads are part of a process and therefore they share the same address space. Switching between threads is a lot faster. The following diagram lists the unique items for threads and processes.\nEach process can have many threads. The following figure shows the hierarchy of processes and threads.\nIn modern programming, the line between processes and threads gets somewhat blurred, because processes often start with a single thread. Standalone processes without any threads are a thing of the past.\nThreads increase CPU efficiency. Whenever a thread has to block, for example, because it has to wait for I/O input, another thread of the same process can quickly continue and make use of the CPU. Constantly switching between processes would be very inefficient.\nA distinction is made between two types of threads. Kernel-level threads, which are managed directly by the operating system\u0026rsquo;s scheduler, and User-level threads, where the programmer has to do the scheduling himself. Here is where Virtual Threads come into play.\nVirtual Threads Similar to how threads were created as lightweight processes, virtual threads were created as lightweight threads. Therefore they line up in the hierarchy right behind the kernel threads.\nSimilar to how the scheduler of an operating system allocates CPU time to each kernel thread, the JVM allocates CPU time to each virtual thread.\nSuperficially, this is all there is to virtual threads. A lightweight thread, which is supposed to make switching even more efficient than switching between threads.\nUnderneath the hood, there is a concept, which the developers of Project Loom call Continuation. We will look at that now. This chapter will be rather low-level and just about coding details, which we found when analyzing the source code. If that\u0026rsquo;s too detailed for you, skip ahead to the evaluation chapter, in which we run various benchmarks with virtual threads.\nContinuation In this chapter, we will analyze continuations conceptually and especially how they are implemented by Project Loom on a code level.\nA continuation is a sequence of instructions, that can be yielded and continued. The following example shows how a continuation can calculate the sum of the first five natural numbers while yielding after each increment.\nvar scope = new ContinuationScope(\u0026#34;ContinuationScope\u0026#34;); var continuation = new Continuation(scope, () -\u0026gt; { int n = 0; for(int i = 0; i \u0026lt; 6; i++) { n = n + i; System.out.println(\u0026#34;i: \u0026#34; + i + \u0026#34;\\t\u0026#34; + \u0026#34;n: \u0026#34; + n); Continuation.yield(scope); } }); while(!continuation.isDone()) { System.out.println(); continuation.run(); } A continuation requires two arguments: A scope and a runnable. First, a scope is created. Then the continuation is created. Afterwards, the continuation is repeatedly run in a loop until it is done.\nThe scope is used to allow continuations to be nested. Each continuation has exactly one parent continuation and exactly one child continuation. Continuations run on top of kernel-level Java threads, which are referred to as carrier-threads. Every Java carrier-thread has exactly one continuation as one of its attributes. This continuation is supposed to be the innermost one.\nStack Each continuation has two stacks: One for objects and one for non-objects. Project Loom calls the continuation stack horizontal stack and the thread stack vertical stack. The abbreviated versions are called h-stack and v-stack. When a continuation yields, it will be unmounted. Unmounting copies the continuation frames from the thread stack to the continuation stack. Afterwards, the continuation frames are removed from the thread stack. Project Loom calls this process freezing. When a continuation starts or continues, it will be mounted. Mounting is the reverse process of unmounting. Therefore project Loom calls this process thawing.\nSource: https://www.youtube.com/watch?v=NV46KFV1m-4 Looking at the previously mentioned two stacks on code level: One stack is an integer array for primitive values and metadata. The other one is an object array for references. All stack-related methods exist twice: Once for the integer array and once for the object array. The methods are very similar. Therefore this thesis will only explain one set of them, which is the integer set.\nProject Loom uses an integer sp as a stack pointer. Every time the stack is changed, the stack pointer will be updated. This is done by using the fixDecreasingIndexAfterResize method. As an example observe fixDecreasingIndexAfterResize using the arguments: index = 5, oldLength = 10, newLength = 20. It returns 15. The new stack pointer is therefore 15. That means that if a stack size gets changed, the remaining values are inserted starting at the end.\nprivate int sp = -1; // index into the h-stack private int fixDecreasingIndexAfterResize(int index, int oldLength, int newLength) { return newLength - (oldLength - index); } There are 3 functions related to stack management:\ngetStack: The getStack method is used to expand the stack. First, the program will test, whether the stack is null. If that is the case, the stack has not been created yet. Then the program will create the stack and adjust the stack pointer. If the stack is not empty, the program will create a new stack, that is big enough to fit the old and the new frames. Then it will copy the old frames into the new stack. Afterwards, the continuation\u0026rsquo;s stack is set to the new stack. The stack pointer will be updated. The return value is boolean. It is True when getStack succeded. The method fails when the old stack length is larger than the new one.\nresizeStack: Similar to how the getStack method expands the stack, the resizeStack method shrinks it. In contrast to the getStack method, the resizeStack method is a void and does not require to return a boolean on whether it succeeded or not.\nmaybeShrink: The maybeShrink method is called after yielding. It checks whether the stack size is bigger than a watermark it keeps track of. If it is, the watermark will be set to the stack size. Therefore if maybeShrink is called without adjusting the watermark, that means, that the stack size has not increased since the last time. If maybeShrink is called ten times without adjusting the watermark, the resizeStack method will be called with the watermark as its argument.\nAnother interesting detail is how Project Loom solves critical sections. This is done with a classic semaphore design. The method pin increments the semaphore cs and the method unpin decrements it.\nprivate short cs; // critical section semaphore public static void pin() { Continuation cont = currentCarrierThread().getContinuation(); if (cont != null) { if (cont.cs == Short.MAX_VALUE) throw new IllegalStateException(\u0026#34;Too many pins\u0026#34;); cont.cs++; } } public static void unpin() { Continuation cont = currentCarrierThread().getContinuation(); if (cont != null) { if (cont.cs == 0) throw new IllegalStateException(\u0026#34;Not pinned\u0026#34;); cont.cs--; } } Intrinsic Functions Intrinsic Functions are not implemented in Java, but in native code instead. Project Loom uses intrinsics to increase performance. Usually, if there is an intrinsic version of a function, there will still be a non-intrinsic one. In this case, there are no non-intrinsic versions of the functions. The following functions are exclusively intrinsic and relevant for this thesis:\n@HotSpotIntrinsicCandidate private static long getSP() @HotSpotIntrinsicCandidate private void doContinue() @HotSpotIntrinsicCandidate private static int doYield(int scopes) For the x86 platform the following functions in stubGenerator_x86_64.cpp create the bytecode for the intrinsic versions of the functions named above:\naddress generate_cont_getSP() address generate_cont_thaw(bool return_barrier, bool exception) RuntimeStub *generate_cont_doYield() In these functions, the maintainers of project Loom use their macroassembler. The macroassembler class can be found in src/hotspot/cpu/x86/macroAssembler_x86.cpp. The yield method also utilizes two other big classes. One is a codebuffer located in src/hotspot/cpu/x86/asm/codeBuffer.cpp and the other is a oopmap, used for garbage collection, located in src/hotspot/share/compiler/oopMap.cpp.\nAnalyzing these functions was not possible within the timeframe of my Bachelor\u0026rsquo;s Thesis, but if you are curious about even more details, these are the places to look for.\nRun When a continuation gets started or continued the run method is called. First, the run method mounts the continuation. Mounting is realized using a VarHandle, which atomically sets a boolean to true.\nAfterwards, the current carrier-thread\u0026rsquo;s continuation has to be updated. As an example, the current carrier-thread\u0026rsquo;s continuation before the update will be called bar. The new continuation will be called foo.\nThread t = currentCarrierThread(); if (parent != null) { if (parent != t.getContinuation()) throw new IllegalStateException(); } else this.parent = t.getContinuation(); t.setContinuation(this); foo is not supposed to have a parent at this point. If foo has a parent, that parent has to be bar. If it is not, something seriously went wrong and an error is thrown. The expected outcome is, that foo has no parent. In that case, foo will become the child bar. Afterwards, foo will become the current carrier-threads\u0026rsquo; continuation.\nBefore Carrier-Thread\u0026rsquo;s Continuation gets updated After Carrier-Thread\u0026rsquo;s Continuation gets updated After that, the run method will call either the enter method or the continue method depending on whether foo is being started for the first time.\nOnce foo is done, it returns to the run method. The current carrier-thread\u0026rsquo;s continuation will be set to bar. foo will be removed as a child of bar. Finally, foo will be unmounted, similar to how it was mounted at the beginning.\nYield Project Loom uses three different yield functions, which are always called in the same order.\nFirst public static boolean yield(ContinuationScope scope) is called. This method checks, if the current carrier-thread\u0026rsquo;s continuation is part of the given ContinuationScope scope. This is done by looping a variable c through the parents of the current carrier-thread\u0026rsquo;s continuation. The loop stops once c is either null or the scope of c is the same as the given scope. If c ends up being null, that means, that the program is being told to yield a ContinuationScope, while it is running a continuation, that is not part of that scope. This is undesired behavior and throws an exception. If c is not null, the program can continue and the second function will be called.\nThe other two functions were beyond the scope of my thesis, because they either are completely intrinsic or because they utilize variables that are returned from intrinsics. This is once again a nice spot to continue research if you are curious about Project Loom.\nContinue The continue method is entirely intrinsic. Therefore it was beyond the scope of my thesis.\nEvaluation We will run two different sets of experiments. The first one is provided by Project Loom themselves. Those experiments focus on analyzing the performance of the Continuation class and its components. The second one is a simple experiment made by myself, which will benchmark virtual threads instead.\nHardware Setup All experiments were run in the following environment:\nCPU: Intel i7 8700k RAM: 16GB OS: Ubuntu Desktop 18.04 LTS The JVM is always invoked without any additional flags.\nJava Microbenchmark Harness (JMH) The Java Microbenchmark Harness is often abbreviated as JMH . It is a project of OpenJDK. They created it to benchmark JVMs. JVMs automatically make many optimizations to code. In general, this is very helpful, since it increases the performance. Unfortunately, it is not possible to turn off such optimizations. Therefore benchmarking Java programs is not an easy task.\nBenchmarks can be configured by using annotations. The most important annotations for this thesis are:\n@BenchmarkMode @OutputTimeUnit @State @Warmup @Measurement @Fork @BenchmarkMode defines what is measured. An example of that would be average time or throughput. @OutputTimeUnit defines how the measured results are returned. It can be any time unit, such as nanoseconds, seconds, or minutes. Classes marked with the @State annotation are \u0026ldquo;instantiated on demand and will be reused during the entire benchmark trial\u0026rdquo; according to OpenJDK. This annotation is a bit more complex, but most of the time it is used in a very simple manner: Just the benchmark class itself will be annotated with it. Then the JMH can reference its own fields just like any other Java program. This is called the default state. Typically the JVM is running a couple of Warmup runs before starting the actual benchmark. This is to be configured using the @Warmup annotation. Very similar to that the @Measurement annotation is used to configure the actual benchmark runs. Both support arguments like iterations. Which will tell the JMH how often to warm up or measure. There are two Fork options:\n@Fork(0) = forking disabled @Fork(1) = forking enabled By default, JMH forking is enabled. Forking means that the tests will run in separate processes. This is done to avoid JVM optimizations. Often @Fork(1) is still annotated regardless, just for the sake of clarity.\nOpenJDK\u0026rsquo;s Benchmarks The following benchmarks are made by Project Loom themselves using the JMH.\nThe benchmarks contain five different classes:\nFreeze Thaw FreezeAndThaw OneShot Oscillation All classes use the same settings:\n@BenchmarkMode(Mode.AverageTime) @OutputTimeUnit(TimeUnit.NANOSECONDS) @State(Scope.Thread) @Warmup(iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS) @Measurement(iterations = 5, time = 1, timeUnit = TimeUnit.SECONDS) @Fork(1) They measure the average time and return the results in nanoseconds. They run five warmup iterations before measuring. Each measurement is run five times. The state is the default state. Forking is enabled.\nEach class contains at least one function which is benchmarked multiple times using different parameters. All classes except for the Oscillation class use the same two parameters. Those parameters are called paramCount and stackDepth:\n@Param({\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;3\u0026#34;}) public int paramCount; @Param({\u0026#34;5\u0026#34;, \u0026#34;10\u0026#34;, \u0026#34;20\u0026#34;, \u0026#34;100\u0026#34;}) public int stackDepth; Measurement series are run for every combination of those two parameters. Resulting in 12 different measurements. The parameters of the Oscillation class are slightly different:\n@Param({\u0026#34;2\u0026#34;, \u0026#34;3\u0026#34;, \u0026#34;4\u0026#34;}) public int minDepth; @Param({\u0026#34;5\u0026#34;, \u0026#34;6\u0026#34;, \u0026#34;7\u0026#34;, \u0026#34;8\u0026#34;}) public int maxDepth; @Param({\u0026#34;10\u0026#34;, \u0026#34;100\u0026#34;, \u0026#34;1000\u0026#34;}) public int repeat; This results in 36 runs for the Oscillation class.\nAll classes use the same core task to benchmark. It is a simple recursive function. For example, if the stackDepth is 10, the function will call itself 10 times. Depending on the paramCount and which class is being benchmarked, the recursive function will then perform certain actions at a certain stackDepth.\nFreeze The Freeze benchmark optionally yields at a certain depth. It only measures yielding. An increase in the parameter count was expected to increase the time it takes to freeze since there are more frames to copy. This behavior can only be observed at a stack depth of 100. Measurement inaccuracies might be to blame for that. An increase in stack depth consistently increases the time it takes to finish the task, as expected.\nThaw Similar to how the Freeze benchmark only measures yielding, the Thaw benchmark only measures the reverse, which is continuing. The expectations here were the same as before: Both an increase in parameter count and an increase in stack depth should increase the time. This time around only at a stack depth of 5 the results were different from the expectations.\nFreezeAndThaw As the name implies FreezeAndThaw measures both freezing and thawing. It runs two different methods: baseline and yieldAndContinue. The difference between those two is, that the continuation of the baseline method doesn\u0026rsquo;t yield at the limit. Which means it doesn\u0026rsquo;t yield at all. It just completes the task. So one can compare the baseline and the yieldAndContinue run to see how strongly freezing and thawing affected the time to finish the task. As one can see in the figure below, the impact of yielding and continuing is big. Without it, the task is completed up to 10 times faster.\nOneShot The OneShot class has many functions. The names of those functions very clearly describe, what they do. The functions become increasingly more demanding. The first function just runs the method without yielding. The last one yields before and after each call.\nOne can also look at the individual functions in an isolated matter. One can immediately see that an increase in stack depth results in an increase in time. An increase in parameter count also seems to impact the time it takes to finish the task. One can observe it at a stack depth of 100.\nOscillation Here the continuation oscillates between a minimum and a maximum stack depth. It freezes at the maximum and continues afterwards. One expectation here was that an increase in repetitions increases the time it takes to complete the task. This expectation has proven to be true. Another expectation was, that the bigger the difference from the minimal depth to the maximum depth, the more time it would take to finish the task. This has also proven to be true.\nEcho Server Three Java classes are used in this experiment: EchoServerThread, EchoServerVThread, and Responder. If you are curious about the coding details, you can check them out in the GitHub Repo.\nEchoServerThread and EchoServerVThread are almost identical. They both start a server that listens for requests on port 5566. Once a request is made, they start a separate thread, which runs the Responder class. Then they will continue to listen for further requests. The difference between the two classes is, that the EchoServerThread class utilizes kernel threads, while the EchoServerVThread class uses virtual threads. The Responder class sends a valid HTTP 1.1 header. Then it will echo the request it received.\nRequests will be made using ApacheBench. All requests will be made with a concurrency of 100. Instead of running ApacheBench once with for example 1.000.000 requests, a shell script will be used. The shell script will then instead call ApacheBench 100 times in a row, with each run making 10.000 requests. This is done to remove ApacheBench as a potential bottleneck since it might not be fit to run 1.000.000 requests in a single process.\nThere will be two different kinds of measurement series:\nThe first one is being recorded with VisualVM. The second one is recorded with JProfiler. There are multiple reasons for that. First off all a profiler might influence the measurements. Running similar tests with different profilers should improve the accuracy of the results. Also exporting the data is not possible using VisualVM. JProfiler allows one to export all measurements and model them as one wants to.\nResults - Profiler: VisualVM Virtual threads in orange and kernel threads in blue. We meassure Time to complete 1.000, 10.000, and 100.000. requests.\nLooking at the scatter plots, one can immediately see that the virtual threads seem to not only perform better but also much more consistent than the kernel threads. When trying to model the results with linear regression, the first impression is further verified: The graph trying to model the virtual thread results is very precise. The margin for error is very small. On the other hand, the graph trying to model the kernel threads has a much bigger margin for error. This proves to be true in all 3 measurement series.\nModeling the same results using boxplots allows one to take a closer look at the details. The 1000 requests series using virtual threads has a median of 30ms. The first and the third quartile are each 1ms above and below the median. Therefore 50% of all the runs were completed between 29ms and 31ms. The 1000 requests series using kernel threads has a median of around 100ms. The first quartile is slightly below 50ms and the third quartile is slightly above 200ms. That results in a spread of more than 150ms in the 50% box.\nThe 10.000 requests series is more balanced: The 50% box of the virtual threads series spans from 279ms to 289ms, resulting in a span of 10ms. The median is at 284ms. The 50% box of the kernel threads series spans from 395ms to 430ms, resulting in a span of 35ms. The median is at 415ms. Any run that took longer than 480ms was considered an outlier for the kernel thread series and was not plotted. There are 10 such outliers.\nIn the 100.000 requests series, the 50% box of the virtual threads run is slightly bigger than 25ms. The 50% box of the kernel threads is slightly smaller than 20ms. Therefore this is the first series, in which the 50% box spans a smaller range for the kernel threads. When one looks at the corresponding scatter plot, one can see, that there are more than 20 out of 100 runs, in which kernel threads needed more than 5 seconds to finish a run. Also, there are around 10 more runs, which were finished in less than 4 seconds. These 30 runs were considered outliers. No such outliers can be observed in the virtual thread series. Therefore, virtual threads perform more consistently once again.\nResults - Profiler: JProfiler In the previous chapter the series with 1000 requests per run stuck out. Kernel threads performed especially more inconsistent in that series. Therefore the same experiment was run again here. Additionally, the number of runs was increased from 100 to 1000.\nLooking at the boxplots one can see, that the virtual threads performed similarly to before. The median is at 29ms and the 50% box spans from 28ms to 30ms. The median of the kernel thread series is slightly higher than before. The 50% box spans a range that is bigger than 100ms.\nAdditionally, the heap usage is monitored this time around. Virtual threads use less heap space. They also are more consistent in their heap usage.\nThe following series measures 10.000 requests, ran 500 times. Originally this series was intended to be run 1000 times. That was not possible, because the Linux kernel consistently killed the EchoServerThread process at around 600 runs.\nVirtual threads perform better and are more consistent once again. Most runs were finished faster than 300ms. Looking at the kernel threads, the majority of runs were finished around 500ms. There also is a significant minority of runs finishing around 800-900ms.\nThe median of the virtual thread series is 278ms. The median of the kernel thread series is 513ms. That is a decrease of more than 40%. Also, the virtual thread 50% box spans an area of 5ms here, while the kernel threads span an area of around 50ms.\nOnce again, the heap usage is significantly higher using kernel threads. The median of the kernel thread series is 200MB, while the virtual thread series has a median of 100MB.\nConclusion Our experiment was rather simple and tested on a single computer. This does not reflect Big-Data workloads well. Still, it gives us a first hint at the potential of virtual threads. Virtual threads beat the old implementation constantly and recently Oracle announced that virtual threads will come to Java 19. It will be very interesting to see, how quickly the popular Big-Data-Frameworks can update to Java 19 and how good the performance gains will be for real workloads. We will be able to get very precise results by comparing old versions of a Big-Data-Framework with new ones.\n","permalink":"https://huti26.github.io/posts/project-loom/project-loom/","summary":"Preface Nowadays it is universally known that a Java thread corresponds to a Kernel-Level Thread when looking at it in a very simplified matter. Project Loom is trying to introduce User-Level Threads to the Java ecosystem. They call these User-Level Threads Virtual Threads. Virtual Threads are supposed to increase performance and be more resource efficient than just using Kernel-Level Threads. It is planned, that Virtual Threads become a drop-in replacement, which would be a free performance boost for any multithreaded Java application.","title":"OpenJDK's Project Loom: User-Level Threads in Java"},{"content":"Preface Flutter is first and foremost a Framework to create cross-platform applications, but it can also be used to create WebApps. Since I don\u0026rsquo;t have any experience with the three big Web Frameworks: VueJS, React, and Angular, I thought Flutter would be a good fit for me. It also uses Dart instead Javascript. My initial impression was, that this is also an improvement. Since I have not made any proper experiences with other Web Frameworks or Javascript, I won\u0026rsquo;t be able to compare those to Flutter/Dart in this article. However, I will still talk about my impressions as someone with very little experience in Frontend. My interest is in the Backend and in that regard, we will look at Firebase and how Flutter integrates with it.\nThe project is far from in a finished state and I have no intentions of actually finishing it. Visually it\u0026rsquo;s not particularly pretty, but it is decently functional. You can check it out yourself here.\nCode Structure The current code structure looks as follows.\n/lib\r│ generated_plugin_registrant.dart\r│ main.dart\r│\r├───Authentification\r│ auth.dart\r│\r├───Constants\r│ constants.dart\r│\r├───Database\r│ db_user.dart\r│\r├───Helper Functions\r│ navigation.dart\r│ numberformat.dart\r│ textfields.dart\r│\r├───Model\r│ food.dart\r│ meal.dart\r│ user.dart\r│\r├───Pages\r│ │ login.dart\r│ │ register.dart\r│ │ welcome.dart\r│ │\r│ └───Mealprep\r│ │ foods.dart\r│ │ home.dart\r│ │ settings.dart\r│ └───Settings\r│ change_activitylevel_page.dart\r│ change_age_page.dart\r│ change_goal_page.dart\r│ change_height_page.dart\r│ change_language_page.dart\r│ change_region_page.dart\r│ change_sex_page.dart\r│ change_weight_page.dart\r│ settings_card.dart\r│\r├───Providers\r│ currently_created_food.dart\r│ currently_created_meal.dart\r│\r└───Widgets\r└───Cards\rfood_card.dart The entrypoint to the app is main.dart. Further sites of the WebApp are found in the /Pages folder. To mirror our logic with dart classes we have the /Model folder. There we find the concepts of foods, meals, and users. Shared constants among the project are found in /Constants/constants.dart. Most of our interaction with the database and the authentication is bundled within the pages, but small shared parts are in their respective Database and Authentication folder. Reused Widgets are in the Widgets folder, but we put very little effort into the visual part of this project, so there is only one item inside that folder. The last and particularly interesting part is the /Providers folder. There we have two classes, which maintain shared state in our WebApp. A user can create new foods and meals and while doing so he visits multiple sites, which need to share a state.\nApp Walkthrough Now that we have seen the general code structure, we will have a walkthrough of the app and its functions. We will refer to the interesting code excerpts while doing so.\nLog In \u0026amp; Register The first page a user is greeted by is our not-so-pretty welcome screen, in which a user either logs in or registers.\nA nice detail the welcome screen implements is that a user stays logged in even after closing the site, unless of course, he manually logs out. So if a user is logged in already, the welcome screen will automatically refer him to the homepage of the app. This is done as follows.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 @override void initState() { FirebaseAuth.instance.authStateChanges().listen((User user) async { if (user != null \u0026amp;\u0026amp; !attemptingToLogin) { attemptingToLogin = true; print(\u0026#34;Attempting to login\u0026#34;); if (!user.emailVerified) { setState(() { logedInButNotVerified = true; }); } else if (user.emailVerified) { print(\u0026#34;checking for user data\u0026#34;); await userReference.doc(\u0026#34;${user.uid}\u0026#34;).get().then((userSnapshot) async { if (userSnapshot.exists) { print(\u0026#34;user found\u0026#34;); } else { await createUserInFirestore(user.uid); } }).catchError((error) =\u0026gt; print(error)); print(\u0026#34;logged in verified user\u0026#34;); hardNavigate(context, MealprepTabContainer()); } } }); super.initState(); } If this is the first time looking at Flutter/Firebase code, this is a lot to swallow, so we will go over it one by one. First: the function initState(). I am far from a Flutter expert and will describe things in a simple way. If you are curious about details and intending to become a Flutter expert, I strongly suggest you check out the documentation. Simply put, this function is called when a stateful widget in Flutter is loaded. We won\u0026rsquo;t go into furth details on stateful and stateless widgets, but the general concept is easy to guess by the name. Some structures in Flutter maintain a state and some don\u0026rsquo;t. Inside our welcome page, we want to maintain a state, since we want to check, if a user is logged in or not. We do this, by listening to FirebaseAuth. This async function will notify us if the login status changes. Once that happens, we will also check, if our user\u0026rsquo;s email is verified. Our app only allows verified users to log in. If the user is verified, our helper function hardNavigate() is called and sends the user to the mentioned homepage of the app. We won\u0026rsquo;t go into any details on how navigation in Flutter works.\nThe login page looks as follows and supports resetting the user\u0026rsquo;s password.\nThe Flutter version I used, does not support browser autofilling credentials. Since I created this app more than a year ago, I hope they fixed this issue in the meantime.\nAll authentication here is also handled by FirebaseAuth. Additionally, we want to maintain further information on our users. Since this is a calorietracker, we need all sorts of information on our users, such as their age or their weight. So additionally to what FirebaseAuth stores, we create a user object, which we store inside our database Firestore. This exchange between database and dart code is an interesting detail. When we request a user object from the database, we get JSON data, which binds keys to values. Our keys are guaranteed to be strings, while our values might be anything. To create a user object in dart from given data we use the following code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 MealprepUser.fromMap(Map\u0026lt;String, dynamic\u0026gt; data) { uid = data[\u0026#34;uid\u0026#34;]; name = data[\u0026#34;name\u0026#34;]; age = data[\u0026#34;age\u0026#34;]; height = data[\u0026#34;height\u0026#34;]; weight = data[\u0026#34;weight\u0026#34;]; sex = data[\u0026#34;sex\u0026#34;]; goal = data[\u0026#34;goal\u0026#34;]; bmr = data[\u0026#34;bmr\u0026#34;]; caloriegoal = data[\u0026#34;caloriegoal\u0026#34;]; activitylevel = data[\u0026#34;activitylevel\u0026#34;]; language = data[\u0026#34;language\u0026#34;]; region = data[\u0026#34;region\u0026#34;]; } In dart, we can create Maps with elements of a dynamic type. In this case, it is rather boring, since all the dynamic types are either strings or numbers. The meal class is more exciting.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Meal.fromMap(Map\u0026lt;String, dynamic\u0026gt; data) { name = data[\u0026#34;name\u0026#34;]; description = data[\u0026#34;description\u0026#34;]; submitterUID = data[\u0026#34;submitterUID\u0026#34;]; calories = data[\u0026#34;calories\u0026#34;]; carbs = data[\u0026#34;carbs\u0026#34;]; proteins = data[\u0026#34;proteins\u0026#34;]; fats = data[\u0026#34;fats\u0026#34;]; price = data[\u0026#34;price\u0026#34;]; priceonsale = data[\u0026#34;priceonsale\u0026#34;]; public = data[\u0026#34;public\u0026#34;]; List\u0026lt;dynamic\u0026gt; foodList = data[\u0026#34;foods\u0026#34;]; foodList.forEach((element) { foods.add(Food.fromMap(element)); }); } Starting with line 13 we add load food as JSON data from the database, create dart objects of it and add those to our meal.\nBefore this project, I have been taking part in the Stanford online course cs193p, which teaches Swift and SwiftUI. A feature I missed here from Swift is computed properties. In Swift, one can create variables which instead of storing a value, provide a getter and setter function. This would come in handy to automatically update the meal\u0026rsquo;s properties such as its total calorie count, whenever the object changes. This happens, when a user for example adds a new food to a meal. Instead, we have to manually call the following function, every time we change our meals.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 updateFields() { calories = 0; carbs = 0; proteins = 0; fats = 0; price = 0; priceonsale = 0; foods.forEach((food) { calories += food.calories; carbs += food.carbs; proteins += food.proteins; fats += food.fats; price += food.packageprice; priceonsale += food.packagepricesale; }); } Foods \u0026amp; Meals Once logged in, the user is greeted by a typical mobile app design. Of all the 4 tabs, only the settings one is useful. The user can change his goals and body measurements there.\nIf the user presses the + button in the middle, he can either create a new food or a new meal.\nWe will look at the food creation to understand the provider class.\nThe food creation consists of 4 pages. These pages simply contain forms, which the user has to fill out. Calories, price, and the name of the food are to be inserted among other things.\nThis form looks as follows in Flutter.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @override Widget build(BuildContext context) { return Scaffold( appBar: AppBar(), body: Form( key: formKey, child: Padding( padding: const EdgeInsets.all(generalPadding), child: Column( children: [ NumberTextField(\u0026#34;Barcode\u0026#34;, eanController), StringTextFieldRequired(\u0026#34;Name*\u0026#34;, nameController), StringTextField(\u0026#34;Brand\u0026#34;, brandController), RaisedButton( child: Text(\u0026#34;Continue\u0026#34;), onPressed: () { validateAndContinue(); }, ), ], ), ), ), ); } Inside a scaffold, we have a Form consisting of a Column with 4 children: 3 TextFields and 1 Button. When the button is pressed the validateAndContinue() function is called. Here our provider class is finally used. First, let\u0026rsquo;s look at our provider class itself.\n1 2 3 4 5 6 7 class CurrentlyCreatedFood extends ChangeNotifier{ Food food = Food.empty(); bool filledPage1 = false; bool filledPage2 = false; bool filledPage3 = false; } It is very simple, we have a food object and 3 boolean values, which indicate, whether we have already filled a page. Next, we look at how we use this provider class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 validateAndContinue() { if (formKey.currentState.validate()) { var currentFoodProvider = Provider.of\u0026lt;CurrentlyCreatedFood\u0026gt;(context,listen: false); var currentFood = currentFoodProvider.food; currentFood.ean = eanController.text; currentFood.name = nameController.text; currentFood.brand = brandController.text; currentFoodProvider.filledPage1 = true; softNavigate(context, MealprepCreateFoodPage2()); } } Validation is done by Flutter itself, it only verifies that we have numbers in our NumberTextField and text in our StringTextField. Then, we access the currently created food object through this provider class. Next, the user will be navigated to page 2 out of 4.\nLet\u0026rsquo;s assume the user made a typo and returns to page 1 now. We don\u0026rsquo;t want to force the user to retype all values. To achieve that, we use the provider shared state again, this time in our initState() function.\n1 2 3 4 5 6 7 8 9 10 11 12 13 @override void initState() { var currentFoodProvider = Provider.of\u0026lt;CurrentlyCreatedFood\u0026gt;(context,listen: false); var currentFood = Provider.of\u0026lt;CurrentlyCreatedFood\u0026gt;(context,listen: false).food; if(currentFoodProvider.filledPage1){ eanController.text = currentFood.ean; nameController.text = currentFood.name; brandController.text = currentFood.brand; } super.initState(); } Our provider class keeps track of which pages we have already filled out. If we return to page 1, the TextFields will be filled out in this way.\nNow if the user reaches page 4 he will be able to submit the food to the database. The code for that looks as follows.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 submitFood() async { var currentFoodProvider = Provider.of\u0026lt;CurrentlyCreatedFood\u0026gt;(context,listen: false); var currentFood = Provider.of\u0026lt;CurrentlyCreatedFood\u0026gt;(context,listen: false).food; // set final meta data, things that user doesnt input var documentSnapshot = await usersReference.doc(\u0026#34;$authUID\u0026#34;).get(); var user = MealprepUser.fromMap(documentSnapshot.data()); currentFood.submitterUID = authUID; currentFood.region = user.region; var currentFoodAsMap = currentFood.toMap(); FirebaseFirestore.instance .collection(\u0026#34;foods\u0026#34;) .add(currentFoodAsMap) .then((value) { hardNavigate(context, MealprepTabContainer()); // reset food currentFoodProvider.filledPage1 = false; currentFoodProvider.filledPage2 = false; currentFoodProvider.filledPage3 = false; currentFood = Food.empty(); Fluttertoast.showToast( msg: \u0026#34;Successfully added the Food.\u0026#34;, toastLength: Toast.LENGTH_LONG); }).catchError((error) { Fluttertoast.showToast( msg: \u0026#34;Could not add the Food. Something went wrong.\u0026#34;, toastLength: Toast.LENGTH_LONG); }); } First, we access the food object through the provider again. Then we will add some metadata to the food object. Finally, the object can be submitted to the database. For that, we convert our object to JSON data with currentFood.toMap(). Then in the case of a successful submission, we send the user a little feedback in the form of a toast. In the case of an error, we also inform the user appropriately.\nCreating a meal is logically very similar.\nOn the left side, there is a list of all available foods. On the right side, there is a list of already added foods. The user can track the total meal price and calories while adding foods. If now the user wants to add Rigatoni, but not 500g, which is the default package size, he can click the item, and a modal opens.\nNot particularly pretty, but it gets the job done. If the user presses done, the updated package size, here 250g, will be added to the meal.\nIf the user is done, he can submit the meal to the database. The user may also choose if the meal is visible to others with the public toggle. The same applies to foods.\nConclusion From a technical point of view, there are two interesting parts: Flutter and Firebase. Regarding Flutter, I have been very happy with the resources available online. The code structure and concepts are very intuitive. However, I am not able to compare Flutter to other Cross-Platform Frameworks or Web Frameworks. Working with Firebase was also very intuitive. The Firebase Authentication service is well documented and easily integrates into Flutter Apps. Firestore, the Firebase Database, provides 50.000 free reads, 20.000 free writes, and 20.000 free deletes per day. For a hobby project, this is more than plenty. Functionality wise it also provides everything you would expect from a document-oriented database. I have not worked with a database as a service before and can only compare my experience to working with for example self-hosted RocksDB or a typical SQL database such as MySQL or SQLite. The fact, that you don\u0026rsquo;t have to worry about the hardware, on which the database runs, is of course comfortable. However, for a big project, I would be cautious concerning the costs. Changing from one NoSQL database to another can be a difficult process. I would spend a fair amount of time researching which database to use.\nThe second part of this project is the actual content. A calorietracker is of course nothing new. There are plenty of great ones out there already. One feature I am personally missing from them is more statistics! I would love to have something like the Spotify yearly review but for my nutrition. Also, the calorietrackers don\u0026rsquo;t track the costs of the foods consumed. I would love to have that available too. I will not continue the development of this app. While in theory, it should be easy to add missing features and port this app to iOS and Android, Making it a real product and competing with established solutions, which already have a strong userbase and plenty of items in their database seems unrealistic. Rather, I would love to have one of the established apps add some more statistics.\n","permalink":"https://huti26.github.io/posts/flutter-calorietracker/flutter-calorietracker/","summary":"Preface Flutter is first and foremost a Framework to create cross-platform applications, but it can also be used to create WebApps. Since I don\u0026rsquo;t have any experience with the three big Web Frameworks: VueJS, React, and Angular, I thought Flutter would be a good fit for me. It also uses Dart instead Javascript. My initial impression was, that this is also an improvement. Since I have not made any proper experiences with other Web Frameworks or Javascript, I won\u0026rsquo;t be able to compare those to Flutter/Dart in this article.","title":"Building a Calorietracker WebApp with Flutter and Firebase"}]